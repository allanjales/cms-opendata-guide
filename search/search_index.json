{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMS Open Data Guide \u00b6 Warning This guide is under construction Welcome to the guide for CMS open data. This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide. How to use this site \u00b6 There are three main tabs to help you navigate the site. It starts with the Computing Tools most likely needed to deal with CMS open data. Then, there is a little review of CMSSW , which is the software used by CMS. Finally the Analysis section guides you through the different steps (in the most general order) that you need to follow for performing a particle physics analysis with CMS open data. The site's philosophy \u00b6 This site is thought as a navigation aid. The CMS Collaboration has built an extensive amount of documentation over the years. However, given the nature of our rapidly evolving research activities, this documentation is usually scattered around, which makes it difficult to navigate. The main goal of this guide, therefore, is to facilitate the usage of CMS open/legacy data by providing a structured set of instructions that agglutinate those pieces of information already available in other sites. In this sense, we do not pretend to copy every little piece of information and/or code, but to help you get to it and find your way around it. For CMS open data the three main sources of documentation/information are: The CMS public Twiki pages . Particularly the workbook and the software guide Note When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. The CERN CMS Open Portal pages. This portal is not exactly meant to archive documentation. It is mainly a repository for our open data. However, it does host important information that is not so easy to find. This guide will point you to the right pages. The CMSSW code . Although less conventional, exploring the CMSSW code could be a really good source of information. For instance, having hundreds of trigger bits, if the information from a specific module used in a specific trigger (with which data was taken) was needed, it would be impossible to document that explicitly in some guide. Instead, one can explore the code and easily find out the needed information. We will try to show you how it is done. How to get help \u00b6 The best way to get additional help is to visit our open data forum . How to contribute or contact us \u00b6 Please follow these instructions if you would like to contribute. If you find bugs or have suggestions or recommendations to improve this guide, please fill out an issue or contact us .","title":"Home"},{"location":"#cms-open-data-guide","text":"Warning This guide is under construction Welcome to the guide for CMS open data. This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide.","title":"CMS Open Data Guide"},{"location":"#how-to-use-this-site","text":"There are three main tabs to help you navigate the site. It starts with the Computing Tools most likely needed to deal with CMS open data. Then, there is a little review of CMSSW , which is the software used by CMS. Finally the Analysis section guides you through the different steps (in the most general order) that you need to follow for performing a particle physics analysis with CMS open data.","title":"How to use this site"},{"location":"#the-sites-philosophy","text":"This site is thought as a navigation aid. The CMS Collaboration has built an extensive amount of documentation over the years. However, given the nature of our rapidly evolving research activities, this documentation is usually scattered around, which makes it difficult to navigate. The main goal of this guide, therefore, is to facilitate the usage of CMS open/legacy data by providing a structured set of instructions that agglutinate those pieces of information already available in other sites. In this sense, we do not pretend to copy every little piece of information and/or code, but to help you get to it and find your way around it. For CMS open data the three main sources of documentation/information are: The CMS public Twiki pages . Particularly the workbook and the software guide Note When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. The CERN CMS Open Portal pages. This portal is not exactly meant to archive documentation. It is mainly a repository for our open data. However, it does host important information that is not so easy to find. This guide will point you to the right pages. The CMSSW code . Although less conventional, exploring the CMSSW code could be a really good source of information. For instance, having hundreds of trigger bits, if the information from a specific module used in a specific trigger (with which data was taken) was needed, it would be impossible to document that explicitly in some guide. Instead, one can explore the code and easily find out the needed information. We will try to show you how it is done.","title":"The site's philosophy"},{"location":"#how-to-get-help","text":"The best way to get additional help is to visit our open data forum .","title":"How to get help"},{"location":"#how-to-contribute-or-contact-us","text":"Please follow these instructions if you would like to contribute. If you find bugs or have suggestions or recommendations to improve this guide, please fill out an issue or contact us .","title":"How to contribute or contact us"},{"location":"about/","text":"About \u00b6 This is the guide for CMS open data. All CMS instructional material is made available under the Creative Commons Attribution license . This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide. Contributors \u00b6 Matt Bellis Edgar Carrera Kati Lassila-Perini Tibor \u0160imko Marco Vidal Garc\u00eda Audrius Mecionis Allan Jales Contact \u00b6 Please contact us here .","title":"About"},{"location":"about/#about","text":"This is the guide for CMS open data. All CMS instructional material is made available under the Creative Commons Attribution license . This guide is brought to you by the CMS open data group, on a best-effort basis. All software and instructions are provided \"as is\", without warranty of any kind. This is ongoing work and we appreciate your feedback and/or your help building this guide.","title":"About"},{"location":"about/#contributors","text":"Matt Bellis Edgar Carrera Kati Lassila-Perini Tibor \u0160imko Marco Vidal Garc\u00eda Audrius Mecionis Allan Jales","title":"Contributors"},{"location":"about/#contact","text":"Please contact us here .","title":"Contact"},{"location":"faq/","text":"FAQ \u00b6 Warning This page is under construction Frequently Asked Questions and other problems and issues that have come up. Possible subsections below High-level questions \u00b6 Why would I choose VirtualBox over docker? Why would I choose docker over VirtualBox? \u00b6 Great question! Anyone? Docker \u00b6 Docker downloads container but never launches environment \u00b6 This is an issue with newer OSs on your local laptop/desktop running older OSs in the container. For example, suppose you are following the Running CMS analysis code using Docker tutorial. If you run docker run --name opendata -it cmsopendata/cmssw_5_3_32 /bin/bash and the container downloads but you don't find yourself in the CMSSW_5_3_32 environment, then... Data \u00b6 CMSSW \u00b6","title":"FAQ"},{"location":"faq/#faq","text":"Warning This page is under construction Frequently Asked Questions and other problems and issues that have come up. Possible subsections below","title":"FAQ"},{"location":"faq/#high-level-questions","text":"","title":"High-level questions"},{"location":"faq/#why-would-i-choose-virtualbox-over-docker-why-would-i-choose-docker-over-virtualbox","text":"Great question! Anyone?","title":"Why would I choose VirtualBox over docker? Why would I choose docker over VirtualBox?"},{"location":"faq/#docker","text":"","title":"Docker"},{"location":"faq/#docker-downloads-container-but-never-launches-environment","text":"This is an issue with newer OSs on your local laptop/desktop running older OSs in the container. For example, suppose you are following the Running CMS analysis code using Docker tutorial. If you run docker run --name opendata -it cmsopendata/cmssw_5_3_32 /bin/bash and the container downloads but you don't find yourself in the CMSSW_5_3_32 environment, then...","title":"Docker downloads container but never launches environment"},{"location":"faq/#data","text":"","title":"Data"},{"location":"faq/#cmssw","text":"","title":"CMSSW"},{"location":"analysis/backgrounds/qcdestimation/","text":"QCD Estimation \u00b6 Warning This page is under construction","title":"QCD Estimation"},{"location":"analysis/backgrounds/qcdestimation/#qcd-estimation","text":"Warning This page is under construction","title":"QCD Estimation"},{"location":"analysis/backgrounds/techniques/","text":"Techniques \u00b6 Warning This page is under construction","title":"Techniques"},{"location":"analysis/backgrounds/techniques/#techniques","text":"Warning This page is under construction","title":"Techniques"},{"location":"analysis/datasim/collisiondata/","text":"Collision Data \u00b6 Warning This page is under construction The CMS collision data is organized in primary datasets (PD). All CMS open data primary datasets can be found with this search . The dataset name consists of three parts separated by \"/\", e.g.: /TauPlusX/Run2011A-12Oct2013-v1/AOD The first part indicates the primary dataset contents ( TauPlusX ), the second part is the data-taking era ( Run2011A ) and reprocessing ( 12Oct2013 ), and the last one indicates the data format ( AOD ). Dataset contents \u00b6 The primary dataset definition is centered around physics objects (SingleMu, Jet, Tau etc). Events triggered by High Level Triggers (HLT) with a similar physics contents or use are mostly directed in the same PD. This guide gives an overview of the CMS trigger system. Besides requirements on the physics content, the organisation of the primary datasets has to satisfy constraints related to the data processing and handling, such as the average event rate approximately uniform across the different PDs, and the event rate within a certain range. Each CMS collision dataset comes with a brief description of the contents, and the full listing of all possible HLT trigger streams included in the dataset. The instructions how to find the exact definitions and parameters of the HLT trigger definitions can be found in Guide to the CMS Trigger System under \" HLT Trigger Path definitions \". Since a given event can pass more than one HLT path, it can be included in more than one primary dataset. There's an overall overlap between the PDs of around 25-35% during Run1 and it must be taken into account when combining events from different datasets in an analysis. Data taking and reprocessing \u00b6 One year of data taking is divided in several \"eras\" indicated as RunA, RunB, etc. According to the CMS data policy, 50% of data is published after the embargo period, completed with the full release within 10 years. Currently available are Run2010A and Run2010B Run2011A and Run2011B Run2012B and Run2012C Run2015D The data are reprocessed several times, and it is the last complete reprocessing available at the time of the release which is made public. Data format \u00b6 The data format in use for Run1 data is Analysis Object Data (AOD). Starting from Run2, a slimmer version of this format called MINIAOD is used. A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \". References \u00b6 G. Franzoni: Dataset definition for CMS operations and physics analyses CR2014_311.pdf","title":"Collision Data"},{"location":"analysis/datasim/collisiondata/#collision-data","text":"Warning This page is under construction The CMS collision data is organized in primary datasets (PD). All CMS open data primary datasets can be found with this search . The dataset name consists of three parts separated by \"/\", e.g.: /TauPlusX/Run2011A-12Oct2013-v1/AOD The first part indicates the primary dataset contents ( TauPlusX ), the second part is the data-taking era ( Run2011A ) and reprocessing ( 12Oct2013 ), and the last one indicates the data format ( AOD ).","title":"Collision Data"},{"location":"analysis/datasim/collisiondata/#dataset-contents","text":"The primary dataset definition is centered around physics objects (SingleMu, Jet, Tau etc). Events triggered by High Level Triggers (HLT) with a similar physics contents or use are mostly directed in the same PD. This guide gives an overview of the CMS trigger system. Besides requirements on the physics content, the organisation of the primary datasets has to satisfy constraints related to the data processing and handling, such as the average event rate approximately uniform across the different PDs, and the event rate within a certain range. Each CMS collision dataset comes with a brief description of the contents, and the full listing of all possible HLT trigger streams included in the dataset. The instructions how to find the exact definitions and parameters of the HLT trigger definitions can be found in Guide to the CMS Trigger System under \" HLT Trigger Path definitions \". Since a given event can pass more than one HLT path, it can be included in more than one primary dataset. There's an overall overlap between the PDs of around 25-35% during Run1 and it must be taken into account when combining events from different datasets in an analysis.","title":"Dataset contents"},{"location":"analysis/datasim/collisiondata/#data-taking-and-reprocessing","text":"One year of data taking is divided in several \"eras\" indicated as RunA, RunB, etc. According to the CMS data policy, 50% of data is published after the embargo period, completed with the full release within 10 years. Currently available are Run2010A and Run2010B Run2011A and Run2011B Run2012B and Run2012C Run2015D The data are reprocessed several times, and it is the last complete reprocessing available at the time of the release which is made public.","title":"Data taking and reprocessing"},{"location":"analysis/datasim/collisiondata/#data-format","text":"The data format in use for Run1 data is Analysis Object Data (AOD). Starting from Run2, a slimmer version of this format called MINIAOD is used. A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"Data format"},{"location":"analysis/datasim/collisiondata/#references","text":"G. Franzoni: Dataset definition for CMS operations and physics analyses CR2014_311.pdf","title":"References"},{"location":"analysis/datasim/eventgeneration/","text":"Event Generation \u00b6 Warning This page is under construction Physical event generation and detector simulation are the first steps in producing Monte Carlo samples suitable for physical analysis. Here we will teach you how to use the CMS datasets in the CERN Open Data Portal and the CMSSW machinery for the generation of events in simple steps: Generation and Simulation: To simulate beam collisions. Triggers: To simulate the effect of the detectors and electronics. Reconstruction: For the reconstruction of the events in the collisions. What you will find here: Virtual machines Dataset name System details Configuration files cmsDriver Generation from Matrix Element (ME) generators LHE Simulation High Level Trigger (HLT) Reconstruction Generation from general-purpose generators Generation and Simulation High Level Trigger (HLT) Reconstruction Example for event generation with 2011 CMSSW machinery Example for event generation with 2012 CMSSW machinery Virtual machines \u00b6 A specific CMS virtual machine includes the ROOT framework and CMSSW. Follow these instructions to configure a CERN virtual machine on your computer to be used with the 2011 and 2012 CMS open data. Dataset name \u00b6 When exploring a simulated dataset on the CERN Open Data Portal , the first thing you will see is the name of the dataset. CMS uses the following naming convention : PROCESS_RANGETYPE-RANGELOWtoRANGEHIGH_FILTER_TUNE_COMMENT_COMENERGY-GENERATOR Take as an example the name of record 12201 : QCD_Pt-15to3000_TuneZ2star_Flat_8TeV_pythia6 System details \u00b6 In the record of each dataset, you can find the recommended global tag and release for analysis (CMSSW is the data analysis library). A global tag stores additional data that is required by the reconstruction and analysis software. Take as an example section System details of record 12201 : Recommended global tag for analysis: START53_V27 Recommended release for analysis: CMSSW_5_3_32 Configuration files \u00b6 The CMS software framework uses a software bus model, where data is stored in the event which is passed to a series of modules. A single executable, cmsRun , is used, and the modules are loaded at runtime. A configuration file defines which modules are loaded, in which order they are run, and with which configurable parameters they are run. You can find the configuration files for the generation of events for each dataset in its respective record within the CERN Open Data Portal . Check, for example, the section How were these data generated? of record 12201 . cmsDriver \u00b6 The cmsDriver is a tool to create production-solid configuration files from minimal command line options. Its code implementation, the cmsDriver.py script, is part of the CMSSW software. A summary of the cmsDriver.py script's options with a detailed message about each one can be visualized by getting the help: cmsDriver.py --help Generation from Matrix Element (ME) generators \u00b6 Generator-level datasets can be produced using a Matrix Element (ME) generator (e.g., Powheg , MadGraph5_aMCatNLO , Alpgen ) to deliver the event at the parton level and then a general-purpose generator to hadronise the event. Here we will reproduce the steps in the generation of record 1352 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MySim LHE \u00b6 The Les Houches Event file format ( LHE ) is an agreement between Monte Carlo event generators and theorists to define Matrix Element level event listings in a common language. The LHE input file that store process and event information can be one generated by you or you can look for examples in /eos/cms/store/lhe/ . Here we will use a file with events generated for record 1352 : cmsDriver.py step1 --filein lhe:10270 --fileout file:LHE.root --mc --eventcontent LHE --datatier GEN --conditions START53_LV6A1::All --step NONE --python_filename LHE.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Run the CMSSW executable: cmsRun LHE.py Simulation \u00b6 The next step is to generate fully hadronised events. We need to use the appropriate configuration file for this purpose. Take as an example the file in Step SIM for the simulation of record 1352 . The configuration file is in this link . We add this file to our local area: curl http://uaf-10.t2.ucsd.edu/~phchang/analysis/generator/genproductions/python/SevenTeV/Hadronizer_TuneZ2_7TeV_generic_LHE_pythia_tauola_cff.py -o MySim/python/mysim.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MySim/python/mysim.py --filein file:LHE.root --fileout file:sim.root --mc --eventcontent RAWSIM --customise SimG4Core/Application/reproc2011_2012_cff.customiseG4,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START53_LV6A1::All --beamspot Realistic7TeV2011CollisionV2 --step GEN,SIM --datamix NODATAMIXER --python_filename sim.py --no_exec -n 3 Run the CMSSW executable: cmsRun sim.py High Level Trigger (HLT) \u00b6 It is a crucial part of the CMS data flow since it is the HLT algorithms and filters which will decide whether an event should be kept for an offline analysis: any offline analysis depends on the outcome of HLT. Execute the cmsDriver command as: cmsDriver.py step1 --filein file:sim.root --fileout file:hlt.root --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --datatier GEN-RAW --conditions START53_LV6A1::All --step DIGI,L1,DIGI2RAW,HLT:2011 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun hlt.py Reconstruction \u00b6 The algorithms that make up the CMS event reconstruction software build physics objects (e.g., muons, electrons, jets) from the raw data recorded by the detector. All events collected by the CMS trigger system are reconstructed by the CMS prompt reconstruction system soon after being collected. Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_LV6A1::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created. Generation from general-purpose generators \u00b6 Generator-level datasets can be produced using a general-purpose generator (e.g., Pythia , Herwig , Tauola ) to simulate the event and the hadronisation. Here we will reproduce the steps in the generation of record 12201 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MyGen Generation and Simulation \u00b6 We need to use the appropriate configuration file. Take as an example the file in Step SIM for the generation and simulation of record 12201 . The configuration file is in this link . We add this file to our local area: curl https://raw.githubusercontent.com/cms-sw/genproductions/master/python/EightTeV/QCD_Pt/QCD_Pt_15to3000_TuneZ2star_Flat_8TeV_pythia6_cff.py -o MyGen/python/mygen.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MyGen/python/mygen.py --fileout file:gen.root --mc --eventcontent RAWSIM --pileup NoPileUp --customise Configuration/StandardSequences/SimWithCastor_cff.customise,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START50_V13::All --beamspot Realistic8TeVCollision --step GEN,SIM --datamix NODATAMIXER --python_filename gen.py --no_exec -n 3 Run the CMSSW executable: cmsRun gen.py High Level Trigger (HLT) \u00b6 Execute the cmsDriver command as: cmsDriver.py step1 --filein file:gen.root --fileout file:hlt.root --pileup_input dbs:/MinBias_TuneZ2star_8TeV-pythia6/Summer12-START50_V13-v3/GEN-SIM --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --pileup fromDB --datatier GEN-SIM-RAW --conditions START53_V7N::All --step DIGI,L1,DIGI2RAW,HLT:7E33v2 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 In section How were these data generated? of the record, you can find the pile-up dataset. Additionally, you can manually add ROOT files to the hlt.py file for the pile-up configuration by looking at the list of ROOT files that were used in the Step HLT configuration file of the record you are studying. This involves, for instance, opening file hlt.py and replacing the line process.mix.input.fileNames = cms.untracked.vstring([]) with process.mix.input.fileNames = cms.untracked.vstring([ 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/005825F1-F260-E111-BD97-003048C692DA.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/003EEBD4-8061-E111-9A23-003048D437F2.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/0005E496-3661-E111-B31E-003048F0E426.root']) Now, run the CMSSW executable: cmsRun hlt.py Reconstruction \u00b6 Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_V7N::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created. Example for event generation with 2011 CMSSW machinery \u00b6 In this example , you will learn how to generate 2011 MC Drell-Yan events from scratch. A Drell-Yan process occurs when a quark and an antiquark annihilate, creating a virtual photon or Z boson, which then decays into a pair of oppositely charged leptons. Example for event generation with 2012 CMSSW machinery \u00b6 In this example , you will learn how to generate 2012 MC QCD events, which involve the strong interaction between quarks and gluons. Additionally, you will know what are the steps to extract the tracking information of these events.","title":"Event Generation"},{"location":"analysis/datasim/eventgeneration/#event-generation","text":"Warning This page is under construction Physical event generation and detector simulation are the first steps in producing Monte Carlo samples suitable for physical analysis. Here we will teach you how to use the CMS datasets in the CERN Open Data Portal and the CMSSW machinery for the generation of events in simple steps: Generation and Simulation: To simulate beam collisions. Triggers: To simulate the effect of the detectors and electronics. Reconstruction: For the reconstruction of the events in the collisions. What you will find here: Virtual machines Dataset name System details Configuration files cmsDriver Generation from Matrix Element (ME) generators LHE Simulation High Level Trigger (HLT) Reconstruction Generation from general-purpose generators Generation and Simulation High Level Trigger (HLT) Reconstruction Example for event generation with 2011 CMSSW machinery Example for event generation with 2012 CMSSW machinery","title":"Event Generation"},{"location":"analysis/datasim/eventgeneration/#virtual-machines","text":"A specific CMS virtual machine includes the ROOT framework and CMSSW. Follow these instructions to configure a CERN virtual machine on your computer to be used with the 2011 and 2012 CMS open data.","title":"Virtual machines"},{"location":"analysis/datasim/eventgeneration/#dataset-name","text":"When exploring a simulated dataset on the CERN Open Data Portal , the first thing you will see is the name of the dataset. CMS uses the following naming convention : PROCESS_RANGETYPE-RANGELOWtoRANGEHIGH_FILTER_TUNE_COMMENT_COMENERGY-GENERATOR Take as an example the name of record 12201 : QCD_Pt-15to3000_TuneZ2star_Flat_8TeV_pythia6","title":"Dataset name"},{"location":"analysis/datasim/eventgeneration/#system-details","text":"In the record of each dataset, you can find the recommended global tag and release for analysis (CMSSW is the data analysis library). A global tag stores additional data that is required by the reconstruction and analysis software. Take as an example section System details of record 12201 : Recommended global tag for analysis: START53_V27 Recommended release for analysis: CMSSW_5_3_32","title":"System details"},{"location":"analysis/datasim/eventgeneration/#configuration-files","text":"The CMS software framework uses a software bus model, where data is stored in the event which is passed to a series of modules. A single executable, cmsRun , is used, and the modules are loaded at runtime. A configuration file defines which modules are loaded, in which order they are run, and with which configurable parameters they are run. You can find the configuration files for the generation of events for each dataset in its respective record within the CERN Open Data Portal . Check, for example, the section How were these data generated? of record 12201 .","title":"Configuration files"},{"location":"analysis/datasim/eventgeneration/#cmsdriver","text":"The cmsDriver is a tool to create production-solid configuration files from minimal command line options. Its code implementation, the cmsDriver.py script, is part of the CMSSW software. A summary of the cmsDriver.py script's options with a detailed message about each one can be visualized by getting the help: cmsDriver.py --help","title":"cmsDriver"},{"location":"analysis/datasim/eventgeneration/#generation-from-matrix-element-me-generators","text":"Generator-level datasets can be produced using a Matrix Element (ME) generator (e.g., Powheg , MadGraph5_aMCatNLO , Alpgen ) to deliver the event at the parton level and then a general-purpose generator to hadronise the event. Here we will reproduce the steps in the generation of record 1352 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MySim","title":"Generation from Matrix Element (ME) generators"},{"location":"analysis/datasim/eventgeneration/#lhe","text":"The Les Houches Event file format ( LHE ) is an agreement between Monte Carlo event generators and theorists to define Matrix Element level event listings in a common language. The LHE input file that store process and event information can be one generated by you or you can look for examples in /eos/cms/store/lhe/ . Here we will use a file with events generated for record 1352 : cmsDriver.py step1 --filein lhe:10270 --fileout file:LHE.root --mc --eventcontent LHE --datatier GEN --conditions START53_LV6A1::All --step NONE --python_filename LHE.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Run the CMSSW executable: cmsRun LHE.py","title":"LHE"},{"location":"analysis/datasim/eventgeneration/#simulation","text":"The next step is to generate fully hadronised events. We need to use the appropriate configuration file for this purpose. Take as an example the file in Step SIM for the simulation of record 1352 . The configuration file is in this link . We add this file to our local area: curl http://uaf-10.t2.ucsd.edu/~phchang/analysis/generator/genproductions/python/SevenTeV/Hadronizer_TuneZ2_7TeV_generic_LHE_pythia_tauola_cff.py -o MySim/python/mysim.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MySim/python/mysim.py --filein file:LHE.root --fileout file:sim.root --mc --eventcontent RAWSIM --customise SimG4Core/Application/reproc2011_2012_cff.customiseG4,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START53_LV6A1::All --beamspot Realistic7TeV2011CollisionV2 --step GEN,SIM --datamix NODATAMIXER --python_filename sim.py --no_exec -n 3 Run the CMSSW executable: cmsRun sim.py","title":"Simulation"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt","text":"It is a crucial part of the CMS data flow since it is the HLT algorithms and filters which will decide whether an event should be kept for an offline analysis: any offline analysis depends on the outcome of HLT. Execute the cmsDriver command as: cmsDriver.py step1 --filein file:sim.root --fileout file:hlt.root --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --datatier GEN-RAW --conditions START53_LV6A1::All --step DIGI,L1,DIGI2RAW,HLT:2011 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun hlt.py","title":"High Level Trigger (HLT)"},{"location":"analysis/datasim/eventgeneration/#reconstruction","text":"The algorithms that make up the CMS event reconstruction software build physics objects (e.g., muons, electrons, jets) from the raw data recorded by the detector. All events collected by the CMS trigger system are reconstructed by the CMS prompt reconstruction system soon after being collected. Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_LV6A1::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created.","title":"Reconstruction"},{"location":"analysis/datasim/eventgeneration/#generation-from-general-purpose-generators","text":"Generator-level datasets can be produced using a general-purpose generator (e.g., Pythia , Herwig , Tauola ) to simulate the event and the hadronisation. Here we will reproduce the steps in the generation of record 12201 . Guided by the system details specified in the dataset, you should start by setting up your run time environment: cmsrel CMSSW_5_3_32 cd CMSSW_5_3_32/src/ cmsenv We will create a package according to our dataset: mkdir MyPackage cd MyPackage mkedanlzr MyGen","title":"Generation from general-purpose generators"},{"location":"analysis/datasim/eventgeneration/#generation-and-simulation","text":"We need to use the appropriate configuration file. Take as an example the file in Step SIM for the generation and simulation of record 12201 . The configuration file is in this link . We add this file to our local area: curl https://raw.githubusercontent.com/cms-sw/genproductions/master/python/EightTeV/QCD_Pt/QCD_Pt_15to3000_TuneZ2star_Flat_8TeV_pythia6_cff.py -o MyGen/python/mygen.py Compile everything: scram b Execute the cmsDriver command as: cmsDriver.py MyPackage/MyGen/python/mygen.py --fileout file:gen.root --mc --eventcontent RAWSIM --pileup NoPileUp --customise Configuration/StandardSequences/SimWithCastor_cff.customise,Configuration/DataProcessing/Utils.addMonitoring --datatier GEN-SIM --conditions START50_V13::All --beamspot Realistic8TeVCollision --step GEN,SIM --datamix NODATAMIXER --python_filename gen.py --no_exec -n 3 Run the CMSSW executable: cmsRun gen.py","title":"Generation and Simulation"},{"location":"analysis/datasim/eventgeneration/#high-level-trigger-hlt_1","text":"Execute the cmsDriver command as: cmsDriver.py step1 --filein file:gen.root --fileout file:hlt.root --pileup_input dbs:/MinBias_TuneZ2star_8TeV-pythia6/Summer12-START50_V13-v3/GEN-SIM --mc --eventcontent RAWSIM --runsScenarioForMC Run2012_AB_C_D_oneRunPerEra --pileup fromDB --datatier GEN-SIM-RAW --conditions START53_V7N::All --step DIGI,L1,DIGI2RAW,HLT:7E33v2 --python_filename hlt.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 In section How were these data generated? of the record, you can find the pile-up dataset. Additionally, you can manually add ROOT files to the hlt.py file for the pile-up configuration by looking at the list of ROOT files that were used in the Step HLT configuration file of the record you are studying. This involves, for instance, opening file hlt.py and replacing the line process.mix.input.fileNames = cms.untracked.vstring([]) with process.mix.input.fileNames = cms.untracked.vstring([ 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/005825F1-F260-E111-BD97-003048C692DA.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/003EEBD4-8061-E111-9A23-003048D437F2.root', 'root://eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12/MinBias_TuneZ2star_8TeV-pythia6/GEN-SIM/START50_V13-v3/0000/0005E496-3661-E111-B31E-003048F0E426.root']) Now, run the CMSSW executable: cmsRun hlt.py","title":"High Level Trigger (HLT)"},{"location":"analysis/datasim/eventgeneration/#reconstruction_1","text":"Execute the cmsDriver command as: cmsDriver.py step2 --filein file:hlt.root --fileout file:reco.root --mc --eventcontent AODSIM,DQM --datatier AODSIM,DQM --conditions START53_V7N::All --step RAW2DIGI,L1Reco,RECO,VALIDATION:validation_prod,DQM:DQMOfflinePOGMC --python_filename reco.py --no_exec --customise Configuration/DataProcessing/Utils.addMonitoring -n 3 Now, run the CMSSW executable: cmsRun reco.py You can start ROOT and type TBrowser t to explore the files that were created.","title":"Reconstruction"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2011-cmssw-machinery","text":"In this example , you will learn how to generate 2011 MC Drell-Yan events from scratch. A Drell-Yan process occurs when a quark and an antiquark annihilate, creating a virtual photon or Z boson, which then decays into a pair of oppositely charged leptons.","title":"Example for event generation with 2011 CMSSW machinery"},{"location":"analysis/datasim/eventgeneration/#example-for-event-generation-with-2012-cmssw-machinery","text":"In this example , you will learn how to generate 2012 MC QCD events, which involve the strong interaction between quarks and gluons. Additionally, you will know what are the steps to extract the tracking information of these events.","title":"Example for event generation with 2012 CMSSW machinery"},{"location":"analysis/datasim/mcsimulations/","text":"Monte Carlo Simulations \u00b6 A set of simulated data (Monte Carlo - MC) corresponding to the collision data is made available. All directly available MC datasets can be found with this search . For 2012 data taking, large amount of MC, thought to be of less frequent use, is available on demand and included in search results if \" include on-demand datasets \" option is selected. MC dataset are searchable by categories , which can be found under \"Filter by category\" on the left bar of the search page. The dataset name consists of three parts separated by / e.g.: /DYToMuMu_M-15To50_Tune4C_8TeV-pythia8/Summer12_DR53X-PU_S10_START53_V19-v1/AODSIM The first part indicates the simulated physics process ( DYToMuMu ), some of the production parameters ( M-15To50_Tune4C ), collision energy ( 8TeV ), and the event generator used in the processing chain. CMS simulated datasets names gives more details in the naming. The second part is the production campaign ( Summer12_DR53X ), pile-up profile ( PU_S10 ) and processing conditions ( START53_V19 ), and the last one indicates the data format ( AODSIM ). Dataset contents \u00b6 The dataset naming reflects the contents of the dataset, and the actual generator parameters with which the dataset contents have been defined can be found as explained under \" Finding the generator parameters \" in the CMS Monte Carlo production overview . Processing \u00b6 CMS Monte Carlo production overview briefly describes the steps in the MC production chain. Data format \u00b6 The data format in use for Run1 MC data is Analysis Object Data (AODSIM). Starting from Run2, a slimmer version of this format called MINIAODSIM is used. A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"MC Simulations"},{"location":"analysis/datasim/mcsimulations/#monte-carlo-simulations","text":"A set of simulated data (Monte Carlo - MC) corresponding to the collision data is made available. All directly available MC datasets can be found with this search . For 2012 data taking, large amount of MC, thought to be of less frequent use, is available on demand and included in search results if \" include on-demand datasets \" option is selected. MC dataset are searchable by categories , which can be found under \"Filter by category\" on the left bar of the search page. The dataset name consists of three parts separated by / e.g.: /DYToMuMu_M-15To50_Tune4C_8TeV-pythia8/Summer12_DR53X-PU_S10_START53_V19-v1/AODSIM The first part indicates the simulated physics process ( DYToMuMu ), some of the production parameters ( M-15To50_Tune4C ), collision energy ( 8TeV ), and the event generator used in the processing chain. CMS simulated datasets names gives more details in the naming. The second part is the production campaign ( Summer12_DR53X ), pile-up profile ( PU_S10 ) and processing conditions ( START53_V19 ), and the last one indicates the data format ( AODSIM ).","title":"Monte Carlo Simulations"},{"location":"analysis/datasim/mcsimulations/#dataset-contents","text":"The dataset naming reflects the contents of the dataset, and the actual generator parameters with which the dataset contents have been defined can be found as explained under \" Finding the generator parameters \" in the CMS Monte Carlo production overview .","title":"Dataset contents"},{"location":"analysis/datasim/mcsimulations/#processing","text":"CMS Monte Carlo production overview briefly describes the steps in the MC production chain.","title":"Processing"},{"location":"analysis/datasim/mcsimulations/#data-format","text":"The data format in use for Run1 MC data is Analysis Object Data (AODSIM). Starting from Run2, a slimmer version of this format called MINIAODSIM is used. A brief description of data formats can be found in the introductory About CMS under \" Primary and simulated datasets \".","title":"Data format"},{"location":"analysis/interpretation/limits/","text":"Upper-limit calculations \u00b6 Warning This page is under construction","title":"Upper-limit Calculations"},{"location":"analysis/interpretation/limits/#upper-limit-calculations","text":"Warning This page is under construction","title":"Upper-limit calculations"},{"location":"analysis/interpretation/stats/","text":"Statistics \u00b6 Warning This page is under construction","title":"Statistics"},{"location":"analysis/interpretation/stats/#statistics","text":"Warning This page is under construction","title":"Statistics"},{"location":"analysis/luminosity/lumi/","text":"Luminosity \u00b6 Warning This page is under construction","title":"Luminosity"},{"location":"analysis/luminosity/lumi/#luminosity","text":"Warning This page is under construction","title":"Luminosity"},{"location":"analysis/selection/objectid/","text":"Object ID \u00b6 Warning This page is under construction","title":"Physics Object ID"},{"location":"analysis/selection/objectid/#object-id","text":"Warning This page is under construction","title":"Object ID"},{"location":"analysis/selection/objects/","text":"Physics Objects \u00b6 Warning This page is under construction Description \u00b6 The CMS is a giant detector that acts like a camera that \"photographs\" particle collisions, allowing us to interpret their nature. Certainly we cannot directly observe all the particles created in the collisions because some of them decay very quickly or simply do not interact with our detector. However, we can infer their presence. If they decay to other stable particles and interact with the apparatus, they leave signals in the CMS subdetectors. These signals are used to reconstruct the decay products or infer their presence; we call these physics objects . These objects could be electrons, muons, jets, missing energy, etc., but also lower level objects like tracks. For the current releases of open data, we store them in ROOT files following the EDM data model in AOD format. In the CERN Open Portal site one can find a description of these physical objects and a list of them corresponding to 2010 and 2011/2012 releases of open data. For Run 2 data from 2015, a detailed listing is available in the CMS WorkBook. DataFormats \u00b6 Run 1 Data Run 2 Data As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are most commonly obtained from the reco::Muon collection. The AOD Data Format Table gives a good description of the different collections (or data formats) for the AOD tier. Unfortunately, the links for the containers column got broken after CMSSW was moved to Github. Those links would have pointed us to the corresponding CMSSW C++ classes associated with those containers. This is important because one needs to know which CMSSW class matches a given collection of objects to include the headers of those classes in the header of your analyzer code. But let that not let us down. Fortunately, the names of the collections containers actually match the name of its associated CMSSW classes. These classes (data format classes) live under the DataFormats directory in CMSSW. If we browse through, we find the MuonReco package. In its interface area we find the DataFormats/MuonReco/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. This is corroborated by this Muon Analysis Twiki section . Remember When accessing a specific piece of code in the CMSSW github repository, and want to explore its methods, variables, etc., make sure you select the right git branch. E.g., CMSSW_5_3_X for 2011/2012 open data. In addition to this base class, sometimes it is necessary to invoke other auxiliary classes. For instance, DataFormats/MuonReco/interface/MuonFwd.h , which can be found in the same interface area. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following lines: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\" #include \"DataFormats/MuonReco/interface/MuonFwd.h\" As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are obtained from the C++ class std::vector<pat::Muon> (most often called pat::MuonCollection using its definition in the muon class ). The MINIAOD physics objects table gives a good description of the different collections (or data formats) for the MINIAOD tier. This is the muon entry in that table: The MINIAOD data format classes live under the DataFormats/PatCandidates/interface directory in CMSSW. Here we find the DataFormats/PatCandidates/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following line: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\" Access methods \u00b6 In the Event methods for data access section of the Getting Data From an Event Twiki page, one can find a complete description of the different methods available for Event data access. Remember When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. Run 1 Data Run 2 Data As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. As an example, during Run 1, the recommended method was the getByLabel one. This method needed an InputTag . This can also be extracted from the AOD Data Format Table . The first column indicate the InputTag: Therefore, in the context of this muon example, in the analyze method of your EDAnalyzer you should include the following lines: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muons\" , mymuons ); If you required cosmic muons, for some reason, you would need instead: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muonsFromCosmics\" , mymuons ); Alternatively, it would be also possible to retrieve the InputTag name from configuration . In that case, in your configuration file you would need something like: process . demo = cms . EDAnalyzer ( 'MuonAnalyzer' , InputCollection = cms . InputTag ( \"muons\" ) ) In this case, you would need to declare the appropriate input tag in your EDAnalyzer class: //declare the input tag for MuonCollection edm :: InputTag muonInput ; Extract it from the ParameterSet in the constructor MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) { //now do what ever initialization is needed muonInput = iConfig . getParameter < edm :: InputTag > ( \"InputCollection\" ); } and use in the analyze routine: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( muonInput , mymuons ); As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. For Run 2 data, the method is getByToken . This method needs a \"token\" and an InputTag , which will pass the name of the collection to the analyzed. This name is indicated as \"Label\" the MINIAOD table , slimmedMuons for muons. The InputTag name is defined in the configuration . Your configuration file would have: process . mymuons = cms . EDAnalyzer ( 'MuonAnalyzer' , muons = cms . InputTag ( \"slimmedMuons\" ) ) In this case, you would need to declare the appropriate token in your EDAnalyzer class: //declare the token for MuonCollection edm :: EDGetTokenT < pat :: MuonCollection > muonToken_ ; which is then passed with the Input Tag to the constructor of the EDAnalyzer class MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) : muonToken_ ( consumes < pat :: MuonCollection > ( iConfig . getParameter < edm :: InputTag > ( \"muons\" ))) { //now do what ever initialization is needed and used in the analyze routine: Handle < pat :: MuonCollection > muons ; iEvent . getByToken ( muonToken_ , muons ); Additional information for accessing CMS physics objects \u00b6 In Chapter 7 of the CMS Workbook one can find Analysis pages that provide additional information, which can be useful to check on top of the general strategy for accessing objects that was discussed above.","title":"Physics Objects"},{"location":"analysis/selection/objects/#physics-objects","text":"Warning This page is under construction","title":"Physics Objects"},{"location":"analysis/selection/objects/#description","text":"The CMS is a giant detector that acts like a camera that \"photographs\" particle collisions, allowing us to interpret their nature. Certainly we cannot directly observe all the particles created in the collisions because some of them decay very quickly or simply do not interact with our detector. However, we can infer their presence. If they decay to other stable particles and interact with the apparatus, they leave signals in the CMS subdetectors. These signals are used to reconstruct the decay products or infer their presence; we call these physics objects . These objects could be electrons, muons, jets, missing energy, etc., but also lower level objects like tracks. For the current releases of open data, we store them in ROOT files following the EDM data model in AOD format. In the CERN Open Portal site one can find a description of these physical objects and a list of them corresponding to 2010 and 2011/2012 releases of open data. For Run 2 data from 2015, a detailed listing is available in the CMS WorkBook.","title":"Description"},{"location":"analysis/selection/objects/#dataformats","text":"Run 1 Data Run 2 Data As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are most commonly obtained from the reco::Muon collection. The AOD Data Format Table gives a good description of the different collections (or data formats) for the AOD tier. Unfortunately, the links for the containers column got broken after CMSSW was moved to Github. Those links would have pointed us to the corresponding CMSSW C++ classes associated with those containers. This is important because one needs to know which CMSSW class matches a given collection of objects to include the headers of those classes in the header of your analyzer code. But let that not let us down. Fortunately, the names of the collections containers actually match the name of its associated CMSSW classes. These classes (data format classes) live under the DataFormats directory in CMSSW. If we browse through, we find the MuonReco package. In its interface area we find the DataFormats/MuonReco/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. This is corroborated by this Muon Analysis Twiki section . Remember When accessing a specific piece of code in the CMSSW github repository, and want to explore its methods, variables, etc., make sure you select the right git branch. E.g., CMSSW_5_3_X for 2011/2012 open data. In addition to this base class, sometimes it is necessary to invoke other auxiliary classes. For instance, DataFormats/MuonReco/interface/MuonFwd.h , which can be found in the same interface area. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following lines: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\" #include \"DataFormats/MuonReco/interface/MuonFwd.h\" As one can see in those guides, these physical objects are usually stored in specific collections . For instance, muons are obtained from the C++ class std::vector<pat::Muon> (most often called pat::MuonCollection using its definition in the muon class ). The MINIAOD physics objects table gives a good description of the different collections (or data formats) for the MINIAOD tier. This is the muon entry in that table: The MINIAOD data format classes live under the DataFormats/PatCandidates/interface directory in CMSSW. Here we find the DataFormats/PatCandidates/interface/Muon.h class header, which is the one we would need to incorporate in our analyzer. So, in the context of this example, in order to support muons information, at the top of your EDAnalyzer you should include the following line: //classes to extract Muon information #include \"DataFormats/MuonReco/interface/Muon.h\"","title":"DataFormats"},{"location":"analysis/selection/objects/#access-methods","text":"In the Event methods for data access section of the Getting Data From an Event Twiki page, one can find a complete description of the different methods available for Event data access. Remember When accessing the CMS twiki pages we will usually point you to the most recent page. However, historical Twiki documentation, i.e., earlier revision of the pages, may provide more accurate information for open data that is already a few years old. One can access this historical archive by going to the bottom of any Twiki page, clicking on History and exploring the revisions closer to the open data release year. Run 1 Data Run 2 Data As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. As an example, during Run 1, the recommended method was the getByLabel one. This method needed an InputTag . This can also be extracted from the AOD Data Format Table . The first column indicate the InputTag: Therefore, in the context of this muon example, in the analyze method of your EDAnalyzer you should include the following lines: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muons\" , mymuons ); If you required cosmic muons, for some reason, you would need instead: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( \"muonsFromCosmics\" , mymuons ); Alternatively, it would be also possible to retrieve the InputTag name from configuration . In that case, in your configuration file you would need something like: process . demo = cms . EDAnalyzer ( 'MuonAnalyzer' , InputCollection = cms . InputTag ( \"muons\" ) ) In this case, you would need to declare the appropriate input tag in your EDAnalyzer class: //declare the input tag for MuonCollection edm :: InputTag muonInput ; Extract it from the ParameterSet in the constructor MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) { //now do what ever initialization is needed muonInput = iConfig . getParameter < edm :: InputTag > ( \"InputCollection\" ); } and use in the analyze routine: Handle < reco :: MuonCollection > mymuons ; iEvent . getByLabel ( muonInput , mymuons ); As indicated in that page, all Event data access methods use the edm::Handle<T> , where T is the C++ type of the requested object, to hold the result of an access. For Run 2 data, the method is getByToken . This method needs a \"token\" and an InputTag , which will pass the name of the collection to the analyzed. This name is indicated as \"Label\" the MINIAOD table , slimmedMuons for muons. The InputTag name is defined in the configuration . Your configuration file would have: process . mymuons = cms . EDAnalyzer ( 'MuonAnalyzer' , muons = cms . InputTag ( \"slimmedMuons\" ) ) In this case, you would need to declare the appropriate token in your EDAnalyzer class: //declare the token for MuonCollection edm :: EDGetTokenT < pat :: MuonCollection > muonToken_ ; which is then passed with the Input Tag to the constructor of the EDAnalyzer class MuonAnalyzer :: MuonAnalyzer ( const edm :: ParameterSet & iConfig ) : muonToken_ ( consumes < pat :: MuonCollection > ( iConfig . getParameter < edm :: InputTag > ( \"muons\" ))) { //now do what ever initialization is needed and used in the analyze routine: Handle < pat :: MuonCollection > muons ; iEvent . getByToken ( muonToken_ , muons );","title":"Access methods"},{"location":"analysis/selection/objects/#additional-information-for-accessing-cms-physics-objects","text":"In Chapter 7 of the CMS Workbook one can find Analysis pages that provide additional information, which can be useful to check on top of the general strategy for accessing objects that was discussed above.","title":"Additional information for accessing CMS physics objects"},{"location":"analysis/selection/triggers/","text":"Triggers \u00b6 Warning This page is under construction","title":"Triggers"},{"location":"analysis/selection/triggers/#triggers","text":"Warning This page is under construction","title":"Triggers"},{"location":"analysis/selection/idefficiencystudy/signalextraction/","text":"Signal Extraction \u00b6 Detector reconstruction efficiencies are calculated using signal muons, that is, only true candidates decaying to dimuons. This is achieved in this study by extracting signal from the data by the usage of some methods. Here it is presented two: sideband subtraction and fitting. Sideband subtraction method \u00b6 The sideband subtraction method involves choosing sideband and signal regions in invariant mass distribution for each tag+probe pair. The signal region is selected by finding the ressonance position and defining a region around it. While the signal region contains both signal and background, the sideband region is chosen such as to have only background, with a distance from signal region. A example of those regions selection can be seen below for the J/\u03c8 ressonance. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty: Applying those equations we get histograms like this: Solid blue line (Total) = particles in signal region; Dashed blue line (Background) = particles in sideband regions; Solid magenta line (signal) = signal histogram subtracted. Fitting method \u00b6 In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.","title":"Signal extraction"},{"location":"analysis/selection/idefficiencystudy/signalextraction/#signal-extraction","text":"Detector reconstruction efficiencies are calculated using signal muons, that is, only true candidates decaying to dimuons. This is achieved in this study by extracting signal from the data by the usage of some methods. Here it is presented two: sideband subtraction and fitting.","title":"Signal Extraction"},{"location":"analysis/selection/idefficiencystudy/signalextraction/#sideband-subtraction-method","text":"The sideband subtraction method involves choosing sideband and signal regions in invariant mass distribution for each tag+probe pair. The signal region is selected by finding the ressonance position and defining a region around it. While the signal region contains both signal and background, the sideband region is chosen such as to have only background, with a distance from signal region. A example of those regions selection can be seen below for the J/\u03c8 ressonance. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty: Applying those equations we get histograms like this: Solid blue line (Total) = particles in signal region; Dashed blue line (Background) = particles in sideband regions; Solid magenta line (signal) = signal histogram subtracted.","title":"Sideband subtraction method"},{"location":"analysis/selection/idefficiencystudy/signalextraction/#fitting-method","text":"In this method, the signal is extracted not by histogram manipulation but by likelihood fitting. The procedure is applied after splitting the data in sub-samples, corresponding to bins of the kinematic variable of interest of the probe objects. As such, the efficiency will be measured as a function of that variable. Each sub-sample contains signal and background events; the signal is accessed by fitting the invariant mass spectra The fit for each bin allows to statistically discriminate between signal and background. In particular, the fit yields the number of signal events. The efficiency is finally obtained by simply forming the ratio of the signal yield from the fit to the passing category by the signal yield from the fit of the inclusive all category. This approach is illustrated below.","title":"Fitting method"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/","text":"Tag and Probe \u00b6 The Tag and Probe method is an experimental procedure commonly used in particle physics that allows to measure a process\u2019 efficiency directly from data. The procedure provides an unbiased sample of probe objects that can be then used to measure the efficiency of a particular selection criteria. Tag and Probe method \u00b6 This method is a data-driven technique and it is based on decays of known ressonances in pair of particles. The decaying muons are labeled according to the following criteria: Tag muon : well identified, triggered muon (tight selection criteria). Probe muon : unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the eciency is to be measured. Tag muon are employed to trigger the presence of a resonance decay while probe muons, paired to tag muons, will be used for getting efficiency due its' unbiased characteristic. CMS Efficiency \u00b6 The efficiency will be given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which is explained below): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria. CMS Muon identification and reconstruction \u00b6 In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, three reconstruction approaches are used: Tracker Muon reconstruction: all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates, and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction: all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes in the barrel region, Cathode Strip Chambers and Resistive Plates Chambers in the endcaps) are used to generate \u201cseeds\u201d consisting of position and direction vectors and an estimate of the muon transverse momentum; Global Muon reconstruction: starts from a Standalone reconstructed muon track and extrapolates its trajectory from the innermost muon station through the coil and both calorimeters to the outer tracker surface. These are illustrated below: Note You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"Tag and Probe"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#tag-and-probe","text":"The Tag and Probe method is an experimental procedure commonly used in particle physics that allows to measure a process\u2019 efficiency directly from data. The procedure provides an unbiased sample of probe objects that can be then used to measure the efficiency of a particular selection criteria.","title":"Tag and Probe"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#tag-and-probe-method","text":"This method is a data-driven technique and it is based on decays of known ressonances in pair of particles. The decaying muons are labeled according to the following criteria: Tag muon : well identified, triggered muon (tight selection criteria). Probe muon : unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the eciency is to be measured. Tag muon are employed to trigger the presence of a resonance decay while probe muons, paired to tag muons, will be used for getting efficiency due its' unbiased characteristic.","title":"Tag and Probe method"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#cms-efficiency","text":"The efficiency will be given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which is explained below): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria.","title":"CMS Efficiency"},{"location":"analysis/selection/idefficiencystudy/tagandprobe/#cms-muon-identification-and-reconstruction","text":"In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, three reconstruction approaches are used: Tracker Muon reconstruction: all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates, and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction: all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes in the barrel region, Cathode Strip Chambers and Resistive Plates Chambers in the endcaps) are used to generate \u201cseeds\u201d consisting of position and direction vectors and an estimate of the muon transverse momentum; Global Muon reconstruction: starts from a Standalone reconstructed muon track and extrapolates its trajectory from the innermost muon station through the coil and both calorimeters to the outer tracker surface. These are illustrated below: Note You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"CMS Muon identification and reconstruction"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/","text":"Overview of fitting method \u00b6 The fitting method folder is structured in folders and main files. Main files are those ones that are used to run the most important codes. Below is a list of folders presented in this method and the files encontered here. It is important to note that this code has been tested on root 6.22/00. Fitting method folder structure \u00b6 The folders contained in fitting method are described below. Folder Purpose \ud83d\udcc2 DATA Where .root with data should be placed for measuring efficiency \ud83d\udcc2 src Where important files related to main code are keeped \u2514 \ud83d\udcc2 dofits Here it keeps files that are responsible to do the fitting over invariant masses histograms \ud83d\udcc2 tests Some teste made during the development of this tool \ud83d\udcc2 results This folder stores the results output and it is created when any code finnish running Main files \u00b6 There are six main files in the fitting method. For simple results like the ones obtained in sideband subtraction method the file used is efficiency.cpp. Main files are explained below. \ud83d\udcc4 simplify_data.cpp \u00b6 The simplify_data.cpp file, as the name sugest, simplify a DATA file obtained from this Tag and Probe tool. It is necessary to simplify due RooFit limitations where fitting method codes here used are based on. There are two lines responsable for input and output file: TFile * file0 = TFile :: Open ( \"INPUT_FILE_PATH.root\" ); TFile * fileIO = TFile :: Open ( \"OUTPUT_FILE_PATH.root\" , \"RECREATE\" ); Every user should run this code firstly to simplify .root files in order to use it in fitting method. The input files here are provenient from the main Tag and Probe tool in this repository. If you want to get a new ntuple, you should run it. \ud83d\udcc4 efficiency.cpp \u00b6 This file is responsible to measure the efficiency simple by fitting method as described in this fitting method section. Choosing ressonance \u00b6 Here it include the file that is responsible to fit the ressonance and return the yield obtained with error. //Change if you need #include \"src/dofits/DoFit_Jpsi_Run.h\" By default out tool keeps all ressonance fit in the folder src/dofits . There are some example there for specific ressonances and fits. There are two main parameters to control this code. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; The string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. double bins[] is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on. Output \u00b6 There are two output folders in this file by default. They are defined in those lines of code: //Path where is going to save results png for every bin const char * path_bins_fit_folder = \"results/bins_fit/efficiency/\" ; path_bins_fit_folder refers to the path where each individual fit of bins will be stored as .png . In this folder you can find every fit made in this method. //Path where is going to save efficiency string directoryToSave = string ( \"results/efficiencies/efficiency/\" ) + output_folder_name + string ( \"/\" ); The directoryToSave stores the path to save the efficiency result. It is saved as a .root file containing passing and total histograms as well the efficiency result histogram. Informations about the output is printed at end of running. \ud83d\udcc4 loop_over_efficiencies.cpp \u00b6 The purpose of this code is rerun the efficiency.cpp for differents configurations. This code is not recommended for systematic calculations indeed and it was firstly created for systematic studies only. The importants variables to keep in mind are listed below Type Name Purpose double default_min the minimum invariant mass window postion double default_max the maximum invariant mass window postion bool should_loop_muon_id if true, it loops over all muons id (tracking, standalone, global) bool should_loop_settings if true, it loops over all settings presented in set_settings() function int setting if should_loop_settings is false, it uses only this setting number bool exactly This only affect the name of output plots inside .root . Its recommended to keep it set to false set_settings(...) \u00b6 It is one of four functions presented in this code. Its is called by: void set_settings ( int index , bool exactly = false ) Inside this function are preset settings that this file runs over. Each setting is associated with a number here named as index . This function is responsible to set the index configuration to the efficiency for running the efficiency.cpp file. loop_settings() \u00b6 void loop_settings () If should_loop_muon_id is true, this function is called. It loops over all muon ids: tracking, standalone, global. loop_muon_id() \u00b6 void loop_muon_id () If should_loop_settings is true, this function is called. It loops over all settings preset in set_settings(...) function. loop_over_efficiencies() \u00b6 void loop_over_efficiencies () It is the main function of this file. It is the function which calls every other function when it is needed. \ud83d\udcc4 plot_sys_efficiency.cpp \u00b6 The plot_sys_efficiency.cpp code creates a single .root with variations made. Unlike the previous code, the loop_over_efficiencies.cpp , that makes each source of uncertainty be in a separate .root, this one puts all of them in a single .root . This code has been further optimized than his precursor and also as a differential it already calculates the systematic uncertainty. Below it is specified main variables used in this code. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; The string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. double bins[] is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on. Inside plot_sys_efficiency() , there is some useful variables too: Type Name Purpose string path_bins_fit_folder Stores the path to the output folder where .png of fit for each bin made will be string directoryToSave Stores the path to output file \ud83d\udcc4 overplot_efficiencies.cpp \u00b6 The overplot_efficiencies.cpp code will take the results of the previous topic and make a single graph containing all its variations and will output a .png containing the graph. All main variables in this file are in overplot_efficiencies() function Type Name Purpose const char* input_folder_name Stores the path to input folder where .root is const char* output_folder_name Stores the path to output folder string MuonId It accepts values of \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" string quantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" Remeber when selecting MuonId and quantity to run plot_sys_efficiency.cpp before with same configurations. \ud83d\udcc4 plot_sys_efficiency_2d.cpp \u00b6 In order to calculate systematic uncertainties in 2D, it was necessary to create another code: the plot_sys_efficiency_2d.cpp . It has a .root output containing the efficiency histograms that can be viewed through the new TBrowser on root command. The variables in this file is shown below: Type Name Purpose string MuonId It accepts values of \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" string xquantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" for horizontal axis double[] xbins is used to set histogram bins limits for horizontal axis string yquantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" for vertical axis double[] ybins is used to set histogram bins limits for vertical axis string path_bins_fit_folder Stores the path folder where is going to save fit results png for every bin const char* output_folder_name Stores the path to output folder where is going to save the 2D efficiency result","title":"Overview"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#overview-of-fitting-method","text":"The fitting method folder is structured in folders and main files. Main files are those ones that are used to run the most important codes. Below is a list of folders presented in this method and the files encontered here. It is important to note that this code has been tested on root 6.22/00.","title":"Overview of fitting method"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#fitting-method-folder-structure","text":"The folders contained in fitting method are described below. Folder Purpose \ud83d\udcc2 DATA Where .root with data should be placed for measuring efficiency \ud83d\udcc2 src Where important files related to main code are keeped \u2514 \ud83d\udcc2 dofits Here it keeps files that are responsible to do the fitting over invariant masses histograms \ud83d\udcc2 tests Some teste made during the development of this tool \ud83d\udcc2 results This folder stores the results output and it is created when any code finnish running","title":"Fitting method folder structure"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#main-files","text":"There are six main files in the fitting method. For simple results like the ones obtained in sideband subtraction method the file used is efficiency.cpp. Main files are explained below.","title":"Main files"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#simplify_datacpp","text":"The simplify_data.cpp file, as the name sugest, simplify a DATA file obtained from this Tag and Probe tool. It is necessary to simplify due RooFit limitations where fitting method codes here used are based on. There are two lines responsable for input and output file: TFile * file0 = TFile :: Open ( \"INPUT_FILE_PATH.root\" ); TFile * fileIO = TFile :: Open ( \"OUTPUT_FILE_PATH.root\" , \"RECREATE\" ); Every user should run this code firstly to simplify .root files in order to use it in fitting method. The input files here are provenient from the main Tag and Probe tool in this repository. If you want to get a new ntuple, you should run it.","title":"\ud83d\udcc4 simplify_data.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#efficiencycpp","text":"This file is responsible to measure the efficiency simple by fitting method as described in this fitting method section.","title":"\ud83d\udcc4 efficiency.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#choosing-ressonance","text":"Here it include the file that is responsible to fit the ressonance and return the yield obtained with error. //Change if you need #include \"src/dofits/DoFit_Jpsi_Run.h\" By default out tool keeps all ressonance fit in the folder src/dofits . There are some example there for specific ressonances and fits. There are two main parameters to control this code. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; The string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. double bins[] is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on.","title":"Choosing ressonance"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#output","text":"There are two output folders in this file by default. They are defined in those lines of code: //Path where is going to save results png for every bin const char * path_bins_fit_folder = \"results/bins_fit/efficiency/\" ; path_bins_fit_folder refers to the path where each individual fit of bins will be stored as .png . In this folder you can find every fit made in this method. //Path where is going to save efficiency string directoryToSave = string ( \"results/efficiencies/efficiency/\" ) + output_folder_name + string ( \"/\" ); The directoryToSave stores the path to save the efficiency result. It is saved as a .root file containing passing and total histograms as well the efficiency result histogram. Informations about the output is printed at end of running.","title":"Output"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_over_efficienciescpp","text":"The purpose of this code is rerun the efficiency.cpp for differents configurations. This code is not recommended for systematic calculations indeed and it was firstly created for systematic studies only. The importants variables to keep in mind are listed below Type Name Purpose double default_min the minimum invariant mass window postion double default_max the maximum invariant mass window postion bool should_loop_muon_id if true, it loops over all muons id (tracking, standalone, global) bool should_loop_settings if true, it loops over all settings presented in set_settings() function int setting if should_loop_settings is false, it uses only this setting number bool exactly This only affect the name of output plots inside .root . Its recommended to keep it set to false","title":"\ud83d\udcc4 loop_over_efficiencies.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#set_settings","text":"It is one of four functions presented in this code. Its is called by: void set_settings ( int index , bool exactly = false ) Inside this function are preset settings that this file runs over. Each setting is associated with a number here named as index . This function is responsible to set the index configuration to the efficiency for running the efficiency.cpp file.","title":"set_settings(...)"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_settings","text":"void loop_settings () If should_loop_muon_id is true, this function is called. It loops over all muon ids: tracking, standalone, global.","title":"loop_settings()"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_muon_id","text":"void loop_muon_id () If should_loop_settings is true, this function is called. It loops over all settings preset in set_settings(...) function.","title":"loop_muon_id()"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#loop_over_efficiencies","text":"void loop_over_efficiencies () It is the main function of this file. It is the function which calls every other function when it is needed.","title":"loop_over_efficiencies()"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#plot_sys_efficiencycpp","text":"The plot_sys_efficiency.cpp code creates a single .root with variations made. Unlike the previous code, the loop_over_efficiencies.cpp , that makes each source of uncertainty be in a separate .root, this one puts all of them in a single .root . This code has been further optimized than his precursor and also as a differential it already calculates the systematic uncertainty. Below it is specified main variables used in this code. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; The string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. double bins[] is used to set histogram bins limits. In the example above, the first bin is [0., 2.), the second is [2., 4.) and so on. Inside plot_sys_efficiency() , there is some useful variables too: Type Name Purpose string path_bins_fit_folder Stores the path to the output folder where .png of fit for each bin made will be string directoryToSave Stores the path to output file","title":"\ud83d\udcc4 plot_sys_efficiency.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#overplot_efficienciescpp","text":"The overplot_efficiencies.cpp code will take the results of the previous topic and make a single graph containing all its variations and will output a .png containing the graph. All main variables in this file are in overplot_efficiencies() function Type Name Purpose const char* input_folder_name Stores the path to input folder where .root is const char* output_folder_name Stores the path to output folder string MuonId It accepts values of \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" string quantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" Remeber when selecting MuonId and quantity to run plot_sys_efficiency.cpp before with same configurations.","title":"\ud83d\udcc4 overplot_efficiencies.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/overview/#plot_sys_efficiency_2dcpp","text":"In order to calculate systematic uncertainties in 2D, it was necessary to create another code: the plot_sys_efficiency_2d.cpp . It has a .root output containing the efficiency histograms that can be viewed through the new TBrowser on root command. The variables in this file is shown below: Type Name Purpose string MuonId It accepts values of \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" string xquantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" for horizontal axis double[] xbins is used to set histogram bins limits for horizontal axis string yquantity It accepts values of \"Pt\" , \"Eta\" and \"Phi\" for vertical axis double[] ybins is used to set histogram bins limits for vertical axis string path_bins_fit_folder Stores the path folder where is going to save fit results png for every bin const char* output_folder_name Stores the path to output folder where is going to save the 2D efficiency result","title":"\ud83d\udcc4 plot_sys_efficiency_2d.cpp"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/","text":"Src files \u00b6 In this section there is a brief explanation for each file in src/ folder. In general, this files are headers called by main files defined on the section Overview . \ud83d\udcc4 create_TH2D.h \u00b6 TH2D * create_TH2D ( const char * name , const char * title , string xquantity , string yquantity , int nbinsx , int nbinsy , double * xbins , double * ybins ) Create a empty TH2D histogram according xquantity and yquantity variables. these varibles supports \"Pt\" , \"Eta\" and \"Phi\" values. \ud83d\udcc4 create_folder.h \u00b6 void create_folder ( const char * folderPath , bool deleteOld = false ) This function creates folder path recursively. If deleteOld is true, it deleted the old folder if the path already exists. \ud83d\udcc4 get_efficiency.h \u00b6 TEfficiency * get_efficiency ( TH1D * all , TH1D * pass , string quantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) Function used to calculate the efficiency. The MuonId , quantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened. \ud83d\udcc4 get_efficiency_2D.h \u00b6 TEfficiency * get_efficiency_2D ( TH2D * all , TH2D * pass , string xquantity , string yquantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) Function used to calculate the 2D efficiency. The MuonId , xquantity , yquantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened. \ud83d\udcc4 get_efficiency_TH2D.h \u00b6 TH2D * get_efficiency_TH2D ( TH2D * hall , TH2D * hpass , string xquantity , string yquantity , string MuonId , string prefix_name = \"\" ) Function used to calculate the 2D efficiency. The MuonId , xquantity , yquantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened. Same function idea as TEfficiency* get_efficiency_2D(...) , but it creates a TH2D objects instead which allows better control of uncertainty calculus. \ud83d\udcc4 make_TH1D.h \u00b6 TH1D * make_TH1D ( string name , double ** values , int index , double * bins , int nbins , string quantity = \"\" , bool draw = false ) Creates TH1D * histogram direclty from values which stores doFit 's outputs. int index is related with the information above: 0 means all histogram and 1 means pass histogram. Choose the number due the histogram you are looking to make. double* bins is used to set histogram bins limits. int nbins represents the number of bins in double* bins . string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. If bool draw it draws the plot on screen. \ud83d\udcc4 yields_n_errs_to_TH2Ds_bin.h \u00b6 void yields_n_errs_to_TH2Ds_bin ( TH2D * hist2d_all , TH2D * hist2d_pass , int x , int y , double * yields_n_errs ) This function fills hist2d_all and hist2d_pass histogram in cell (x,y) with yields_n_errs which is a output from doFit functions. \ud83d\udcc2 dofits \u00b6 Here is stored functions that measures the yields and errors from each bin fit. The return from each function follows this structure: [yield_all, yield_pass, error_all, error_pass] . Functions in this files are defined by: double * doFit ( string condition , string MuonId , const char * savePath = NULL ) string condition selects the bin conditions. string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. const char* savePath where the fit output file from the fit will be saved for further checks.","title":"Src files"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#src-files","text":"In this section there is a brief explanation for each file in src/ folder. In general, this files are headers called by main files defined on the section Overview .","title":"Src files"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#create_th2dh","text":"TH2D * create_TH2D ( const char * name , const char * title , string xquantity , string yquantity , int nbinsx , int nbinsy , double * xbins , double * ybins ) Create a empty TH2D histogram according xquantity and yquantity variables. these varibles supports \"Pt\" , \"Eta\" and \"Phi\" values.","title":"\ud83d\udcc4 create_TH2D.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#create_folderh","text":"void create_folder ( const char * folderPath , bool deleteOld = false ) This function creates folder path recursively. If deleteOld is true, it deleted the old folder if the path already exists.","title":"\ud83d\udcc4 create_folder.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiencyh","text":"TEfficiency * get_efficiency ( TH1D * all , TH1D * pass , string quantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) Function used to calculate the efficiency. The MuonId , quantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened.","title":"\ud83d\udcc4 get_efficiency.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiency_2dh","text":"TEfficiency * get_efficiency_2D ( TH2D * all , TH2D * pass , string xquantity , string yquantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) Function used to calculate the 2D efficiency. The MuonId , xquantity , yquantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened.","title":"\ud83d\udcc4 get_efficiency_2D.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#get_efficiency_th2dh","text":"TH2D * get_efficiency_TH2D ( TH2D * hall , TH2D * hpass , string xquantity , string yquantity , string MuonId , string prefix_name = \"\" ) Function used to calculate the 2D efficiency. The MuonId , xquantity , yquantity and prefix_name are used to set the name and title of TEfficiency* . If shouldWrite is true, it writes the result in any root file opened. Same function idea as TEfficiency* get_efficiency_2D(...) , but it creates a TH2D objects instead which allows better control of uncertainty calculus.","title":"\ud83d\udcc4 get_efficiency_TH2D.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#make_th1dh","text":"TH1D * make_TH1D ( string name , double ** values , int index , double * bins , int nbins , string quantity = \"\" , bool draw = false ) Creates TH1D * histogram direclty from values which stores doFit 's outputs. int index is related with the information above: 0 means all histogram and 1 means pass histogram. Choose the number due the histogram you are looking to make. double* bins is used to set histogram bins limits. int nbins represents the number of bins in double* bins . string quantity supports \"Pt\" , \"Eta\" and \"Phi\" values. If bool draw it draws the plot on screen.","title":"\ud83d\udcc4 make_TH1D.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#yields_n_errs_to_th2ds_binh","text":"void yields_n_errs_to_TH2Ds_bin ( TH2D * hist2d_all , TH2D * hist2d_pass , int x , int y , double * yields_n_errs ) This function fills hist2d_all and hist2d_pass histogram in cell (x,y) with yields_n_errs which is a output from doFit functions.","title":"\ud83d\udcc4 yields_n_errs_to_TH2Ds_bin.h"},{"location":"analysis/selection/idefficiencystudy/fittingreferenceguide/src_files/#dofits","text":"Here is stored functions that measures the yields and errors from each bin fit. The return from each function follows this structure: [yield_all, yield_pass, error_all, error_pass] . Functions in this files are defined by: double * doFit ( string condition , string MuonId , const char * savePath = NULL ) string condition selects the bin conditions. string MuonId supports \"trackerMuon\" , \"standaloneMuon\" and \"globalMuon\" values. const char* savePath where the fit output file from the fit will be saved for further checks.","title":"\ud83d\udcc2 dofits"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/","text":"class FitFunctions \u00b6 This class hold all fit functions for histograms. class FitFunctions::Primary \u00b6 This class is holding primary fit functions for histograms. Content list double Gaus(...) double Pol1(...) double Exp(...) double CrystalBall(...) Functions details Gaus(...) \u00b6 static double Gaus ( double * x , double * par ) Parameters: par = [ height , position , sigma ] Pol1(...) \u00b6 static double Pol1 ( double * x , double * par ) Parameters: par = [ b , a ] Pol3(...) \u00b6 static double Pol3 ( double * x , double * par ) Parameters: par = [ d , c , b , a ] Exp(...) \u00b6 static double Exp ( double * x , double * par ) Parameters: par = [ height , width ] CrystalBall(...) \u00b6 static double CrystalBall ( double * x , double * par ) Parameters: par = [ alpha , n , mean , sigma , yield ] class FitFunctions::Merged \u00b6 This class holds merged fit functions for histograms. Content list double Jpsi::Signal_InvariantMass() double Jpsi::Background_InvariantMass() double Jpsi::InvariantMass() double Upsilon::Signal_InvariantMass() double Upsilon::Background_InvariantMass() double Upsilon::InvariantMass() Functions details Jpsi::Signal_InvariantMass(...) \u00b6 static double Signal_InvariantMass ( double * x , double * par ) Form: Gaus + CrystalBall Parameters: par = [ height , position , sigma , alpha , n , mean , sigma , yield ] Jpsi::Background_InvariantMass(...) \u00b6 static double Background_InvariantMass ( double * x , double * par ) Form: Exp Parameters: par = [ b , a ] Jpsi::InvariantMass(...) \u00b6 static double Signal_InvariantMass ( double * x , double * par ) + Background_InvariantMass ( double * x , double * par ) Form: Gaus + CrystalBall + Exp Parameters: par = [ height1 , position1 , sigma1 , alpha2 , n2 , mean2 , sigma2 , yield2 , b , a ] Upsilon::Signal_InvariantMass(...) \u00b6 static double Signal_InvariantMass ( double * x , double * par ) Form: CrystalBall + Gaus + Gaus Parameters: par = [ alpha1 , n1 , mean1 , sigma1 , yield1 , height2 , position2 , sigma2 , height3 , position3 , sigma3 ] Upsilon::Background_InvariantMass(...) \u00b6 static double Background_InvariantMass ( double * x , double * par ) Form: Pol3 Parameters: par = [ d , c , b , a ] Upsilon::InvariantMass(...) \u00b6 static double Signal_InvariantMass ( double * x , double * par ) + Background_InvariantMass ( double * x , double * par ) Form: CrystalBall + Gaus + Gaus + Pol3 Parameters: par = [ alpha1 , n1 , mean1 , sigma1 , yield1 , height2 , position2 , sigma2 , height3 , position3 , sigma3 , d , c , b , a ]","title":"FitFunction class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#class-fitfunctions","text":"This class hold all fit functions for histograms.","title":"class FitFunctions"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#class-fitfunctionsprimary","text":"This class is holding primary fit functions for histograms. Content list double Gaus(...) double Pol1(...) double Exp(...) double CrystalBall(...) Functions details","title":"class FitFunctions::Primary"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#gaus","text":"static double Gaus ( double * x , double * par ) Parameters: par = [ height , position , sigma ]","title":"Gaus(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#pol1","text":"static double Pol1 ( double * x , double * par ) Parameters: par = [ b , a ]","title":"Pol1(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#pol3","text":"static double Pol3 ( double * x , double * par ) Parameters: par = [ d , c , b , a ]","title":"Pol3(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#exp","text":"static double Exp ( double * x , double * par ) Parameters: par = [ height , width ]","title":"Exp(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#crystalball","text":"static double CrystalBall ( double * x , double * par ) Parameters: par = [ alpha , n , mean , sigma , yield ]","title":"CrystalBall(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#class-fitfunctionsmerged","text":"This class holds merged fit functions for histograms. Content list double Jpsi::Signal_InvariantMass() double Jpsi::Background_InvariantMass() double Jpsi::InvariantMass() double Upsilon::Signal_InvariantMass() double Upsilon::Background_InvariantMass() double Upsilon::InvariantMass() Functions details","title":"class FitFunctions::Merged"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsisignal_invariantmass","text":"static double Signal_InvariantMass ( double * x , double * par ) Form: Gaus + CrystalBall Parameters: par = [ height , position , sigma , alpha , n , mean , sigma , yield ]","title":"Jpsi::Signal_InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsibackground_invariantmass","text":"static double Background_InvariantMass ( double * x , double * par ) Form: Exp Parameters: par = [ b , a ]","title":"Jpsi::Background_InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#jpsiinvariantmass","text":"static double Signal_InvariantMass ( double * x , double * par ) + Background_InvariantMass ( double * x , double * par ) Form: Gaus + CrystalBall + Exp Parameters: par = [ height1 , position1 , sigma1 , alpha2 , n2 , mean2 , sigma2 , yield2 , b , a ]","title":"Jpsi::InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsilonsignal_invariantmass","text":"static double Signal_InvariantMass ( double * x , double * par ) Form: CrystalBall + Gaus + Gaus Parameters: par = [ alpha1 , n1 , mean1 , sigma1 , yield1 , height2 , position2 , sigma2 , height3 , position3 , sigma3 ]","title":"Upsilon::Signal_InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsilonbackground_invariantmass","text":"static double Background_InvariantMass ( double * x , double * par ) Form: Pol3 Parameters: par = [ d , c , b , a ]","title":"Upsilon::Background_InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/FitFunctions/#upsiloninvariantmass","text":"static double Signal_InvariantMass ( double * x , double * par ) + Background_InvariantMass ( double * x , double * par ) Form: CrystalBall + Gaus + Gaus + Pol3 Parameters: par = [ alpha1 , n1 , mean1 , sigma1 , yield1 , height2 , position2 , sigma2 , height3 , position3 , sigma3 , d , c , b , a ]","title":"Upsilon::InvariantMass(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/","text":"class InvariantMass \u00b6 Holds MassValues struct . Constructor details \u00b6 InvariantMass ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ) { if ( strcmp ( resonance , \"Jpsi\" ) == 0 ) { xMin = 2.9 ; xMax = 3.3 ; nBins = 160 ; } if ( strcmp ( resonance , \"Upsilon\" ) == 0 ) { xMin = 8.7 ; xMax = 11. ; nBins = 60 ; } if ( strcmp ( resonance , \"Upsilon1S\" ) == 0 ) { xMin = 8.7 ; xMax = 11. ; nBins = 60 ; } createMassHistogram ( Pass . hMass , \"Passing\" ); createMassHistogram ( All . hMass , \"All\" ); } Private variable details \u00b6 Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType All variables here are reference for public variables in mother class: Type class Private Functions details \u00b6 createMassHistogram(...) \u00b6 void createMassHistogram ( TH1D * & hMass , const char * PassingOrFailing ) Create invariant mass histogram with a specific title. The argument hMass is a pointer where the histogram shall be stored. drawCanvasQuarter(...) \u00b6 void drawCanvasQuarter ( TCanvas * & canvas , bool drawRegions , int quarter , MassValues * ObjMassValues , int color = kBlue ) Draw a quarter of whole canvas with invariant mass histogram pointed. Public variable details \u00b6 Summary Type Name Default value double xMin 0. double xMax 0. int nBins 0 int decimals 3 Constructed objects MassValues Pass Stores information about passing mass histograms. MassValues All Stores information about passing mass histograms. Public Functions details \u00b6 createMassCanvas(...) \u00b6 TCanvas * createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for invariant mass (passing and all muons). defineMassHistogramNumbers() \u00b6 void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of mass histograms in Mass object. doFit() \u00b6 void doFit () Apply a fit over invariant mass in MassValues objects. fillMassHistograms(...) \u00b6 void fillMassHistograms ( double ** quantities , int ** types ) Automatically fill masses histograms. Needs to be called in a loop over all dataset. updateMassValuesAll() \u00b6 void updateMassValuesAll () After fill invariant mass histogram, you need to set signal regions and sideband regions. This function will set it for you. updateMassValuesAll(...) \u00b6 void updateMassValuesFor ( MassValues * ObjMassValues , bool isAll = false ) After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you. writeMassHistogramsOnFile(...) \u00b6 void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.","title":"InvariantMass class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#class-invariantmass","text":"Holds MassValues struct .","title":"class InvariantMass"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#constructor-details","text":"InvariantMass ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ) { if ( strcmp ( resonance , \"Jpsi\" ) == 0 ) { xMin = 2.9 ; xMax = 3.3 ; nBins = 160 ; } if ( strcmp ( resonance , \"Upsilon\" ) == 0 ) { xMin = 8.7 ; xMax = 11. ; nBins = 60 ; } if ( strcmp ( resonance , \"Upsilon1S\" ) == 0 ) { xMin = 8.7 ; xMax = 11. ; nBins = 60 ; } createMassHistogram ( Pass . hMass , \"Passing\" ); createMassHistogram ( All . hMass , \"All\" ); }","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#private-variable-details","text":"Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType All variables here are reference for public variables in mother class: Type class","title":"Private variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#private-functions-details","text":"","title":"Private Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#createmasshistogram","text":"void createMassHistogram ( TH1D * & hMass , const char * PassingOrFailing ) Create invariant mass histogram with a specific title. The argument hMass is a pointer where the histogram shall be stored.","title":"createMassHistogram(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#drawcanvasquarter","text":"void drawCanvasQuarter ( TCanvas * & canvas , bool drawRegions , int quarter , MassValues * ObjMassValues , int color = kBlue ) Draw a quarter of whole canvas with invariant mass histogram pointed.","title":"drawCanvasQuarter(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#public-variable-details","text":"Summary Type Name Default value double xMin 0. double xMax 0. int nBins 0 int decimals 3 Constructed objects MassValues Pass Stores information about passing mass histograms. MassValues All Stores information about passing mass histograms.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#createmasscanvas","text":"TCanvas * createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for invariant mass (passing and all muons).","title":"createMassCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#definemasshistogramnumbers","text":"void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of mass histograms in Mass object.","title":"defineMassHistogramNumbers()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#dofit","text":"void doFit () Apply a fit over invariant mass in MassValues objects.","title":"doFit()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#fillmasshistograms","text":"void fillMassHistograms ( double ** quantities , int ** types ) Automatically fill masses histograms. Needs to be called in a loop over all dataset.","title":"fillMassHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#updatemassvaluesall","text":"void updateMassValuesAll () After fill invariant mass histogram, you need to set signal regions and sideband regions. This function will set it for you.","title":"updateMassValuesAll()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#updatemassvaluesall_1","text":"void updateMassValuesFor ( MassValues * ObjMassValues , bool isAll = false ) After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.","title":"updateMassValuesAll(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/InvariantMass/#writemasshistogramsonfile","text":"void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.","title":"writeMassHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/","text":"struct MassValues \u00b6 Holds informations about passing or all particles fit. Public variable details \u00b6 Summary Type Name Default value TH1D* hMass NULL TF1* fitFunction NULL TF1* fitSignal NULL TF1* fitBackground NULL double sidebandRegion1_x1 0. double sidebandRegion1_x2 0. double signalRegion_x1 0. double signalRegion_x2 0. double sidebandRegion2_x1 0. double sidebandRegion2_x2 0. TFitResultPtr fitResult 0 Public Functions details \u00b6 createTBox(...) \u00b6 TBox * createTBox ( double Ymax , int index = 0 , double Ymin = 0. ) Return TBox of sideband or signal region. if index = -1 return TBox representing left sideband region. if index = 0 return TBox representing signal region. if index = 1 return TBox representing right sideband region. doFitJpsi() \u00b6 void doFitJpsi () Do fit for J/psi resonance. doFitUpsilon() \u00b6 void doFitUpsilon () Do fit for Upsilon resonance with 3 resonances peaks (1S, 2S, 3S). doFitUpsilon1S() \u00b6 void doFitUpsilon1S () Do fit for Upsilon (1S) resonance. isInSidebandRegion(...) \u00b6 bool isInSidebandRegion ( double InvariantMass ) Check if InvariantMass is in sideband region. isInSignalRegion(...) \u00b6 bool isInSignalRegion ( double InvariantMass ) Check if InvariantMass is in signal region. subtractionFactor() \u00b6 double subtractionFactor () Get the subtraction factor calculated by the ratio between yield of background particles in signal region by yield of background particles in sideband region. This yield is get by the integral of function stored in fitBackground variable.","title":"MassValues struct"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#struct-massvalues","text":"Holds informations about passing or all particles fit.","title":"struct MassValues"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#public-variable-details","text":"Summary Type Name Default value TH1D* hMass NULL TF1* fitFunction NULL TF1* fitSignal NULL TF1* fitBackground NULL double sidebandRegion1_x1 0. double sidebandRegion1_x2 0. double signalRegion_x1 0. double signalRegion_x2 0. double sidebandRegion2_x1 0. double sidebandRegion2_x2 0. TFitResultPtr fitResult 0","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#createtbox","text":"TBox * createTBox ( double Ymax , int index = 0 , double Ymin = 0. ) Return TBox of sideband or signal region. if index = -1 return TBox representing left sideband region. if index = 0 return TBox representing signal region. if index = 1 return TBox representing right sideband region.","title":"createTBox(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitjpsi","text":"void doFitJpsi () Do fit for J/psi resonance.","title":"doFitJpsi()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitupsilon","text":"void doFitUpsilon () Do fit for Upsilon resonance with 3 resonances peaks (1S, 2S, 3S).","title":"doFitUpsilon()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#dofitupsilon1s","text":"void doFitUpsilon1S () Do fit for Upsilon (1S) resonance.","title":"doFitUpsilon1S()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#isinsidebandregion","text":"bool isInSidebandRegion ( double InvariantMass ) Check if InvariantMass is in sideband region.","title":"isInSidebandRegion(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#isinsignalregion","text":"bool isInSignalRegion ( double InvariantMass ) Check if InvariantMass is in signal region.","title":"isInSignalRegion(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/MassValues/#subtractionfactor","text":"double subtractionFactor () Get the subtraction factor calculated by the ratio between yield of background particles in signal region by yield of background particles in sideband region. This yield is get by the integral of function stored in fitBackground variable.","title":"subtractionFactor()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/","text":"class PassingFailing \u00b6 Holds histograms of passing and all particle quantities. Constructor details \u00b6 PassingFailing ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char *& tagOrProbe , const char * passingOrFailing , const char *& quantityName , const char *& xAxisName , const char *& quantityUnit , const char *& extendedQuantityName , double & xMin , double & xMax , int & nBins , int & decimals ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ), passingOrFailing ( passingOrFailing ), quantityName ( quantityName ), xAxisName ( xAxisName ), quantityUnit ( quantityUnit ), extendedQuantityName ( extendedQuantityName ), nBins ( nBins ), xMin ( xMin ), xMax ( xMax ), decimals ( decimals ) { createHistogram ( hSigBack , \"SigBack\" ); createHistogram ( hSig , \"Sig\" ); createHistogram ( hBack , \"Back\" ); } Private variable details \u00b6 Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType const char*& tagOrProbe InvariantMass& ObjMass const char*& tagOrProbe const char*& xAxisName const char*& quantityUnit const char*& extendedQuantityName double& xMin double& xMax int& nBins int& decimals All variables here are reference for public variables in mother class: PtEtaPhi class . Private Functions details \u00b6 createHistogram() \u00b6 void createHistogram () Create quantity histogram. fillAfter() \u00b6 string fillAfter ( string text , char fillWith , int targetLength ) Fill blank space of a string. It is used in consistencyDebugCout(). Public variable details \u00b6 Summary Type Name Default value const char* passingOrFailing NULL TH1D* hSigBack NULL TH1D* hSig NULL TH1D* hBack NULL Details const char* passingOrFailing Set if it is \"Passing\" or \"All\" object. TH1D* hSigBack Stores the histogram for particles in signal region. TH1D* hSig Stores the subtracted histogram. TH1D* hBack Stores the histogram for particles in sideband region. Public Functions details \u00b6 consistencyDebugCout() \u00b6 void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistogram(). It is result for this equation: Where: alpha = yield of background particles signal region / yield of background particles sideband region createQuantitiesCanvas(...) \u00b6 TCanvas * createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms(). fillQuantitiesHistograms(...) \u00b6 void fillQuantitiesHistograms ( double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset. normalizeHistograms() \u00b6 void normalizeHistograms () Normalize quantities histograms of variable bin after filling it. PassFailObj() \u00b6 MassValues * PassFailObj () Get the MassValue object of corresponding MassValue object. subtractSigHistogram() \u00b6 void subtractSigHistogram () Apply sideband subtraction over histograms. writeQuantitiesHistogramsOnFile(...) \u00b6 void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write quantity histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"PassingFailing class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#class-passingfailing","text":"Holds histograms of passing and all particle quantities.","title":"class PassingFailing"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#constructor-details","text":"PassingFailing ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char *& tagOrProbe , const char * passingOrFailing , const char *& quantityName , const char *& xAxisName , const char *& quantityUnit , const char *& extendedQuantityName , double & xMin , double & xMax , int & nBins , int & decimals ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ), passingOrFailing ( passingOrFailing ), quantityName ( quantityName ), xAxisName ( xAxisName ), quantityUnit ( quantityUnit ), extendedQuantityName ( extendedQuantityName ), nBins ( nBins ), xMin ( xMin ), xMax ( xMax ), decimals ( decimals ) { createHistogram ( hSigBack , \"SigBack\" ); createHistogram ( hSig , \"Sig\" ); createHistogram ( hBack , \"Back\" ); }","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#private-variable-details","text":"Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType const char*& tagOrProbe InvariantMass& ObjMass const char*& tagOrProbe const char*& xAxisName const char*& quantityUnit const char*& extendedQuantityName double& xMin double& xMax int& nBins int& decimals All variables here are reference for public variables in mother class: PtEtaPhi class .","title":"Private variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#private-functions-details","text":"","title":"Private Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#createhistogram","text":"void createHistogram () Create quantity histogram.","title":"createHistogram()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#fillafter","text":"string fillAfter ( string text , char fillWith , int targetLength ) Fill blank space of a string. It is used in consistencyDebugCout().","title":"fillAfter()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#public-variable-details","text":"Summary Type Name Default value const char* passingOrFailing NULL TH1D* hSigBack NULL TH1D* hSig NULL TH1D* hBack NULL Details const char* passingOrFailing Set if it is \"Passing\" or \"All\" object. TH1D* hSigBack Stores the histogram for particles in signal region. TH1D* hSig Stores the subtracted histogram. TH1D* hBack Stores the histogram for particles in sideband region.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#consistencydebugcout","text":"void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistogram(). It is result for this equation: Where: alpha = yield of background particles signal region / yield of background particles sideband region","title":"consistencyDebugCout()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#createquantitiescanvas","text":"TCanvas * createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms().","title":"createQuantitiesCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#fillquantitieshistograms","text":"void fillQuantitiesHistograms ( double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.","title":"fillQuantitiesHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#normalizehistograms","text":"void normalizeHistograms () Normalize quantities histograms of variable bin after filling it.","title":"normalizeHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#passfailobj","text":"MassValues * PassFailObj () Get the MassValue object of corresponding MassValue object.","title":"PassFailObj()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#subtractsighistogram","text":"void subtractSigHistogram () Apply sideband subtraction over histograms.","title":"subtractSigHistogram()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PassingFailing/#writequantitieshistogramsonfile","text":"void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write quantity histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"writeQuantitiesHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/","text":"class PtEtaPhi \u00b6 Holds PassingFailing class . Constructor details \u00b6 PtEtaPhi ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char *& tagOrProbe , const char * quantityName , const char * xAxisName , const char * quantityUnit , const char * extendedQuantityName , int nBins , double xMin , double xMax , int decimals = 3 ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ), quantityName ( quantityName ), xAxisName ( xAxisName ), quantityUnit ( quantityUnit ), extendedQuantityName ( extendedQuantityName ), nBins ( nBins ), xMin ( xMin ), xMax ( xMax ), decimals ( decimals ) {} Private variable details \u00b6 Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType const char*& tagOrProbe InvariantMass& ObjMass All variables here are reference for public variables in mother class: TagProbe class . Public variable details \u00b6 Summary Type Name Default value const char* tagOrProbe NULL const char* xAxisName NULL const char* quantityUnit NULL const char* extendedQuantityName NULL double xMin 0. double xMax 0. int nBins 0 int decimals 3 TEfficiency* pEff NULL Details const char* quantityName Stores the quantity name. E.g.: \"pT\". const char* extendedQuantityName Stores the extended quantity name. E.g.: \"Transversal Momentum\". const char* quantityUnit Stores the quantity unit. E.g.: \"GeV/c\". const char* xAxisName Stores the quantity name for histogram horizontal axis in LaTeX form. E.g.: \"p_{t}\". int nBins Stores the number of bins in histograms. int decimals = 3 Number of decimals showed in bin width on histogram vertical axis. double xMin Lower horizontal value of histogram. double xMax Higher horizontal value of histogram. TEfficiency* pEff Stores the efficiency plot. Constructed objects PassingFailing Pass Stores all informations about invariant masses, including fit and histograms. PassingFailing All Stores all informations about tag muons, incuding quantities histograms and efficiencies. Public Functions details \u00b6 consistencyDebugCout() \u00b6 void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms(). createEfficiencyCanvas(...) \u00b6 void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...). createEfficiencyPlot(...) \u00b6 TEfficiency * createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms(). createQuantitiesCanvas(...) \u00b6 TCanvas * createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms(). fillQuantitiesHistograms(...) \u00b6 void fillQuantitiesHistograms ( double & quantity , double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset. normalizeHistograms() \u00b6 void normalizeHistograms () Normalize quantities histograms of variable bin after filling it. subtractSigHistograms() \u00b6 void subtractSigHistograms () Apply sideband subtraction over all histograms. writeQuantitiesHistogramsOnFile(...) \u00b6 void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"PtEtaPhi class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#class-ptetaphi","text":"Holds PassingFailing class .","title":"class PtEtaPhi"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#constructor-details","text":"PtEtaPhi ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char *& tagOrProbe , const char * quantityName , const char * xAxisName , const char * quantityUnit , const char * extendedQuantityName , int nBins , double xMin , double xMax , int decimals = 3 ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ), quantityName ( quantityName ), xAxisName ( xAxisName ), quantityUnit ( quantityUnit ), extendedQuantityName ( extendedQuantityName ), nBins ( nBins ), xMin ( xMin ), xMax ( xMax ), decimals ( decimals ) {}","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#private-variable-details","text":"Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType const char*& tagOrProbe InvariantMass& ObjMass All variables here are reference for public variables in mother class: TagProbe class .","title":"Private variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#public-variable-details","text":"Summary Type Name Default value const char* tagOrProbe NULL const char* xAxisName NULL const char* quantityUnit NULL const char* extendedQuantityName NULL double xMin 0. double xMax 0. int nBins 0 int decimals 3 TEfficiency* pEff NULL Details const char* quantityName Stores the quantity name. E.g.: \"pT\". const char* extendedQuantityName Stores the extended quantity name. E.g.: \"Transversal Momentum\". const char* quantityUnit Stores the quantity unit. E.g.: \"GeV/c\". const char* xAxisName Stores the quantity name for histogram horizontal axis in LaTeX form. E.g.: \"p_{t}\". int nBins Stores the number of bins in histograms. int decimals = 3 Number of decimals showed in bin width on histogram vertical axis. double xMin Lower horizontal value of histogram. double xMax Higher horizontal value of histogram. TEfficiency* pEff Stores the efficiency plot. Constructed objects PassingFailing Pass Stores all informations about invariant masses, including fit and histograms. PassingFailing All Stores all informations about tag muons, incuding quantities histograms and efficiencies.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#consistencydebugcout","text":"void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms().","title":"consistencyDebugCout()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createefficiencycanvas","text":"void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).","title":"createEfficiencyCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createefficiencyplot","text":"TEfficiency * createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().","title":"createEfficiencyPlot(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#createquantitiescanvas","text":"TCanvas * createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms().","title":"createQuantitiesCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#fillquantitieshistograms","text":"void fillQuantitiesHistograms ( double & quantity , double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.","title":"fillQuantitiesHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#normalizehistograms","text":"void normalizeHistograms () Normalize quantities histograms of variable bin after filling it.","title":"normalizeHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#subtractsighistograms","text":"void subtractSigHistograms () Apply sideband subtraction over all histograms.","title":"subtractSigHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/PtEtaPhi/#writequantitieshistogramsonfile","text":"void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"writeQuantitiesHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/","text":"class SidebandSubtraction \u00b6 Holds Type class . This is the mother class. Constructor details \u00b6 SidebandSubtraction () {} SidebandSubtraction ( const char * resonance ) : resonance ( resonance ) {} Public variable details \u00b6 Summary Type Name Default value const char* resonance \"Jpsi\" const char* particleName \"Muon\" const char* canvasWatermark \"#bf{CMS Open Data}\" const char* directoryToSave \"../result/\" bool doTracker true bool doStandalone true bool doGlobal true bool doTagMuon true bool doProbeMuon true Details const char* resonance = \"Jpsi\" Supports values \"Jpsi\" , \"Upsilon\" or \"Upsilon(1S)\" . const char* particleName = \"Muon\" Stores the particle name for titles. const char* canvasWatermark = \"#bf{CMS Open Data}\" Stores what watermark will be showed in plots. const char* directoryToSave = \"../result/\" Where all canvas will be stored. bool doTracker = true If it will compute Tracker muons efficiency. bool doStandalone = true If it will compute Standalone muons efficiency. bool doGlobal = true If it will compute Global muons efficiency. Constructed objects Type Tracker Stores all informations about Tracker muons. Type Standalone Stores all informations about Standalone muons. Type Global Stores all informations about Global muons. Public Functions details \u00b6 consistencyDebugCout() \u00b6 void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms(). createEfficiencyCanvas(...) \u00b6 void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...). createEfficiencyPlot(...) \u00b6 void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms(). createMassCanvas(...) \u00b6 void createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all invariant mass (passing and all muons). createQuantitiesCanvas(...) \u00b6 void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms(). defineMassHistogramNumbers() \u00b6 void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of all mass histograms. doFit() \u00b6 void doFit () Apply a fit over all invariant mass stored. fillMassHistograms(...) \u00b6 void fillMassHistograms ( double ** quantities , int ** types ) Automatically fill all masses histograms. Needs to be called in a loop over all dataset. fillQuantitiesHistograms(...) \u00b6 void fillQuantitiesHistograms ( double ** quantities , int ** types ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset. normalizeHistograms() \u00b6 void normalizeHistograms () Normalize quantities histograms of variable bin after filling it. subtractSigHistograms() \u00b6 void subtractSigHistograms () Apply sideband subtraction over all histograms. updateMassValuesAll() \u00b6 void updateMassValuesAll () After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you. writeMassHistogramsOnFile(...) \u00b6 void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written. writeQuantitiesHistogramsOnFile(...) \u00b6 void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"SidebandSubtraction class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#class-sidebandsubtraction","text":"Holds Type class . This is the mother class.","title":"class SidebandSubtraction"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#constructor-details","text":"SidebandSubtraction () {} SidebandSubtraction ( const char * resonance ) : resonance ( resonance ) {}","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#public-variable-details","text":"Summary Type Name Default value const char* resonance \"Jpsi\" const char* particleName \"Muon\" const char* canvasWatermark \"#bf{CMS Open Data}\" const char* directoryToSave \"../result/\" bool doTracker true bool doStandalone true bool doGlobal true bool doTagMuon true bool doProbeMuon true Details const char* resonance = \"Jpsi\" Supports values \"Jpsi\" , \"Upsilon\" or \"Upsilon(1S)\" . const char* particleName = \"Muon\" Stores the particle name for titles. const char* canvasWatermark = \"#bf{CMS Open Data}\" Stores what watermark will be showed in plots. const char* directoryToSave = \"../result/\" Where all canvas will be stored. bool doTracker = true If it will compute Tracker muons efficiency. bool doStandalone = true If it will compute Standalone muons efficiency. bool doGlobal = true If it will compute Global muons efficiency. Constructed objects Type Tracker Stores all informations about Tracker muons. Type Standalone Stores all informations about Standalone muons. Type Global Stores all informations about Global muons.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#consistencydebugcout","text":"void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms().","title":"consistencyDebugCout()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createefficiencycanvas","text":"void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).","title":"createEfficiencyCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createefficiencyplot","text":"void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().","title":"createEfficiencyPlot(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createmasscanvas","text":"void createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all invariant mass (passing and all muons).","title":"createMassCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#createquantitiescanvas","text":"void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms().","title":"createQuantitiesCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#definemasshistogramnumbers","text":"void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of all mass histograms.","title":"defineMassHistogramNumbers()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#dofit","text":"void doFit () Apply a fit over all invariant mass stored.","title":"doFit()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#fillmasshistograms","text":"void fillMassHistograms ( double ** quantities , int ** types ) Automatically fill all masses histograms. Needs to be called in a loop over all dataset.","title":"fillMassHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#fillquantitieshistograms","text":"void fillQuantitiesHistograms ( double ** quantities , int ** types ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.","title":"fillQuantitiesHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#normalizehistograms","text":"void normalizeHistograms () Normalize quantities histograms of variable bin after filling it.","title":"normalizeHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#subtractsighistograms","text":"void subtractSigHistograms () Apply sideband subtraction over all histograms.","title":"subtractSigHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#updatemassvaluesall","text":"void updateMassValuesAll () After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.","title":"updateMassValuesAll()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#writemasshistogramsonfile","text":"void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.","title":"writeMassHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/SidebandSubtraction/#writequantitieshistogramsonfile","text":"void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"writeQuantitiesHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/","text":"class TagProbe \u00b6 Holds TagProbe class and InvariantMass class . Constructor details \u00b6 TagProbe ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char * tagOrProbe ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ) {} Private variable details \u00b6 Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType InvariantMass& ObjMass All variables here are reference for public variables in mother class: Type class Public variable details \u00b6 Summary Type Name Default value const char* tagOrProbe NULL Details const char* tagOrProbe = NULL Set if it is \"Tag\" or \"Probe\" object Constructed objects PtEtaPhi Pt Transversal momentum histograms. PtEtaPhi Eta Pseudorapidity histograms. PtEtaPhi Phi Azimutal angle histograms. Public Functions details \u00b6 consistencyDebugCout() \u00b6 void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms(). createEfficiencyCanvas(...) \u00b6 void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...). createEfficiencyPlot(...) \u00b6 void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms(). createQuantitiesCanvas(...) \u00b6 void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms(). fillQuantitiesHistograms(...) \u00b6 void fillQuantitiesHistograms ( double ** quantities , double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset. normalizeHistograms() \u00b6 void normalizeHistograms () Normalize quantities histograms of variable bin after filling it. subtractSigHistograms() \u00b6 void subtractSigHistograms () Apply sideband subtraction over all histograms. writeQuantitiesHistogramsOnFile(...) \u00b6 void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"TagProbe class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#class-tagprobe","text":"Holds TagProbe class and InvariantMass class .","title":"class TagProbe"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#constructor-details","text":"TagProbe ( const char *& resonance , const char *& particleName , const char *& canvasWatermark , const char *& directoryToSave , const char *& particleType , InvariantMass & ObjMass , const char * tagOrProbe ) : resonance ( resonance ), particleName ( particleName ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ), ObjMass ( ObjMass ), tagOrProbe ( tagOrProbe ) {}","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#private-variable-details","text":"Summary Type Name const char*& resonance const char*& particleName const char*& canvasWatermark const char*& directoryToSave const char*& particleType InvariantMass& ObjMass All variables here are reference for public variables in mother class: Type class","title":"Private variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#public-variable-details","text":"Summary Type Name Default value const char* tagOrProbe NULL Details const char* tagOrProbe = NULL Set if it is \"Tag\" or \"Probe\" object Constructed objects PtEtaPhi Pt Transversal momentum histograms. PtEtaPhi Eta Pseudorapidity histograms. PtEtaPhi Phi Azimutal angle histograms.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#consistencydebugcout","text":"void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms().","title":"consistencyDebugCout()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createefficiencycanvas","text":"void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).","title":"createEfficiencyCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createefficiencyplot","text":"void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().","title":"createEfficiencyPlot(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#createquantitiescanvas","text":"void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms().","title":"createQuantitiesCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#fillquantitieshistograms","text":"void fillQuantitiesHistograms ( double ** quantities , double & InvariantMass , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.","title":"fillQuantitiesHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#normalizehistograms","text":"void normalizeHistograms () Normalize quantities histograms of variable bin after filling it.","title":"normalizeHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#subtractsighistograms","text":"void subtractSigHistograms () Apply sideband subtraction over all histograms.","title":"subtractSigHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/TagProbe/#writequantitieshistogramsonfile","text":"void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"writeQuantitiesHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/","text":"class Type \u00b6 Holds TagProbe class and InvariantMass class . Constructor details \u00b6 Type ( const char *& resonance , const char *& particleName , bool & doTagMuon , bool & doProbeMuon , const char *& canvasWatermark , const char *& directoryToSave , const char * particleType ) : resonance ( resonance ), particleName ( particleName ), doTagMuon ( doTagMuon ), doProbeMuon ( doProbeMuon ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ) {} Private variable details \u00b6 Summary Type Name const char*& resonance const char*& particleName bool& doTagMuon bool& doProbeMuon const char*& canvasWatermark const char*& directoryToSave All variables here are reference for public variables in mother class: SidebandSubtraction class . Public variable details \u00b6 Summary Type Name Default value const char* particleType NULL Details const char* particleType = NULL Set the name of particle type. Constructed objects InvariantMass Mass Stores all informations about invariant masses, including fit and histograms. TagProbe Tag Stores all informations about tag muons, incuding quantities histograms and efficiencies. TagProbe Probe Stores all informations about probe muons, incuding quantities histograms and efficiencies. Public Functions details \u00b6 consistencyDebugCout() \u00b6 void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms(). createEfficiencyCanvas(...) \u00b6 void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...). createEfficiencyPlot(...) \u00b6 void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms(). createMassCanvas(...) \u00b6 void createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all invariant mass (passing and all muons). createQuantitiesCanvas(...) \u00b6 void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms(). defineMassHistogramNumbers() \u00b6 void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of mass histograms in Mass object. doFit() \u00b6 void doFit () Apply a fit over invariant mass in Mass object. fillMassHistograms(...) \u00b6 void fillMassHistograms ( double & InvariantMass , int & isPassing ) Automatically fill all masses histograms. Needs to be called in a loop over all dataset. fillQuantitiesHistograms(...) \u00b6 void fillQuantitiesHistograms ( double ** quantities , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset. normalizeHistograms() \u00b6 void normalizeHistograms () Normalize quantities histograms of variable bin after filling it. subtractSigHistograms() \u00b6 void subtractSigHistograms () Apply sideband subtraction over all histograms. updateMassValuesAll() \u00b6 void updateMassValuesAll () After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you. writeMassHistogramsOnFile(...) \u00b6 void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written. writeQuantitiesHistogramsOnFile(...) \u00b6 void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"Type class"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#class-type","text":"Holds TagProbe class and InvariantMass class .","title":"class Type"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#constructor-details","text":"Type ( const char *& resonance , const char *& particleName , bool & doTagMuon , bool & doProbeMuon , const char *& canvasWatermark , const char *& directoryToSave , const char * particleType ) : resonance ( resonance ), particleName ( particleName ), doTagMuon ( doTagMuon ), doProbeMuon ( doProbeMuon ), canvasWatermark ( canvasWatermark ), directoryToSave ( directoryToSave ), particleType ( particleType ) {}","title":"Constructor details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#private-variable-details","text":"Summary Type Name const char*& resonance const char*& particleName bool& doTagMuon bool& doProbeMuon const char*& canvasWatermark const char*& directoryToSave All variables here are reference for public variables in mother class: SidebandSubtraction class .","title":"Private variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#public-variable-details","text":"Summary Type Name Default value const char* particleType NULL Details const char* particleType = NULL Set the name of particle type. Constructed objects InvariantMass Mass Stores all informations about invariant masses, including fit and histograms. TagProbe Tag Stores all informations about tag muons, incuding quantities histograms and efficiencies. TagProbe Probe Stores all informations about probe muons, incuding quantities histograms and efficiencies.","title":"Public variable details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#public-functions-details","text":"","title":"Public Functions details"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#consistencydebugcout","text":"void consistencyDebugCout () Print on terminal the consistency check after subtractSigHistograms().","title":"consistencyDebugCout()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createefficiencycanvas","text":"void createEfficiencyCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all efficiencies calculated. It need to be called after createEfficiencyPlot(...).","title":"createEfficiencyCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createefficiencyplot","text":"void createEfficiencyPlot ( bool shouldWrite = false ) Create a TEfficiency object with calculated efficiency. It needs do be called after subtractSigHistograms().","title":"createEfficiencyPlot(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createmasscanvas","text":"void createMassCanvas ( bool drawRegions = false , bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all invariant mass (passing and all muons).","title":"createMassCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#createquantitiescanvas","text":"void createQuantitiesCanvas ( bool shouldWrite = false , bool shouldSavePNG = false ) Create canvas for all quantities after subtractSigHistograms().","title":"createQuantitiesCanvas(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#definemasshistogramnumbers","text":"void defineMassHistogramNumbers ( int nBins , double xMin , double xMax , int decimals = 3 ) Redefine number parameters of mass histograms in Mass object.","title":"defineMassHistogramNumbers()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#dofit","text":"void doFit () Apply a fit over invariant mass in Mass object.","title":"doFit()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#fillmasshistograms","text":"void fillMassHistograms ( double & InvariantMass , int & isPassing ) Automatically fill all masses histograms. Needs to be called in a loop over all dataset.","title":"fillMassHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#fillquantitieshistograms","text":"void fillQuantitiesHistograms ( double ** quantities , int & isPassing ) Automatically fill all quantities histograms. Needs to be called in a loop over all dataset.","title":"fillQuantitiesHistograms(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#normalizehistograms","text":"void normalizeHistograms () Normalize quantities histograms of variable bin after filling it.","title":"normalizeHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#subtractsighistograms","text":"void subtractSigHistograms () Apply sideband subtraction over all histograms.","title":"subtractSigHistograms()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#updatemassvaluesall","text":"void updateMassValuesAll () After fill invariant mass histograms, you need to set signal regions and sideband regions. This function will set it for you.","title":"updateMassValuesAll()"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#writemasshistogramsonfile","text":"void writeMassHistogramsOnFile ( bool writehPass , bool writehAll ) Write all mass canvas histograms in a root file. Just need to call this function and all mass histograms will be written.","title":"writeMassHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/Type/#writequantitieshistogramsonfile","text":"void writeQuantitiesHistogramsOnFile ( bool hSigBack , bool hSig , bool hBack ) Write all quantities histograms in a root file. Just need to call this function and all quantities histograms will be written. It needs to be called after subtractSigHistograms().","title":"writeQuantitiesHistogramsOnFile(...)"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/","text":"The Macro \u00b6 A macro is a code file create to be interpreted by a program. In this case, ROOT program will interpret it. The main code of this tool is in the file macro.ccp . In this section what compose this file is explained in details. It is important to note that this code has been tested on root 6.22/00. About the code \u00b6 macro.cpp is a example how to use Sideband Subtraction to get reconstruction efficiencies for a Tag & Probe ntupple. It analyzes J/psi and Upsilon reconstruction efficiency for tracker , standalone and global muons. The file is encountered in folder main . Now, I going to talk about what this function do and how it does in the text below. Dataset The datasets used in this code are obtained with the main code of this Tag an Probe tool. Classes list \u00b6 There are some classes in Sideband Subtraction Tag And Probe project and they are distributed in these files with same name: Static functions : FitFunctions Primary Merged Jpsi Upsilon Classes and struct : SidebandSubtraction Type InvariantMass MassValues TagProbe PtEtaPhi PassingFailing This format shows what nested classes. Classes or structs below slided at right represents they are nested with the class above it. Sideband Subtraction code structure \u00b6 The diagram below represents the structure of objects in code. At left we have the structure of objects name. At right we have the correspondent class name of objects in these line. Also in Mass object we have: Notice that all objects in same line shares the same structure. Before macro.cpp \u00b6 There are some files in folder config aside of macro.ccp . The sections below explain about them. cuts.h \u00b6 This is it content: //This files holds some functions used in macro.cpp for particle selection //Return if is a accepted particle or no bool applyCuts ( double ** quantities , int ** types ) { //Assign variables for easy visualization double & ProbeMuon_Pt = * quantities [ 0 ]; double & ProbeMuon_Eta = * quantities [ 1 ]; double & ProbeMuon_Phi = * quantities [ 2 ]; double & TagMuon_Pt = * quantities [ 3 ]; double & TagMuon_Eta = * quantities [ 4 ]; double & TagMuon_Phi = * quantities [ 5 ]; double & InvariantMass = * quantities [ 6 ]; int & PassingProbeTrackingMuon = * types [ 0 ]; int & PassingProbeStandAloneMuon = * types [ 1 ]; int & PassingProbeGlobalMuon = * types [ 2 ]; //Apply cuts if ( TagMuon_Pt >= 7.0 && fabs ( TagMuon_Eta ) <= 2.4 ) return true ; return false ; } It stores the function applyCuts(), where return true for allowed pair of particles and false for not allowed. createHistogram.h \u00b6 This file is called in PassingFailing.cpp and set quantity histograms bins and create the hitogram. Its default content is shwon bellow: void createHistogram ( TH1D * & histo , const char * histoName ) { //Set parameters string hName = string ( particleType ) + string ( passingOrFailing ) + string ( tagOrProbe ) + string ( particleName ) + \"_\" + string ( quantityName ) + string ( histoName ); string hTitle = string ( passingOrFailing ) + \" in \" + string ( particleType ) + \" \" + string ( tagOrProbe ); string xAxisTitle = string ( xAxisName ); string yAxisTitleForm = \"Events\" ; //Add unit if has if ( strcmp ( quantityUnit , \"\" ) != 0 ) xAxisTitle += \" [\" + string ( quantityUnit ) + \"]\" ; //Change title is passing if ( strcmp ( passingOrFailing , \"Passing\" ) == 0 ) hTitle = string ( particleType ) + \" \" + string ( particleName ) + \" \" + string ( tagOrProbe ); if ( strcmp ( passingOrFailing , \"All\" ) == 0 ) hTitle = \"All \" + string ( particleName ) + \" \" + string ( tagOrProbe ); //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Edit histogram axis histo -> GetYaxis () -> SetTitle ( Form ( yAxisTitleForm . data (), histo -> GetBinWidth ( 0 ))); histo -> GetXaxis () -> SetTitle ( xAxisTitle . data ()); } settings.cpp \u00b6 It stores many configurations used in macro.cpp : //List of files const char * files [] = { \"../data_histoall.root\" , \"../Run2011AMuOnia_mergeNtuple.root\" , \"../JPsiToMuMu_mergeMCNtuple.root\" , \"../Run2011A_MuOnia_Upsilon.root\" , \"../Upsilon1SToMuMu_MC_full.root\" }; const char * directoriesToSave [] = { \"../results/result/\" , \"../results/Jpsi_Run_2011/\" , \"../results/Jpsi_MC_2020_sbs/\" , \"../results/Upsilon_Run_2011/\" , \"../results/Upsilon_MC_2020_sbs/\" }; //MAIN OPTIONS //Which file of files (variable above) should use int useFile = 4 ; //Set the canvasW wtermark const char * canvasWatermark = \"#bf{CMS Open Data}\" ; //Path where is going to save results const char * directoryToSave = directoriesToSave [ useFile ]; //directoryToSave = \"../result/\"; //Should limit data? long long limitData = 0 ; //0 -> do not limit //Canvas drawing bool shouldDrawInvariantMassCanvas = true ; bool shouldDrawInvariantMassCanvasRegion = true ; bool shouldDrawQuantitiesCanvas = true ; bool shouldDrawEfficiencyCanvas = true ; //Muon id anlyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //Muon label anlyse bool doTagMuon = false ; bool doProbeMuon = true ; //ENDED MAIN OPTIONS And then there are more automatically set options: //Auto detect resonance due file index const char * resonance = \"Jpsi\" ; if ( useFile > 2 ) resonance = \"Upsilon\" ; if ( useFile == 4 ) resonance = \"Upsilon1S\" ; //Auto detect limit of data if ( limitData > 0 ) directoryToSave = \"../partial_result/\" ; //Compatibility adjusts on file read (for data_histoall ntupples) bool needsRetroCompatibility = false ; if ( useFile == 0 ) needsRetroCompatibility = true ; Code explained in parts \u00b6 macro.cpp is the main file of this program. Its the main code. It is explained in parts below: //Input files, options are set here! #include \"config/settings.cpp\" It imports configurations about macro.cpp //Check if the name of dir is ok if ( string ( directoryToSave ). back () != string ( \"/\" )) { cerr << \"To avoid errors, please end the result directory with a \\\" / \\\" \" << endl ; abort (); } //Check if dir exists and create if ( gSystem -> AccessPathName ( directoryToSave )) { if ( gSystem -> mkdir ( directoryToSave , true )) { cerr << \" \\\" \" << directoryToSave << \" \\\" path could not be found and could not be created ERROR\" << endl ; cerr << \"Try to create manually this folder path\" << endl ; abort (); } else { cout << \" \\\" \" << directoryToSave << \" \\\" directory created OK\" << endl ; } } else { cout << \" \\\" \" << directoryToSave << \" \\\" directory OK\" << endl ; } Check if the directoryToSave (setted in settings.cpp) has a valid name and if exists. If not, the code creates the folder. //Compatibility adjusts on file read (for data_histoall ntupples) string folderName = \"tagandprobe/\" ; if ( needsRetroCompatibility ) folderName = \"demo/\" ; //Open and read files TFile * file0 = TFile :: Open ( files [ useFile ]); TTree * TreePC = ( TTree * ) file0 -> Get (( folderName + \"PlotControl\" ). data ()); TTree * TreeAT = ( TTree * ) file0 -> Get (( folderName + \"AnalysisTree\" ). data ()); cout << \"Using \\\" \" << files [ useFile ] << \" \\\" ntupple\" << endl ; This part is responsible to open the file and do conversions. The first one file is a bit different of the other ones, so it needs compatibiliy besides its not important anymore and is a obsolete file. //Create variables double ProbeMuon_Pt ; double ProbeMuon_Eta ; double ProbeMuon_Phi ; double TagMuon_Pt ; double TagMuon_Eta ; double TagMuon_Phi ; double InvariantMass ; int PassingProbeTrackingMuon ; int PassingProbeStandAloneMuon ; int PassingProbeGlobalMuon ; //Assign variables TreePC -> SetBranchAddress ( \"ProbeMuon_Pt\" , & ProbeMuon_Pt ); TreePC -> SetBranchAddress ( \"ProbeMuon_Eta\" , & ProbeMuon_Eta ); TreePC -> SetBranchAddress ( \"ProbeMuon_Phi\" , & ProbeMuon_Phi ); TreePC -> SetBranchAddress ( \"TagMuon_Pt\" , & TagMuon_Pt ); TreePC -> SetBranchAddress ( \"TagMuon_Eta\" , & TagMuon_Eta ); TreePC -> SetBranchAddress ( \"TagMuon_Phi\" , & TagMuon_Phi ); if ( needsRetroCompatibility ) TreePC -> SetBranchAddress ( \"InvariantMass\" , & InvariantMass ); else TreeAT -> SetBranchAddress ( \"InvariantMass\" , & InvariantMass ); TreeAT -> SetBranchAddress ( \"PassingProbeTrackingMuon\" , & PassingProbeTrackingMuon ); TreeAT -> SetBranchAddress ( \"PassingProbeStandAloneMuon\" , & PassingProbeStandAloneMuon ); TreeAT -> SetBranchAddress ( \"PassingProbeGlobalMuon\" , & PassingProbeGlobalMuon ); double * quantities [] = { & ProbeMuon_Pt , & ProbeMuon_Eta , & ProbeMuon_Phi , & TagMuon_Pt , & TagMuon_Eta , & TagMuon_Phi , & InvariantMass , }; int * types [] = { & PassingProbeTrackingMuon , & PassingProbeStandAloneMuon , & PassingProbeGlobalMuon }; Now variables are created and linked to branches in ntupple. Then a array of these variables are set. //Create a object and set configs SidebandSubtraction SdS { resonance }; SdS . canvasWatermark = canvasWatermark ; SdS . directoryToSave = directoryToSave ; SdS . doTracker = doTracker ; SdS . doStandalone = doStandalone ; SdS . doGlobal = doGlobal ; SdS . doTagMuon = doTagMuon ; SdS . doProbeMuon = doProbeMuon ; cout << \"resonance: \" << SdS . resonance << \" \\n \" ; cout << \"Using subtraction factor as integral of background fit \\n \" ; The macro.cpp now creates the SdS object and assign variables setted in settings.cpp. At this point, it creates all histograms that you will need such as invariant mass histograms and pT, eta, phi histograms. //Get data size and set data limit if has long long numberEntries = TreePC -> GetEntries (); if ( limitData > 0 && limitData < numberEntries ) numberEntries = limitData ; printf ( \"Data analysed = %lld of %lld \\n \" , numberEntries , TreePC -> GetEntries ()); //Prepare for showing progress string progressFormat = \"Progress: %05.2f%% %0\" + to_string ( strlen ( to_string ( numberEntries ). data ())) + \"lld/%lld \\r \" ; auto lastTime = std :: chrono :: steady_clock :: now (); auto start = std :: chrono :: steady_clock :: now (); Now the code are limiting data if you setted and setting a string for progress information while filling histograms. cout << \" \\n Filling Invariant Mass Histograms..... (1/2) \\n \" ; //Loop between the components for ( long long i = 0 ; i < numberEntries ; i ++ ) { //Select particle pair TreePC -> GetEntry ( i ); TreeAT -> GetEntry ( i ); //Show progress on screen if ( chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - lastTime ). count () >= 1000 || i == numberEntries - 1 ) { printf ( progressFormat . data (), ( float )( i + 1 ) / ( float ) numberEntries * 100 , i + 1 , numberEntries ); lastTime = chrono :: steady_clock :: now (); } //Fill histograms if ( applyCuts ( quantities , types )) { SdS . fillMassHistograms ( quantities , types ); } } cout << \" \\n Took \" << chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - start ). count () << \" ms \\n \" ; This part of the code fill invariant mass histograms. Cuts are applyied in cuts.h. At this point, macro.cpp separes in passing and all muons. //Do function fit over the histogram SdS . doFit (); //Get values for invariant mass and sigma from plot SdS . updateMassValuesAll (); After filling mass histograms, it is necessary to apply the fit function. After doing fit, updateMassValuesAll() get regions for sideband subtraction mostly based in fitting. //------------------------------------- // Generate and save files //------------------------------------- //Create file root to store generated files TFile * generatedFile = TFile :: Open (( string ( directoryToSave ) + \"generated_hist.root\" ). data (), \"RECREATE\" ); generatedFile -> mkdir ( \"canvas/\" ); generatedFile -> cd ( \"canvas/\" ); if ( shouldDrawInvariantMassCanvas ) { bool drawRegions = false ; bool shouldWrite = true ; bool shouldSavePNG = true ; SdS . createMassCanvas ( drawRegions , shouldWrite , shouldSavePNG ); } if ( shouldDrawInvariantMassCanvasRegion && ! isMC ) { bool drawRegions = true ; bool shouldWrite = true ; bool shouldSavePNG = true ; SdS . createMassCanvas ( drawRegions , shouldWrite , shouldSavePNG ); } Canvas are drawn and saved in the generated_hist.root file and in the folder as .png . //Prepare for showing progress lastTime = std :: chrono :: steady_clock :: now (); start = std :: chrono :: steady_clock :: now (); cout << \" \\n Filling Quantities Histograms..... (2/2) \\n \" ; //Loop between the components again for ( long long i = 0 ; i < numberEntries ; i ++ ) { //Select particle pair TreePC -> GetEntry ( i ); TreeAT -> GetEntry ( i ); //Show progress on screen if ( chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - lastTime ). count () >= 1000 || i == numberEntries - 1 ) { printf ( progressFormat . data (), ( float )( i + 1 ) / ( float ) numberEntries * 100 , i + 1 , numberEntries ); lastTime = chrono :: steady_clock :: now (); } //Fill histograms if ( applyCuts ( quantities , types )) { SdS . fillQuantitiesHistograms ( quantities , types ); } } cout << \" \\n Took \" << chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - start ). count () << \" ms \\n \" ; At this point of the code, this will separate all histogram in signal + background (signal region) and background (sideband region) due the regions for sideband choosen before. //Normalize Histograms for variable binning cout << \" \\n \" ; SdS . normalizeHistograms (); After folling histograms, as some of them has variable bins, it needs to be normalized. This function does this. //For sideband subtraction SdS . subtractSigHistograms (); Subtract background from signal + background histogram to create signal histogram. This method is what is called sideband subtraction . if ( shouldDrawQuantitiesCanvas ) { bool shouldWrite = true ; bool shouldSavePNG = true ; cout << endl ; SdS . createQuantitiesCanvas ( shouldWrite , shouldSavePNG ); } The code here draw the canvas for all pT, eta and phi quantities it has. Including background , signal and signal + background . //Debug consistency for histograms SdS . consistencyDebugCout (); This is a checker of how consistent is our result values and print on terminal results. For all histograms this calculations should result 0. For more details about how exactly it works, see consistencyDebugCout() . //Save histograms generatedFile -> mkdir ( \"histograms/\" ); generatedFile -> cd ( \"histograms/\" ); //Write quantities histograms on file { bool writehSigBack = true ; bool writehSig = true ; bool writehBack = true ; SdS . writeQuantitiesHistogramsOnFile ( writehSigBack , writehSig , writehBack ); } //Write mass histograms on file { bool writehPass = true ; bool writehAll = true ; SdS . writeMassHistogramsOnFile ( writehPass , writehAll ); } At this point, the code will write all histograms in a folder in the .root generated file. Including mass histograms and quantities histograms. //Save plots generatedFile -> mkdir ( \"efficiency/plots/\" ); generatedFile -> cd ( \"efficiency/plots/\" ); //Creates efficiency plots { bool shouldWrite = true ; SdS . createEfficiencyPlot ( shouldWrite ); } It calculates the efficiency of the quantities by using TEfficiency class of ROOT. Then saves the plots in another folder inside the .root file. //Saves new histograms and canvas in file generatedFile -> mkdir ( \"efficiency/canvas/\" ); generatedFile -> cd ( \"efficiency/canvas/\" ); if ( shouldDrawEfficiencyCanvas ) { bool shouldWrite = true ; bool shouldSavePNG = true ; cout << \" \\n \" ; SdS . createEfficiencyCanvas ( shouldWrite , shouldSavePNG ); } //Close files generatedFile -> Close (); cout << \" \\n Done. All result files can be found at \\\" \" << SdS . directoryToSave << \" \\\"\\n\\n \" ; The end point of this function. It creates a canvas for every efficiency plot calculated above and also saves in the generated file. After this, the task is done. Results \u00b6 All results are saved in a folder setted in directoryToSave variable. The result contains a file .root with all canvas, histograms and plots aside of .png images of all canvas created.","title":"The Macro"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#the-macro","text":"A macro is a code file create to be interpreted by a program. In this case, ROOT program will interpret it. The main code of this tool is in the file macro.ccp . In this section what compose this file is explained in details. It is important to note that this code has been tested on root 6.22/00.","title":"The Macro"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#about-the-code","text":"macro.cpp is a example how to use Sideband Subtraction to get reconstruction efficiencies for a Tag & Probe ntupple. It analyzes J/psi and Upsilon reconstruction efficiency for tracker , standalone and global muons. The file is encountered in folder main . Now, I going to talk about what this function do and how it does in the text below. Dataset The datasets used in this code are obtained with the main code of this Tag an Probe tool.","title":"About the code"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#classes-list","text":"There are some classes in Sideband Subtraction Tag And Probe project and they are distributed in these files with same name: Static functions : FitFunctions Primary Merged Jpsi Upsilon Classes and struct : SidebandSubtraction Type InvariantMass MassValues TagProbe PtEtaPhi PassingFailing This format shows what nested classes. Classes or structs below slided at right represents they are nested with the class above it.","title":"Classes list"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#sideband-subtraction-code-structure","text":"The diagram below represents the structure of objects in code. At left we have the structure of objects name. At right we have the correspondent class name of objects in these line. Also in Mass object we have: Notice that all objects in same line shares the same structure.","title":"Sideband Subtraction code structure"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#before-macrocpp","text":"There are some files in folder config aside of macro.ccp . The sections below explain about them.","title":"Before macro.cpp"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#cutsh","text":"This is it content: //This files holds some functions used in macro.cpp for particle selection //Return if is a accepted particle or no bool applyCuts ( double ** quantities , int ** types ) { //Assign variables for easy visualization double & ProbeMuon_Pt = * quantities [ 0 ]; double & ProbeMuon_Eta = * quantities [ 1 ]; double & ProbeMuon_Phi = * quantities [ 2 ]; double & TagMuon_Pt = * quantities [ 3 ]; double & TagMuon_Eta = * quantities [ 4 ]; double & TagMuon_Phi = * quantities [ 5 ]; double & InvariantMass = * quantities [ 6 ]; int & PassingProbeTrackingMuon = * types [ 0 ]; int & PassingProbeStandAloneMuon = * types [ 1 ]; int & PassingProbeGlobalMuon = * types [ 2 ]; //Apply cuts if ( TagMuon_Pt >= 7.0 && fabs ( TagMuon_Eta ) <= 2.4 ) return true ; return false ; } It stores the function applyCuts(), where return true for allowed pair of particles and false for not allowed.","title":"cuts.h"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#createhistogramh","text":"This file is called in PassingFailing.cpp and set quantity histograms bins and create the hitogram. Its default content is shwon bellow: void createHistogram ( TH1D * & histo , const char * histoName ) { //Set parameters string hName = string ( particleType ) + string ( passingOrFailing ) + string ( tagOrProbe ) + string ( particleName ) + \"_\" + string ( quantityName ) + string ( histoName ); string hTitle = string ( passingOrFailing ) + \" in \" + string ( particleType ) + \" \" + string ( tagOrProbe ); string xAxisTitle = string ( xAxisName ); string yAxisTitleForm = \"Events\" ; //Add unit if has if ( strcmp ( quantityUnit , \"\" ) != 0 ) xAxisTitle += \" [\" + string ( quantityUnit ) + \"]\" ; //Change title is passing if ( strcmp ( passingOrFailing , \"Passing\" ) == 0 ) hTitle = string ( particleType ) + \" \" + string ( particleName ) + \" \" + string ( tagOrProbe ); if ( strcmp ( passingOrFailing , \"All\" ) == 0 ) hTitle = \"All \" + string ( particleName ) + \" \" + string ( tagOrProbe ); //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Edit histogram axis histo -> GetYaxis () -> SetTitle ( Form ( yAxisTitleForm . data (), histo -> GetBinWidth ( 0 ))); histo -> GetXaxis () -> SetTitle ( xAxisTitle . data ()); }","title":"createHistogram.h"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#settingscpp","text":"It stores many configurations used in macro.cpp : //List of files const char * files [] = { \"../data_histoall.root\" , \"../Run2011AMuOnia_mergeNtuple.root\" , \"../JPsiToMuMu_mergeMCNtuple.root\" , \"../Run2011A_MuOnia_Upsilon.root\" , \"../Upsilon1SToMuMu_MC_full.root\" }; const char * directoriesToSave [] = { \"../results/result/\" , \"../results/Jpsi_Run_2011/\" , \"../results/Jpsi_MC_2020_sbs/\" , \"../results/Upsilon_Run_2011/\" , \"../results/Upsilon_MC_2020_sbs/\" }; //MAIN OPTIONS //Which file of files (variable above) should use int useFile = 4 ; //Set the canvasW wtermark const char * canvasWatermark = \"#bf{CMS Open Data}\" ; //Path where is going to save results const char * directoryToSave = directoriesToSave [ useFile ]; //directoryToSave = \"../result/\"; //Should limit data? long long limitData = 0 ; //0 -> do not limit //Canvas drawing bool shouldDrawInvariantMassCanvas = true ; bool shouldDrawInvariantMassCanvasRegion = true ; bool shouldDrawQuantitiesCanvas = true ; bool shouldDrawEfficiencyCanvas = true ; //Muon id anlyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //Muon label anlyse bool doTagMuon = false ; bool doProbeMuon = true ; //ENDED MAIN OPTIONS And then there are more automatically set options: //Auto detect resonance due file index const char * resonance = \"Jpsi\" ; if ( useFile > 2 ) resonance = \"Upsilon\" ; if ( useFile == 4 ) resonance = \"Upsilon1S\" ; //Auto detect limit of data if ( limitData > 0 ) directoryToSave = \"../partial_result/\" ; //Compatibility adjusts on file read (for data_histoall ntupples) bool needsRetroCompatibility = false ; if ( useFile == 0 ) needsRetroCompatibility = true ;","title":"settings.cpp"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#code-explained-in-parts","text":"macro.cpp is the main file of this program. Its the main code. It is explained in parts below: //Input files, options are set here! #include \"config/settings.cpp\" It imports configurations about macro.cpp //Check if the name of dir is ok if ( string ( directoryToSave ). back () != string ( \"/\" )) { cerr << \"To avoid errors, please end the result directory with a \\\" / \\\" \" << endl ; abort (); } //Check if dir exists and create if ( gSystem -> AccessPathName ( directoryToSave )) { if ( gSystem -> mkdir ( directoryToSave , true )) { cerr << \" \\\" \" << directoryToSave << \" \\\" path could not be found and could not be created ERROR\" << endl ; cerr << \"Try to create manually this folder path\" << endl ; abort (); } else { cout << \" \\\" \" << directoryToSave << \" \\\" directory created OK\" << endl ; } } else { cout << \" \\\" \" << directoryToSave << \" \\\" directory OK\" << endl ; } Check if the directoryToSave (setted in settings.cpp) has a valid name and if exists. If not, the code creates the folder. //Compatibility adjusts on file read (for data_histoall ntupples) string folderName = \"tagandprobe/\" ; if ( needsRetroCompatibility ) folderName = \"demo/\" ; //Open and read files TFile * file0 = TFile :: Open ( files [ useFile ]); TTree * TreePC = ( TTree * ) file0 -> Get (( folderName + \"PlotControl\" ). data ()); TTree * TreeAT = ( TTree * ) file0 -> Get (( folderName + \"AnalysisTree\" ). data ()); cout << \"Using \\\" \" << files [ useFile ] << \" \\\" ntupple\" << endl ; This part is responsible to open the file and do conversions. The first one file is a bit different of the other ones, so it needs compatibiliy besides its not important anymore and is a obsolete file. //Create variables double ProbeMuon_Pt ; double ProbeMuon_Eta ; double ProbeMuon_Phi ; double TagMuon_Pt ; double TagMuon_Eta ; double TagMuon_Phi ; double InvariantMass ; int PassingProbeTrackingMuon ; int PassingProbeStandAloneMuon ; int PassingProbeGlobalMuon ; //Assign variables TreePC -> SetBranchAddress ( \"ProbeMuon_Pt\" , & ProbeMuon_Pt ); TreePC -> SetBranchAddress ( \"ProbeMuon_Eta\" , & ProbeMuon_Eta ); TreePC -> SetBranchAddress ( \"ProbeMuon_Phi\" , & ProbeMuon_Phi ); TreePC -> SetBranchAddress ( \"TagMuon_Pt\" , & TagMuon_Pt ); TreePC -> SetBranchAddress ( \"TagMuon_Eta\" , & TagMuon_Eta ); TreePC -> SetBranchAddress ( \"TagMuon_Phi\" , & TagMuon_Phi ); if ( needsRetroCompatibility ) TreePC -> SetBranchAddress ( \"InvariantMass\" , & InvariantMass ); else TreeAT -> SetBranchAddress ( \"InvariantMass\" , & InvariantMass ); TreeAT -> SetBranchAddress ( \"PassingProbeTrackingMuon\" , & PassingProbeTrackingMuon ); TreeAT -> SetBranchAddress ( \"PassingProbeStandAloneMuon\" , & PassingProbeStandAloneMuon ); TreeAT -> SetBranchAddress ( \"PassingProbeGlobalMuon\" , & PassingProbeGlobalMuon ); double * quantities [] = { & ProbeMuon_Pt , & ProbeMuon_Eta , & ProbeMuon_Phi , & TagMuon_Pt , & TagMuon_Eta , & TagMuon_Phi , & InvariantMass , }; int * types [] = { & PassingProbeTrackingMuon , & PassingProbeStandAloneMuon , & PassingProbeGlobalMuon }; Now variables are created and linked to branches in ntupple. Then a array of these variables are set. //Create a object and set configs SidebandSubtraction SdS { resonance }; SdS . canvasWatermark = canvasWatermark ; SdS . directoryToSave = directoryToSave ; SdS . doTracker = doTracker ; SdS . doStandalone = doStandalone ; SdS . doGlobal = doGlobal ; SdS . doTagMuon = doTagMuon ; SdS . doProbeMuon = doProbeMuon ; cout << \"resonance: \" << SdS . resonance << \" \\n \" ; cout << \"Using subtraction factor as integral of background fit \\n \" ; The macro.cpp now creates the SdS object and assign variables setted in settings.cpp. At this point, it creates all histograms that you will need such as invariant mass histograms and pT, eta, phi histograms. //Get data size and set data limit if has long long numberEntries = TreePC -> GetEntries (); if ( limitData > 0 && limitData < numberEntries ) numberEntries = limitData ; printf ( \"Data analysed = %lld of %lld \\n \" , numberEntries , TreePC -> GetEntries ()); //Prepare for showing progress string progressFormat = \"Progress: %05.2f%% %0\" + to_string ( strlen ( to_string ( numberEntries ). data ())) + \"lld/%lld \\r \" ; auto lastTime = std :: chrono :: steady_clock :: now (); auto start = std :: chrono :: steady_clock :: now (); Now the code are limiting data if you setted and setting a string for progress information while filling histograms. cout << \" \\n Filling Invariant Mass Histograms..... (1/2) \\n \" ; //Loop between the components for ( long long i = 0 ; i < numberEntries ; i ++ ) { //Select particle pair TreePC -> GetEntry ( i ); TreeAT -> GetEntry ( i ); //Show progress on screen if ( chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - lastTime ). count () >= 1000 || i == numberEntries - 1 ) { printf ( progressFormat . data (), ( float )( i + 1 ) / ( float ) numberEntries * 100 , i + 1 , numberEntries ); lastTime = chrono :: steady_clock :: now (); } //Fill histograms if ( applyCuts ( quantities , types )) { SdS . fillMassHistograms ( quantities , types ); } } cout << \" \\n Took \" << chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - start ). count () << \" ms \\n \" ; This part of the code fill invariant mass histograms. Cuts are applyied in cuts.h. At this point, macro.cpp separes in passing and all muons. //Do function fit over the histogram SdS . doFit (); //Get values for invariant mass and sigma from plot SdS . updateMassValuesAll (); After filling mass histograms, it is necessary to apply the fit function. After doing fit, updateMassValuesAll() get regions for sideband subtraction mostly based in fitting. //------------------------------------- // Generate and save files //------------------------------------- //Create file root to store generated files TFile * generatedFile = TFile :: Open (( string ( directoryToSave ) + \"generated_hist.root\" ). data (), \"RECREATE\" ); generatedFile -> mkdir ( \"canvas/\" ); generatedFile -> cd ( \"canvas/\" ); if ( shouldDrawInvariantMassCanvas ) { bool drawRegions = false ; bool shouldWrite = true ; bool shouldSavePNG = true ; SdS . createMassCanvas ( drawRegions , shouldWrite , shouldSavePNG ); } if ( shouldDrawInvariantMassCanvasRegion && ! isMC ) { bool drawRegions = true ; bool shouldWrite = true ; bool shouldSavePNG = true ; SdS . createMassCanvas ( drawRegions , shouldWrite , shouldSavePNG ); } Canvas are drawn and saved in the generated_hist.root file and in the folder as .png . //Prepare for showing progress lastTime = std :: chrono :: steady_clock :: now (); start = std :: chrono :: steady_clock :: now (); cout << \" \\n Filling Quantities Histograms..... (2/2) \\n \" ; //Loop between the components again for ( long long i = 0 ; i < numberEntries ; i ++ ) { //Select particle pair TreePC -> GetEntry ( i ); TreeAT -> GetEntry ( i ); //Show progress on screen if ( chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - lastTime ). count () >= 1000 || i == numberEntries - 1 ) { printf ( progressFormat . data (), ( float )( i + 1 ) / ( float ) numberEntries * 100 , i + 1 , numberEntries ); lastTime = chrono :: steady_clock :: now (); } //Fill histograms if ( applyCuts ( quantities , types )) { SdS . fillQuantitiesHistograms ( quantities , types ); } } cout << \" \\n Took \" << chrono :: duration_cast < chrono :: milliseconds > ( chrono :: steady_clock :: now () - start ). count () << \" ms \\n \" ; At this point of the code, this will separate all histogram in signal + background (signal region) and background (sideband region) due the regions for sideband choosen before. //Normalize Histograms for variable binning cout << \" \\n \" ; SdS . normalizeHistograms (); After folling histograms, as some of them has variable bins, it needs to be normalized. This function does this. //For sideband subtraction SdS . subtractSigHistograms (); Subtract background from signal + background histogram to create signal histogram. This method is what is called sideband subtraction . if ( shouldDrawQuantitiesCanvas ) { bool shouldWrite = true ; bool shouldSavePNG = true ; cout << endl ; SdS . createQuantitiesCanvas ( shouldWrite , shouldSavePNG ); } The code here draw the canvas for all pT, eta and phi quantities it has. Including background , signal and signal + background . //Debug consistency for histograms SdS . consistencyDebugCout (); This is a checker of how consistent is our result values and print on terminal results. For all histograms this calculations should result 0. For more details about how exactly it works, see consistencyDebugCout() . //Save histograms generatedFile -> mkdir ( \"histograms/\" ); generatedFile -> cd ( \"histograms/\" ); //Write quantities histograms on file { bool writehSigBack = true ; bool writehSig = true ; bool writehBack = true ; SdS . writeQuantitiesHistogramsOnFile ( writehSigBack , writehSig , writehBack ); } //Write mass histograms on file { bool writehPass = true ; bool writehAll = true ; SdS . writeMassHistogramsOnFile ( writehPass , writehAll ); } At this point, the code will write all histograms in a folder in the .root generated file. Including mass histograms and quantities histograms. //Save plots generatedFile -> mkdir ( \"efficiency/plots/\" ); generatedFile -> cd ( \"efficiency/plots/\" ); //Creates efficiency plots { bool shouldWrite = true ; SdS . createEfficiencyPlot ( shouldWrite ); } It calculates the efficiency of the quantities by using TEfficiency class of ROOT. Then saves the plots in another folder inside the .root file. //Saves new histograms and canvas in file generatedFile -> mkdir ( \"efficiency/canvas/\" ); generatedFile -> cd ( \"efficiency/canvas/\" ); if ( shouldDrawEfficiencyCanvas ) { bool shouldWrite = true ; bool shouldSavePNG = true ; cout << \" \\n \" ; SdS . createEfficiencyCanvas ( shouldWrite , shouldSavePNG ); } //Close files generatedFile -> Close (); cout << \" \\n Done. All result files can be found at \\\" \" << SdS . directoryToSave << \" \\\"\\n\\n \" ; The end point of this function. It creates a canvas for every efficiency plot calculated above and also saves in the generated file. After this, the task is done.","title":"Code explained in parts"},{"location":"analysis/selection/idefficiencystudy/sidebandreferenceguide/macro/#results","text":"All results are saved in a folder setted in directoryToSave variable. The result contains a file .root with all canvas, histograms and plots aside of .png images of all canvas created.","title":"Results"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/","text":"Introduction \u00b6 What is the tag and probe method? \u00b6 The tag and probe method is a data-driven technique for measuring particle detection efficiencies. It is based on the decays of known resonances (e.g. J/\u03c8, \u03a5 and Z) to pairs of the particles being studied. In this exercise, these particles are muons, and the J/\u03c8 resonance is nominally used. The determination of the detector efficiency is a critical ingredient in any physics measurement. It accounts for the particles that were produced in the collision but escaped detection (did not reach the detector elements, were missed by the reconstructions algorithms, etc). It can be in general estimated using simulations, but simulations need to be calibrated with data. The T&P method here described provides a useful and elegant mechanism for extracting efficiencies directly from data! . What is \"tag\" and \"probe\"? \u00b6 The resonance, used to calculate the efficiencies, decays to a pair of particles: the tag and the probe. Tag muon = well identified, triggered muon (tight selection criteria). Probe muon = unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the efficiency is to be measured. How do we calculate the efficiency? \u00b6 The efficiency is given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which we explain below ): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria. The tag+probe invariant mass distribution is used to select only signal, that is, only true J/\u03c8 candidates decaying to dimuons. This is achieved in this exercise by the usage of two methods: fitting and side-band-subtraction . CMS Muon identification and reconstruction \u00b6 The final objective in this lesson is to measure the efficiency for identifying reconstructed tracker muons . We present here a short description of the muon identification and reconstruction employed in the CMS experiment at the LHC. In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, two reconstruction approaches are used: Tracker Muon reconstruction (red line): In this approach, all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction (green line): they are all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes - DTs in the barrel region, Cathode strip chambers - CSCs in the endcaps and Resistive Plates Chambers - RPCs for all muon system) are used to generate \"seeds\" consisting of position and direction vec01/tors and an estimate of the muon transverse momentum; Global Muon reconstruction (blue line): For each standalone-muon track, a matching tracker track is found by comparing parameters of the two tracks propagated onto a common surface. You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"Introduction"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#introduction","text":"","title":"Introduction"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#what-is-the-tag-and-probe-method","text":"The tag and probe method is a data-driven technique for measuring particle detection efficiencies. It is based on the decays of known resonances (e.g. J/\u03c8, \u03a5 and Z) to pairs of the particles being studied. In this exercise, these particles are muons, and the J/\u03c8 resonance is nominally used. The determination of the detector efficiency is a critical ingredient in any physics measurement. It accounts for the particles that were produced in the collision but escaped detection (did not reach the detector elements, were missed by the reconstructions algorithms, etc). It can be in general estimated using simulations, but simulations need to be calibrated with data. The T&P method here described provides a useful and elegant mechanism for extracting efficiencies directly from data! .","title":"What is the tag and probe method?"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#what-is-tag-and-probe","text":"The resonance, used to calculate the efficiencies, decays to a pair of particles: the tag and the probe. Tag muon = well identified, triggered muon (tight selection criteria). Probe muon = unbiased set of muon candidates (very loose selection criteria), either passing or failing the criteria for which the efficiency is to be measured.","title":"What is \"tag\" and \"probe\"?"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#how-do-we-calculate-the-efficiency","text":"The efficiency is given by the fraction of probe muons that pass a given criteria (in this case, the Muon ID which we explain below ): The denominator corresponds to the number of resonance candidates (tag+probe pairs) reconstructed in the dataset. The numerator corresponds to the subset for which the probe passes the criteria. The tag+probe invariant mass distribution is used to select only signal, that is, only true J/\u03c8 candidates decaying to dimuons. This is achieved in this exercise by the usage of two methods: fitting and side-band-subtraction .","title":"How do we calculate the efficiency?"},{"location":"analysis/selection/idefficiencystudy/tutorial/01-introduction/#cms-muon-identification-and-reconstruction","text":"The final objective in this lesson is to measure the efficiency for identifying reconstructed tracker muons . We present here a short description of the muon identification and reconstruction employed in the CMS experiment at the LHC. In the standard CMS reconstruction for proton-proton collisions, tracks are first reconstructed independently in the inner tracker and in the muon system. Based on these objects, two reconstruction approaches are used: Tracker Muon reconstruction (red line): In this approach, all tracker tracks with pT > 0.5 GeV/c and total momentum p > 2.5 GeV/c are considered as possible muon candidates and are extrapolated to the muon system taking into account the magnetic field; Standalone Muon reconstruction (green line): they are all tracks of the segments reconstructed in the muon chambers (performed using segments and hits from Drift Tubes - DTs in the barrel region, Cathode strip chambers - CSCs in the endcaps and Resistive Plates Chambers - RPCs for all muon system) are used to generate \"seeds\" consisting of position and direction vec01/tors and an estimate of the muon transverse momentum; Global Muon reconstruction (blue line): For each standalone-muon track, a matching tracker track is found by comparing parameters of the two tracks propagated onto a common surface. You can find more details concerning CMS Muon Identification and reconstruction in this paper JINST 7 (2012) P10002 .","title":"CMS Muon identification and reconstruction"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/","text":"Fitting \u00b6 Setting it up \u00b6 In order to run this exercise you do not really need to be in a CMSSW area. It would be actually better if you worked outside your usual CMSSW_5_3_32 environment. So, if, for instance, you are working with the Docker container, instead of working on /home/cmsusr/CMSSW_5_3_32/src you could work on any directory you can create at the /home/cmsusr level. Alternatively, you could work directly on your own host machine if you managed to install ROOT on it. For this example we assume you will be working in either the Docker container or the virtual machine. Since we will be needing ROOT version greater than 6 and this code has been tested in root version 6.22/00, do not forget to set it up from LCG (as you learned in the ROOT pre-exercise) by doing: source /cvmfs/sft.cern.ch/lcg/views/LCG_98/x86_64-slc6-gcc8-opt/setup.sh It is important to metion that codes used in this tutorial uses ROOT batch. So, if you are connecting to a remote compute, do not forget about using -X parameter like so: ssh -X your@remote.computer To get sure if everything will run alright, you might want to try this command: root gROOT->IsBatch () .q If the output has the true value, everything was setted up. Now, clone the repository and go to the fitting method tutorial: git clone git://github.com/allanjales/TagAndProbe cd TagAndProbe/efficiency_tools/fitting You will also need to download the file TagAndProbe_Jpsi_Run2011.root from folder simplified_datasets_for_fitting_method using this link and put it on the DATA folder in your local area. To do so, just run commands below. These files requires 204 MB and 33 MB respectively: wget -O DATA/TagAndProbe_Jpsi_Run2011.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?path=%2Fsimplified_datasets_for_fitting_method&files=TagAndProbe_Jpsi_Run2011.root\" wget -O DATA/TagAndProbe_Jpsi_MC.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?path=%2Fsimplified_datasets_for_fitting_method&files=TagAndProbe_Jpsi_MC.root\" If you are trying to use other ntuple and it does not have the simplified version of that, it should be simplified with simplify_data.cpp in order to run the fitting method over it. Details about this process can be found at overview page of reference guide for fitting method . The Fitting Method \u00b6 First, a brief explanation of the method we\u2019ll be studying. It consists on fitting the invariant mass of the tag & probe pairs, in the two categories: passing probes, and all probes. I.e., for the unbiased leg of the decay, one can apply a selection criteria (a set of cuts) and determine whether the object passes those criteria or not. The procedure is applied after splitting the data in bins of a kinematic variable of the probe object (e.g. the traverse momentum, pT); as such, the efficiency will be measured as a function of that quantity for each of the bins. So, in the picture below, on the left, let's imagine that the pT bin we are selecting is the one marked in red. But, of course, in that bin (like in the rest) you will have true J/\u03c8; decays as well as muon pairs from other processes (maybe QCD, for instance). The true decays would make up our signal , whereas the other events will be considered the background . The fit, which is made in a different space (the invariant mass space) allows to statistically discriminate between signal and background. To compute the efficiency we simply divide the signal yield from the fits to the passing category by the signal yield from the fit of the inclusive (All) category. This approach is depicted in the middle and right-hand plots of the image below. At the end of the day, then, you will have to make these fits for each bin in the range of interest. Let's start exploring our dataset. From the cloned directory, type: cd DATA root -l TagAndProbe_Jpsi_MC.root If everything's right, you should get something like: Attaching file TagAndProbe_Jpsi_MC.root as _file0... (TFile *) 0x563c69a68d90 Of course, you can explore this file if you want using all the tools you learn in the ROOT. This file contains datas that were obtained using procedures the main tag and probe code. Note In the following plots, remember that the units of the x axis are in GeV/c. Now, before we start fitting the invariant mass it's important to look at it's shape first. To visualize our data's invariant mass, do (within ROOT): tagandprobe -> Draw ( \"InvariantMass\" ) If you got the previous result, we're ready to go. The dataset used in this exercise has been collected by the CMS experiment, in proton-proton collisions at the LHC. It contains 1572153 entries (muon pair candidates) with an associated invariant mass. For each candidate, the transverse momentum (pT), rapidity(\u03b7) and azimuthal angle (\u03c6) are stored, along with a binary flag PassingProbeTrackingMuon , which is 1 in case the corresponding probe satisfied the tracker muon selection criteria and 0 in case it doesn't. Note Note that it does not really matter what kind of selection criteria these ntuples were created with. The procedure would be the same. You can create your own, similar ntuples with the criteria that you need to study. As you may have seen, after exploring the content of the root file, the tagandprobe tree has these variables: Type Name double InvarianMass double ProbeMuon_Pt double ProbeMuon_Eta double ProbeMuon_Phi int PassingProbeTrackingMuon int PassingProbeStandAloneMuon int PassingProbeGlobalMuon We'll start by calculating the efficiency as a function of pT. It is useful to have an idea of the distribution of the quantity we want to study. In order to do this, we\u2019ll repeat the steps previously used to plot the invariant mass, but now for the ProbeMuon_Pt variable. tagandprobe -> Draw ( \"ProbeMuon_Pt\" ) Hmm... seems like our domain is larger than we need it to be. To fix this, we can apply a constraint to our plot. Try: tagandprobe -> Draw ( \"ProbeMuon_Pt\" , \"ProbeMuon_Pt < 20\" ) Exit ROOT and get back to the main area: .q cd .. Now that you're acquainted with the data, open the efficiency.cpp file and make some small adjustments to the code in this section: We'll start by choosing the desired muon id and bins for it the transverse momentum, so the lines that shouldn't be commented are the \"trackerMuon\" and the first \"Pt\" as shown below. All other lines in this section should be commented as shown below: //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; In lines above, quantity express which quantity we are looking to measure. The array found at its side named double bins[] express the bins we are making in this process. For example, on the pT histogram, the firsts three bins will be: [0.0,2.0), [2.0,3.4) and [4.0, 4.4). Change it if you want to have fun but maybe a unexpected result. Our choosen bin were already tested to get us a good result. The Probability Density Function used for modeling signal and background \u00b6 We execute a simultaneous fit using a Gaussian curve and a Crystall Ball function. For the background we use a Exponential. The function used, doFit_Jpsi_Run_h() , is implemented in the source file src/dofits/doFit_Jpsi_Run.h . The fitting and storing of the fit output of each bin is achieved by the following loop in the efficiency.cpp code. int nbins = sizeof ( bins ) / sizeof ( * bins ) - 1 ; double ** yields_n_errs = new double * [ nbins ]; for ( int i = 0 ; i < nbins ; i ++ ) { //Creates conditions string conditions = string ( \"ProbeMuon_\" + quantity + \">=\" + to_string ( bins [ i ] )); conditions += string ( \" && ProbeMuon_\" + quantity + \"< \" + to_string ( bins [ i + 1 ])); //Stores [yield_all, yield_pass, err_all, err_pass] yields_n_errs [ i ] = doFit ( conditions , MuonId , path_bins_fit_folder ); } To get the efficiency plot, we use the TEfficiency class from ROOT. You'll see that in order to create a TEfficiency object, one of the constructors requires two TH1 objects (two one-dimensional histograms). One with all the probes and one with only the passing probes. The creation of these TH1 objects is taken care of by the src/make_TH1D.h code. Check out src/make_TH1D.h TH1D * make_TH1D ( string name , double ** values , int index , double * bins , int nbins , string quantity = \"\" , bool draw = false ) { //AddBinContent //HISTOGRAM NEEDS TO HAVE VARIABLE BINS TH1D * hist = new TH1D ( name . c_str (), name . c_str (), nbins , bins ); hist -> GetYaxis () -> SetTitle ( \"Events\" ); if ( quantity == \"Pt\" ) hist -> GetXaxis () -> SetTitle ( \"p_{T} [GeV/c]\" ); else if ( quantity == \"Eta\" ) hist -> GetXaxis () -> SetTitle ( \"#eta\" ); else if ( quantity == \"Phi\" ) hist -> GetXaxis () -> SetTitle ( \"rad\" ); for ( int i = 0 ; i < nbins ; i ++ ) { hist -> SetBinContent ( i + 1 , values [ i ][ index ]); hist -> SetBinError ( i + 1 , values [ i ][ index + 2 ]); //cout << i << \" -> (\" << hist->GetBinLowEdge(i+1) << \",\" << hist->GetBinLowEdge(i+1)+hist->GetBinWidth(i+1) << \") == \" << hist->GetBinContent(i+1) << \"\\n\"; } if ( draw ) { TCanvas * xperiment = new TCanvas ; xperiment -> cd (); hist -> Draw (); } return hist ; } } To plot the efficiency we used the src/get_efficiency.h function. Check out get_efficiency.h TEfficiency * get_efficiency ( TH1D * all , TH1D * pass , string quantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) { //Copy histograms to change axis titles later TH1D * pass_copy = ( TH1D * ) pass -> Clone (); TH1D * all_copy = ( TH1D * ) all -> Clone (); pass_copy -> GetYaxis () -> SetTitle ( \"Efficiency\" ); all_copy -> GetYaxis () -> SetTitle ( \"Efficiency\" ); TEfficiency * pEff = new TEfficiency (); pEff -> SetPassedHistogram ( * pass_copy , \"f\" ); pEff -> SetTotalHistogram ( * all_copy , \"f\" ); delete all_copy ; delete pass_copy ; //Set plot config if ( prefix_name != \"\" ) { pEff -> SetName ( string ( MuonId + \"_\" + quantity + \"_\" + prefix_name + \"_Efficiency\" ). c_str ()); pEff -> SetTitle ( string ( \"Efficiency for \" + MuonId + \" \" + quantity + \" (\" + prefix_name + \")\" ). c_str ()); } else { pEff -> SetName ( string ( MuonId + \"_\" + quantity + \"_Efficiency\" ). c_str ()); pEff -> SetTitle ( string ( \"Efficiency for \" + MuonId + \" \" + quantity ). c_str ()); } pEff -> SetLineColor ( kBlack ); pEff -> SetMarkerStyle ( 21 ); pEff -> SetMarkerSize ( 0.5 ); pEff -> SetMarkerColor ( kBlack ); if ( shouldWrite ) pEff -> Write (); TCanvas * oi = new TCanvas (); oi -> cd (); pEff -> Draw (); gPad -> Update (); auto graph = pEff -> GetPaintedGraph (); graph -> SetMinimum ( 0.8 ); graph -> SetMaximum ( 1.2 ); gPad -> Update (); return pEff ; } Note that we load part of these functions in the src area directly in header of the efficiency.cpp code. Now that you understand what the efficiency.cpp macro does, run your code without flashing the scream ( -l ), with a batch mode ( -b ) and with a quit-when-done switch ( -q ): root -l -b -q efficiency.cpp Note Some [#1] INFO messages will pop up on the terminal. They are expected. Do not worry about it. There is some [#0] WARNING: that should pop up too. They say about ignoring some events. They appear due cuts we are making directly on variables limits. It is ok, just ignore it. If everything went correctly you should have a Pt_tracekrMuon.root in the path below. cd results/efficiencies/efficiency/Jpsi_Run_2011/ We will need to run the code again with a few changes so go back to the main file. cd ../../../.. Efficiency evaluation between real data and Monte Carlo simulations \u00b6 Now we need to create the Monte Carlo (MC) data to compare with the real one, in order to do that we will need to change the efficiency.cpp code, you will have to include the #include \"src/dofits/DoFit_Jpsi_MC.h\" and comment the #include \"src/dofits/DoFit_Jpsi_Run.h\" line. At the end, your code should look like this: //Change if you need //#include \"src/dofits/DoFit_Jpsi_Run.h\" #include \"src/dofits/DoFit_Jpsi_MC.h\" #include \"src/create_folder.h\" #include \"src/get_efficiency.h\" #include \"src/make_TH1D.h\" //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0., 40.}; Run the efficiency.cpp again and you will have 2 new folders at results/efficiencies/efficiency/ one Jpsi_MC_2020 and other Jpsi_Run_2011 . If you want to see each individual efficiency, you can use the new TBrowser command to open the trackerMuon_Pt_efficiency.root file. Now, in order to make the comparison between real and MC data we will need to run a code in folder tests named compare_efficiency.cpp . To run this command from where you are, first load this file on ROOT with the command below: root -l -b .L tests/compare_efficiency.cpp Now you have the function compare_efficiency(...) we made loaded on your root. This function have four arguments. The first one should be the muon identification we are looking for such as trackerMuon , standaloneMuon and globalMuon . THe second one refers to the quatity that we are analysing. In our case we are looking for \"Pt\", but could be \"Eta\" and \"Phi\". The third and forth parameter are paths which points to .root files created previously in this tutorial. So now, in order to make the comparison between real and MC data on tracker muon pT, you should run this command to get your result: compare_efficiency ( \"trackerMuon\" , \"Pt\" , \"results/efficiencies/efficiency/Jpsi_Run_2011/Pt_trackerMuon.root\" , \"results/efficiencies/efficiency/Jpsi_MC_2020/Pt_trackerMuon.root\" ) ; Now you should have a new folder called Comparison Run2011 vs MC in your results/efficiencies/efficiency/ and inside it are the comparisons that you just made. If everything went well and you still have time to go, repeat this process for the two other variables, \u03b7 and \u03c6! Note If you are stucked on root enviroment, do not forget about typing this command to exit it: .q","title":"Fitting"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/#fitting","text":"","title":"Fitting"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/#setting-it-up","text":"In order to run this exercise you do not really need to be in a CMSSW area. It would be actually better if you worked outside your usual CMSSW_5_3_32 environment. So, if, for instance, you are working with the Docker container, instead of working on /home/cmsusr/CMSSW_5_3_32/src you could work on any directory you can create at the /home/cmsusr level. Alternatively, you could work directly on your own host machine if you managed to install ROOT on it. For this example we assume you will be working in either the Docker container or the virtual machine. Since we will be needing ROOT version greater than 6 and this code has been tested in root version 6.22/00, do not forget to set it up from LCG (as you learned in the ROOT pre-exercise) by doing: source /cvmfs/sft.cern.ch/lcg/views/LCG_98/x86_64-slc6-gcc8-opt/setup.sh It is important to metion that codes used in this tutorial uses ROOT batch. So, if you are connecting to a remote compute, do not forget about using -X parameter like so: ssh -X your@remote.computer To get sure if everything will run alright, you might want to try this command: root gROOT->IsBatch () .q If the output has the true value, everything was setted up. Now, clone the repository and go to the fitting method tutorial: git clone git://github.com/allanjales/TagAndProbe cd TagAndProbe/efficiency_tools/fitting You will also need to download the file TagAndProbe_Jpsi_Run2011.root from folder simplified_datasets_for_fitting_method using this link and put it on the DATA folder in your local area. To do so, just run commands below. These files requires 204 MB and 33 MB respectively: wget -O DATA/TagAndProbe_Jpsi_Run2011.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?path=%2Fsimplified_datasets_for_fitting_method&files=TagAndProbe_Jpsi_Run2011.root\" wget -O DATA/TagAndProbe_Jpsi_MC.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?path=%2Fsimplified_datasets_for_fitting_method&files=TagAndProbe_Jpsi_MC.root\" If you are trying to use other ntuple and it does not have the simplified version of that, it should be simplified with simplify_data.cpp in order to run the fitting method over it. Details about this process can be found at overview page of reference guide for fitting method .","title":"Setting it up"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/#the-fitting-method","text":"First, a brief explanation of the method we\u2019ll be studying. It consists on fitting the invariant mass of the tag & probe pairs, in the two categories: passing probes, and all probes. I.e., for the unbiased leg of the decay, one can apply a selection criteria (a set of cuts) and determine whether the object passes those criteria or not. The procedure is applied after splitting the data in bins of a kinematic variable of the probe object (e.g. the traverse momentum, pT); as such, the efficiency will be measured as a function of that quantity for each of the bins. So, in the picture below, on the left, let's imagine that the pT bin we are selecting is the one marked in red. But, of course, in that bin (like in the rest) you will have true J/\u03c8; decays as well as muon pairs from other processes (maybe QCD, for instance). The true decays would make up our signal , whereas the other events will be considered the background . The fit, which is made in a different space (the invariant mass space) allows to statistically discriminate between signal and background. To compute the efficiency we simply divide the signal yield from the fits to the passing category by the signal yield from the fit of the inclusive (All) category. This approach is depicted in the middle and right-hand plots of the image below. At the end of the day, then, you will have to make these fits for each bin in the range of interest. Let's start exploring our dataset. From the cloned directory, type: cd DATA root -l TagAndProbe_Jpsi_MC.root If everything's right, you should get something like: Attaching file TagAndProbe_Jpsi_MC.root as _file0... (TFile *) 0x563c69a68d90 Of course, you can explore this file if you want using all the tools you learn in the ROOT. This file contains datas that were obtained using procedures the main tag and probe code. Note In the following plots, remember that the units of the x axis are in GeV/c. Now, before we start fitting the invariant mass it's important to look at it's shape first. To visualize our data's invariant mass, do (within ROOT): tagandprobe -> Draw ( \"InvariantMass\" ) If you got the previous result, we're ready to go. The dataset used in this exercise has been collected by the CMS experiment, in proton-proton collisions at the LHC. It contains 1572153 entries (muon pair candidates) with an associated invariant mass. For each candidate, the transverse momentum (pT), rapidity(\u03b7) and azimuthal angle (\u03c6) are stored, along with a binary flag PassingProbeTrackingMuon , which is 1 in case the corresponding probe satisfied the tracker muon selection criteria and 0 in case it doesn't. Note Note that it does not really matter what kind of selection criteria these ntuples were created with. The procedure would be the same. You can create your own, similar ntuples with the criteria that you need to study. As you may have seen, after exploring the content of the root file, the tagandprobe tree has these variables: Type Name double InvarianMass double ProbeMuon_Pt double ProbeMuon_Eta double ProbeMuon_Phi int PassingProbeTrackingMuon int PassingProbeStandAloneMuon int PassingProbeGlobalMuon We'll start by calculating the efficiency as a function of pT. It is useful to have an idea of the distribution of the quantity we want to study. In order to do this, we\u2019ll repeat the steps previously used to plot the invariant mass, but now for the ProbeMuon_Pt variable. tagandprobe -> Draw ( \"ProbeMuon_Pt\" ) Hmm... seems like our domain is larger than we need it to be. To fix this, we can apply a constraint to our plot. Try: tagandprobe -> Draw ( \"ProbeMuon_Pt\" , \"ProbeMuon_Pt < 20\" ) Exit ROOT and get back to the main area: .q cd .. Now that you're acquainted with the data, open the efficiency.cpp file and make some small adjustments to the code in this section: We'll start by choosing the desired muon id and bins for it the transverse momentum, so the lines that shouldn't be commented are the \"trackerMuon\" and the first \"Pt\" as shown below. All other lines in this section should be commented as shown below: //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; In lines above, quantity express which quantity we are looking to measure. The array found at its side named double bins[] express the bins we are making in this process. For example, on the pT histogram, the firsts three bins will be: [0.0,2.0), [2.0,3.4) and [4.0, 4.4). Change it if you want to have fun but maybe a unexpected result. Our choosen bin were already tested to get us a good result.","title":"The Fitting Method"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/#the-probability-density-function-used-for-modeling-signal-and-background","text":"We execute a simultaneous fit using a Gaussian curve and a Crystall Ball function. For the background we use a Exponential. The function used, doFit_Jpsi_Run_h() , is implemented in the source file src/dofits/doFit_Jpsi_Run.h . The fitting and storing of the fit output of each bin is achieved by the following loop in the efficiency.cpp code. int nbins = sizeof ( bins ) / sizeof ( * bins ) - 1 ; double ** yields_n_errs = new double * [ nbins ]; for ( int i = 0 ; i < nbins ; i ++ ) { //Creates conditions string conditions = string ( \"ProbeMuon_\" + quantity + \">=\" + to_string ( bins [ i ] )); conditions += string ( \" && ProbeMuon_\" + quantity + \"< \" + to_string ( bins [ i + 1 ])); //Stores [yield_all, yield_pass, err_all, err_pass] yields_n_errs [ i ] = doFit ( conditions , MuonId , path_bins_fit_folder ); } To get the efficiency plot, we use the TEfficiency class from ROOT. You'll see that in order to create a TEfficiency object, one of the constructors requires two TH1 objects (two one-dimensional histograms). One with all the probes and one with only the passing probes. The creation of these TH1 objects is taken care of by the src/make_TH1D.h code. Check out src/make_TH1D.h TH1D * make_TH1D ( string name , double ** values , int index , double * bins , int nbins , string quantity = \"\" , bool draw = false ) { //AddBinContent //HISTOGRAM NEEDS TO HAVE VARIABLE BINS TH1D * hist = new TH1D ( name . c_str (), name . c_str (), nbins , bins ); hist -> GetYaxis () -> SetTitle ( \"Events\" ); if ( quantity == \"Pt\" ) hist -> GetXaxis () -> SetTitle ( \"p_{T} [GeV/c]\" ); else if ( quantity == \"Eta\" ) hist -> GetXaxis () -> SetTitle ( \"#eta\" ); else if ( quantity == \"Phi\" ) hist -> GetXaxis () -> SetTitle ( \"rad\" ); for ( int i = 0 ; i < nbins ; i ++ ) { hist -> SetBinContent ( i + 1 , values [ i ][ index ]); hist -> SetBinError ( i + 1 , values [ i ][ index + 2 ]); //cout << i << \" -> (\" << hist->GetBinLowEdge(i+1) << \",\" << hist->GetBinLowEdge(i+1)+hist->GetBinWidth(i+1) << \") == \" << hist->GetBinContent(i+1) << \"\\n\"; } if ( draw ) { TCanvas * xperiment = new TCanvas ; xperiment -> cd (); hist -> Draw (); } return hist ; } } To plot the efficiency we used the src/get_efficiency.h function. Check out get_efficiency.h TEfficiency * get_efficiency ( TH1D * all , TH1D * pass , string quantity , string MuonId , string prefix_name = \"\" , bool shouldWrite = false ) { //Copy histograms to change axis titles later TH1D * pass_copy = ( TH1D * ) pass -> Clone (); TH1D * all_copy = ( TH1D * ) all -> Clone (); pass_copy -> GetYaxis () -> SetTitle ( \"Efficiency\" ); all_copy -> GetYaxis () -> SetTitle ( \"Efficiency\" ); TEfficiency * pEff = new TEfficiency (); pEff -> SetPassedHistogram ( * pass_copy , \"f\" ); pEff -> SetTotalHistogram ( * all_copy , \"f\" ); delete all_copy ; delete pass_copy ; //Set plot config if ( prefix_name != \"\" ) { pEff -> SetName ( string ( MuonId + \"_\" + quantity + \"_\" + prefix_name + \"_Efficiency\" ). c_str ()); pEff -> SetTitle ( string ( \"Efficiency for \" + MuonId + \" \" + quantity + \" (\" + prefix_name + \")\" ). c_str ()); } else { pEff -> SetName ( string ( MuonId + \"_\" + quantity + \"_Efficiency\" ). c_str ()); pEff -> SetTitle ( string ( \"Efficiency for \" + MuonId + \" \" + quantity ). c_str ()); } pEff -> SetLineColor ( kBlack ); pEff -> SetMarkerStyle ( 21 ); pEff -> SetMarkerSize ( 0.5 ); pEff -> SetMarkerColor ( kBlack ); if ( shouldWrite ) pEff -> Write (); TCanvas * oi = new TCanvas (); oi -> cd (); pEff -> Draw (); gPad -> Update (); auto graph = pEff -> GetPaintedGraph (); graph -> SetMinimum ( 0.8 ); graph -> SetMaximum ( 1.2 ); gPad -> Update (); return pEff ; } Note that we load part of these functions in the src area directly in header of the efficiency.cpp code. Now that you understand what the efficiency.cpp macro does, run your code without flashing the scream ( -l ), with a batch mode ( -b ) and with a quit-when-done switch ( -q ): root -l -b -q efficiency.cpp Note Some [#1] INFO messages will pop up on the terminal. They are expected. Do not worry about it. There is some [#0] WARNING: that should pop up too. They say about ignoring some events. They appear due cuts we are making directly on variables limits. It is ok, just ignore it. If everything went correctly you should have a Pt_tracekrMuon.root in the path below. cd results/efficiencies/efficiency/Jpsi_Run_2011/ We will need to run the code again with a few changes so go back to the main file. cd ../../../..","title":"The Probability Density Function used for modeling signal and background"},{"location":"analysis/selection/idefficiencystudy/tutorial/02-fitting/#efficiency-evaluation-between-real-data-and-monte-carlo-simulations","text":"Now we need to create the Monte Carlo (MC) data to compare with the real one, in order to do that we will need to change the efficiency.cpp code, you will have to include the #include \"src/dofits/DoFit_Jpsi_MC.h\" and comment the #include \"src/dofits/DoFit_Jpsi_Run.h\" line. At the end, your code should look like this: //Change if you need //#include \"src/dofits/DoFit_Jpsi_Run.h\" #include \"src/dofits/DoFit_Jpsi_MC.h\" #include \"src/create_folder.h\" #include \"src/get_efficiency.h\" #include \"src/make_TH1D.h\" //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.8, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 1.8, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0., 40.}; Run the efficiency.cpp again and you will have 2 new folders at results/efficiencies/efficiency/ one Jpsi_MC_2020 and other Jpsi_Run_2011 . If you want to see each individual efficiency, you can use the new TBrowser command to open the trackerMuon_Pt_efficiency.root file. Now, in order to make the comparison between real and MC data we will need to run a code in folder tests named compare_efficiency.cpp . To run this command from where you are, first load this file on ROOT with the command below: root -l -b .L tests/compare_efficiency.cpp Now you have the function compare_efficiency(...) we made loaded on your root. This function have four arguments. The first one should be the muon identification we are looking for such as trackerMuon , standaloneMuon and globalMuon . THe second one refers to the quatity that we are analysing. In our case we are looking for \"Pt\", but could be \"Eta\" and \"Phi\". The third and forth parameter are paths which points to .root files created previously in this tutorial. So now, in order to make the comparison between real and MC data on tracker muon pT, you should run this command to get your result: compare_efficiency ( \"trackerMuon\" , \"Pt\" , \"results/efficiencies/efficiency/Jpsi_Run_2011/Pt_trackerMuon.root\" , \"results/efficiencies/efficiency/Jpsi_MC_2020/Pt_trackerMuon.root\" ) ; Now you should have a new folder called Comparison Run2011 vs MC in your results/efficiencies/efficiency/ and inside it are the comparisons that you just made. If everything went well and you still have time to go, repeat this process for the two other variables, \u03b7 and \u03c6! Note If you are stucked on root enviroment, do not forget about typing this command to exit it: .q","title":"Efficiency evaluation between real data and Monte Carlo simulations"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/","text":"Sideband \u00b6 Signal extraction: sideband subtraction method \u00b6 The reconstruction efficiency is calculated using only signal muons . In order to measure the efficiency, we need a way to extract signal from the dataset. You've used the fitting method and now you'll meet the sideband subtraction method. This method consists in choosing sideband and signal regions in invariant mass distribution. The sideband regions (shaded in red in the figure) have background particles and the signal region (shared in green in the figure) has background and signal particles. Note The background corresponds to candidates that do not correspond to the decay of a genuine resonance; for example, the pair is formed by the tag muon associated to an uncorrelated track produced elsewhere in the collision; the corresponding invariant mass has thus a smooth continuous shape, that is extrapolated from the signal regions into the sideband region. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty: Applying those equations we get histograms like this: Solid blue line (Total) = particles in signal region; Dashed blue line (Background) = particles in sideband regions; Solid magenta line (signal) = signal histogram (background subtracted). You will see this histogram on this exercise. About this code More info about this code can be found in the reference guide . Preparing files \u00b6 First, from the root folder of our downloaded repository, we need to go sideband subtraction method tutorial: cd efficiency_tools/sideband_subtraction To copy the J/\u03c8 dataset of real data file to your machine (requires 3,3 GB), type: wget -O Run2011AMuOnia_mergeNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011AMuOnia_mergeNtuple.root\" Run this code to download the simulation dataset for J/\u03c8 (requires 492 MB): wget -O JPsiToMuMu_mergeMCNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=JPsiToMuMu_mergeMCNtuple.root\" Now, check if everything is ok: ls JPsiToMuMu_mergeMCNtuple.root main README.md Run2011AMuOnia_mergeNtuple.root Your sideband_subtraction folder should have these files: Preparing code for Data \u00b6 Note This tutorial will teach you to manage the files on the terminal, but you can use a graphical file explorer or any other way you are used to. We need to edit some settings. Open settings.cpp : cd main/config ls createHistogram.h cuts.h settings.cpp There are different ways to open this file. You can try to run: gedit settings.cpp Or, if you can not use gedit, try nano: nano settings.cpp I do not have nano! You can try to use any text editor , but here is some commands you cant try to use to install it: Ubuntu/Debian: sudo apt-get -y install nano . RedHat/CentOS/Fedora: sudo yum install nano . Mac OS X: nano is installed by default . We want to calculate efficiencies of tracker muons . With the settings.cpp file opened, make sure to let the variables like this: //Canvas drawing bool shouldDrawInvariantMassCanvas = true ; bool shouldDrawInvariantMassCanvasRegion = true ; bool shouldDrawQuantitiesCanvas = true ; bool shouldDrawEfficiencyCanvas = true ; //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; We want to calculate the efficiency using specific files that we downloaded. They name are Run2011AMuOnia_mergeNtuple.root and JPsiToMuMu_mergeMCNtuple.root and are listed in const char *files[] . While settings.cpp is open, try to use the variable int useFile to run Run2011AMuOnia_mergeNtuple.root . How to do this Make sure useFile is correct: //List of files const char * files [] = { \"../data_histoall.root\" , \"../Run2011AMuOnia_mergeNtuple.root\" , \"\" \"../JPsiToMuMu_mergeMCNtuple.root\" , \"../Run2011A_MuOnia_Upsilon.root\" , \"../Upsilon1SToMuMu_MC_full.root\" }; const char * directoriesToSave [] = { \"../results/result/\" , \"../results/Jpsi Run 2011/\" , \"../results/Jpsi MC 2020/\" , \"../results/Upsilon Run 2011/\" , \"../results/Upsilon MC 2020/\" }; //MAIN OPTIONS //Which file of files (variable above) should use int useFile = 1 ; It will tell which configuration the program will use. So, the macro will run with the ntuple in files[useFile] and the results will be stored in directoriesToSave[useFile] . the first three files won't be used in this execise. About code Normally we need to set the variable const char* resonance , but at this time it is already done and set automatically for these ntuples' names. Editting bins \u00b6 The code allows to define the binning of the kinematic variable, to ensure each bin is sufficiently populated, for increased robustness. To change the binning, open createHistogram.h that is on same folder that settings.cpp : gedit createHistogram.h Search for the createEfficiencyPlot(...) function. You'll find something like this: void createHistogram ( TH1D * & histo , const char * histoName ) {...} For each quantity (pT, eta, phi) we used different bins. To change the bins, look inside the createEfficiencyPlot(...) function. In a simpler version, you'll see a structure like this: //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { //Here creates histogram for pT } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { //Here creates histogram for eta } //Bins for phi else { //Here creates histogram for phi } See the whole scructure Don't be scared! Code does'nt bite. //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Edit histogram axis histo -> GetYaxis () -> SetTitle ( Form ( yAxisTitleForm . data (), histo -> GetBinWidth ( 0 ))); histo -> GetXaxis () -> SetTitle ( xAxisTitle . data ()); The code that creates the histogram bins is located inside the conditionals and is commented. You can edit this code and uncomment to create histogram bins however you want. Instead of using a function to generate the bins, we can also define them manually. As we intend to compare the results between data and simulation, but also between the sideband and fitting methods. You are advised to employ the same bin choice. Garantee your the code uses same bin as the previous here: //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } Running the code \u00b6 After setting the configurations, it's time to run the code. Go back to the main directory and make sure macro.cpp is there. cd .. ls classes compare_efficiency.cpp config macro.cpp Run the macro.cpp: root -l -b -q macro.cpp \"../results/Jpsi_Run_2011/\" directory created OK Using \"../Run2011AMuOnia_mergeNtuple.root\" ntupple resonance: Jpsi Using subtraction factor as integral of background fit Data analysed = 5950253 of 5950253 Note As this dataset is larger, the code will run slowly. It can take several minutes to be completed depending where the code is been running. In this process, more informations will be printed in terminal while plots will be created on a specified folder. The message below tells you that code has finished running: Done. All result files can be found at \"../results/Jpsi_Run_2011/\" Common errors If you run the code and your terminal printed some erros like: Error in <ROOT::Math::Cephes::incbi : Wrong domain for parameter b (must be 0) This occurs when the contents of a bin of the pass histogram is greater than the corresponding bin in the total histogram. With sideband subtraction, depending on bins you choose, this can happen and will result in enormous error bars. This issue may be avoided by fine-tuning the binning choice. For now, these messages may be ignored. Probe Efficiency results for Data \u00b6 If all went well, your results are going to be like these: Preparing and running the code for simulation \u00b6 Challenge Try to run the same code on the JPsiToMuMu_mergeMCNtuple.root file we downloaded. Tip You will need the redo the steps above, but setting: int useFile = 2 ; in main/config/settings.cpp file. Comparison between real data and simulation We'll do this in the last section of this exercise. So the challenge above is mandatory. Extra challenge If you are looking for an extra exercise, you can try to apply the same logic, changing some variables you saw, in order to get results for the \u03a5 nutpple. To download the \u03a5 real data ntupple (requires 442 MB): wget -O Run2011A_MuOnia_Upsilon.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011A_MuOnia_Upsilon.root\" Run this code to download the simulation dataset for \u03a5 (requires 67 MB): wget -O Upsilon1SToMuMu_MC_full.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Upsilon1SToMuMu_MC_full.root\"","title":"Sideband Subtraction"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#sideband","text":"","title":"Sideband"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#signal-extraction-sideband-subtraction-method","text":"The reconstruction efficiency is calculated using only signal muons . In order to measure the efficiency, we need a way to extract signal from the dataset. You've used the fitting method and now you'll meet the sideband subtraction method. This method consists in choosing sideband and signal regions in invariant mass distribution. The sideband regions (shaded in red in the figure) have background particles and the signal region (shared in green in the figure) has background and signal particles. Note The background corresponds to candidates that do not correspond to the decay of a genuine resonance; for example, the pair is formed by the tag muon associated to an uncorrelated track produced elsewhere in the collision; the corresponding invariant mass has thus a smooth continuous shape, that is extrapolated from the signal regions into the sideband region. For each event category (i.e. Pass and All), and for a given variable of interest (e.g., the probe pT), two distributions are obtained, one for each region (Signal and Sideband). In order to obtain the variable distribution for the signal only, we proceed by subtracting the Background distribution (Sideband region) from the Signal+Background one (Signal region): Where the normalization \u03b1 factor quantifies the quantity of background present in the signal region: And for the uncertainty: Applying those equations we get histograms like this: Solid blue line (Total) = particles in signal region; Dashed blue line (Background) = particles in sideband regions; Solid magenta line (signal) = signal histogram (background subtracted). You will see this histogram on this exercise. About this code More info about this code can be found in the reference guide .","title":"Signal extraction: sideband subtraction method"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#preparing-files","text":"First, from the root folder of our downloaded repository, we need to go sideband subtraction method tutorial: cd efficiency_tools/sideband_subtraction To copy the J/\u03c8 dataset of real data file to your machine (requires 3,3 GB), type: wget -O Run2011AMuOnia_mergeNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011AMuOnia_mergeNtuple.root\" Run this code to download the simulation dataset for J/\u03c8 (requires 492 MB): wget -O JPsiToMuMu_mergeMCNtuple.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=JPsiToMuMu_mergeMCNtuple.root\" Now, check if everything is ok: ls JPsiToMuMu_mergeMCNtuple.root main README.md Run2011AMuOnia_mergeNtuple.root Your sideband_subtraction folder should have these files:","title":"Preparing files"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#preparing-code-for-data","text":"Note This tutorial will teach you to manage the files on the terminal, but you can use a graphical file explorer or any other way you are used to. We need to edit some settings. Open settings.cpp : cd main/config ls createHistogram.h cuts.h settings.cpp There are different ways to open this file. You can try to run: gedit settings.cpp Or, if you can not use gedit, try nano: nano settings.cpp I do not have nano! You can try to use any text editor , but here is some commands you cant try to use to install it: Ubuntu/Debian: sudo apt-get -y install nano . RedHat/CentOS/Fedora: sudo yum install nano . Mac OS X: nano is installed by default . We want to calculate efficiencies of tracker muons . With the settings.cpp file opened, make sure to let the variables like this: //Canvas drawing bool shouldDrawInvariantMassCanvas = true ; bool shouldDrawInvariantMassCanvasRegion = true ; bool shouldDrawQuantitiesCanvas = true ; bool shouldDrawEfficiencyCanvas = true ; //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; We want to calculate the efficiency using specific files that we downloaded. They name are Run2011AMuOnia_mergeNtuple.root and JPsiToMuMu_mergeMCNtuple.root and are listed in const char *files[] . While settings.cpp is open, try to use the variable int useFile to run Run2011AMuOnia_mergeNtuple.root . How to do this Make sure useFile is correct: //List of files const char * files [] = { \"../data_histoall.root\" , \"../Run2011AMuOnia_mergeNtuple.root\" , \"\" \"../JPsiToMuMu_mergeMCNtuple.root\" , \"../Run2011A_MuOnia_Upsilon.root\" , \"../Upsilon1SToMuMu_MC_full.root\" }; const char * directoriesToSave [] = { \"../results/result/\" , \"../results/Jpsi Run 2011/\" , \"../results/Jpsi MC 2020/\" , \"../results/Upsilon Run 2011/\" , \"../results/Upsilon MC 2020/\" }; //MAIN OPTIONS //Which file of files (variable above) should use int useFile = 1 ; It will tell which configuration the program will use. So, the macro will run with the ntuple in files[useFile] and the results will be stored in directoriesToSave[useFile] . the first three files won't be used in this execise. About code Normally we need to set the variable const char* resonance , but at this time it is already done and set automatically for these ntuples' names.","title":"Preparing code for Data"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#editting-bins","text":"The code allows to define the binning of the kinematic variable, to ensure each bin is sufficiently populated, for increased robustness. To change the binning, open createHistogram.h that is on same folder that settings.cpp : gedit createHistogram.h Search for the createEfficiencyPlot(...) function. You'll find something like this: void createHistogram ( TH1D * & histo , const char * histoName ) {...} For each quantity (pT, eta, phi) we used different bins. To change the bins, look inside the createEfficiencyPlot(...) function. In a simpler version, you'll see a structure like this: //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { //Here creates histogram for pT } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { //Here creates histogram for eta } //Bins for phi else { //Here creates histogram for phi } See the whole scructure Don't be scared! Code does'nt bite. //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Edit histogram axis histo -> GetYaxis () -> SetTitle ( Form ( yAxisTitleForm . data (), histo -> GetBinWidth ( 0 ))); histo -> GetXaxis () -> SetTitle ( xAxisTitle . data ()); The code that creates the histogram bins is located inside the conditionals and is commented. You can edit this code and uncomment to create histogram bins however you want. Instead of using a function to generate the bins, we can also define them manually. As we intend to compare the results between data and simulation, but also between the sideband and fitting methods. You are advised to employ the same bin choice. Garantee your the code uses same bin as the previous here: //Variable bin for pT if ( strcmp ( quantityName , \"Pt\" ) == 0 ) { double xbins [] = { 0. , 2.0 , 3.4 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Variable bin for eta else if ( strcmp ( quantityName , \"Eta\" ) == 0 ) { double xbins [] = { -2.4 , -1.8 , -1.4 , -1.2 , -1.0 , -0.8 , -0.5 , -0.2 , 0 , 0.2 , 0.5 , 0.8 , 1.0 , 1.2 , 1.4 , 1.8 , 2.4 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); } //Bins for phi else { double xbins [] = { -3.0 , -1.8 , -1.6 , -1.2 , -1.0 , -0.7 , -0.4 , -0.2 , 0 , 0.2 , 0.4 , 0.7 , 1.0 , 1.2 , 1.6 , 1.8 , 3.0 }; int nbins = sizeof ( xbins ) / sizeof ( * xbins ) - 1 ; histo = new TH1D ( hName . data (), hTitle . data (), nbins , xbins ); }","title":"Editting bins"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#running-the-code","text":"After setting the configurations, it's time to run the code. Go back to the main directory and make sure macro.cpp is there. cd .. ls classes compare_efficiency.cpp config macro.cpp Run the macro.cpp: root -l -b -q macro.cpp \"../results/Jpsi_Run_2011/\" directory created OK Using \"../Run2011AMuOnia_mergeNtuple.root\" ntupple resonance: Jpsi Using subtraction factor as integral of background fit Data analysed = 5950253 of 5950253 Note As this dataset is larger, the code will run slowly. It can take several minutes to be completed depending where the code is been running. In this process, more informations will be printed in terminal while plots will be created on a specified folder. The message below tells you that code has finished running: Done. All result files can be found at \"../results/Jpsi_Run_2011/\" Common errors If you run the code and your terminal printed some erros like: Error in <ROOT::Math::Cephes::incbi : Wrong domain for parameter b (must be 0) This occurs when the contents of a bin of the pass histogram is greater than the corresponding bin in the total histogram. With sideband subtraction, depending on bins you choose, this can happen and will result in enormous error bars. This issue may be avoided by fine-tuning the binning choice. For now, these messages may be ignored.","title":"Running the code"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#probe-efficiency-results-for-data","text":"If all went well, your results are going to be like these:","title":"Probe Efficiency results for Data"},{"location":"analysis/selection/idefficiencystudy/tutorial/03-sidebandsubtraction/#preparing-and-running-the-code-for-simulation","text":"Challenge Try to run the same code on the JPsiToMuMu_mergeMCNtuple.root file we downloaded. Tip You will need the redo the steps above, but setting: int useFile = 2 ; in main/config/settings.cpp file. Comparison between real data and simulation We'll do this in the last section of this exercise. So the challenge above is mandatory. Extra challenge If you are looking for an extra exercise, you can try to apply the same logic, changing some variables you saw, in order to get results for the \u03a5 nutpple. To download the \u03a5 real data ntupple (requires 442 MB): wget -O Run2011A_MuOnia_Upsilon.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Run2011A_MuOnia_Upsilon.root\" Run this code to download the simulation dataset for \u03a5 (requires 67 MB): wget -O Upsilon1SToMuMu_MC_full.root \"https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq/download?files=Upsilon1SToMuMu_MC_full.root\"","title":"Preparing and running the code for simulation"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/","text":"How to make a comparison between two previous methods of signal extraction \u00b6 How sideband subtraction method code stores its files \u00b6 the Sideband subtraction code saves every efficiency plot in efficiency/plots/ folder inside a single generated_hist.root file. Lets check it! You're probably on the main directory. Lets go back one directory. cd .. ls JPsiToMuMu_mergeMCNtuple.root README.md Run2011AMuOnia_mergeNtuple.root main results A folder named results showed up on this folder. Lets go check its content. cd results ls Jpsi_MC_2020 Jpsi_Run_2011 If you did every step of the sideband subtraction on this page lesson, these results should match with the results on your pc. Access one of those folders (except comparison). cd Jpsi_Run_2011 ls Efficiency_Tracker_Probe_Eta.png Tracker_Probe_Eta_All.png Efficiency_Tracker_Probe_Phi.png Tracker_Probe_Eta_Passing.png Efficiency_Tracker_Probe_Pt.png Tracker_Probe_Phi_All.png generated_hist.root Tracker_Probe_Phi_Passing.png InvariantMass_Tracker.png Tracker_Probe_Pt_All.png InvariantMass_Tracker_region.png Tracker_Probe_Pt_Passing.png Here, all the output plots you saw when running the sideband subtraction method are stored as a .png . Aside from them, there's a generated_hist.root that stores the efficiency in a way that we can manipulate it after. This file is needed to run the comparison between efficiencies for the sideband subtraction method. Lets look inside of this file. Run this command to open generated_hist.root with ROOT: root -l generated_hist.root root [0] Attaching file generated_hist.root as _file0... (TFile *) 0x55dca0f04c50 root [1] Lets check its content. Type on terminal: new TBrowser You should see something like this: This is a visual navigator of a .root file. Here you can see the struture of generated_hist.root . Double click the folders to open them and see their content. The Efficiency plots we see are stored in efficiency/plots/ folder: You can double click each plot to see its content: Tip To close this window, click on terminal and press Ctrl + C . This command stops any processes happening in the terminal. Key Point As you see, the .root file has paths inside of it and the efficiencies plots have a self path inside them as well! Comparison results between real data and simulations for sideband method \u00b6 After runinng the sideband subtraction code, we get a .root with all the efficiencies plots inside it in two different folders: ../results/Jpsi_Run_2011/generated_hist.root ../results/Jpsi_MC_2020/generated_hist.root We'll get back to this on the discussion below. Head back to the main folder. Inside of it there is a code for the efficiency plot comparison. Lets check it out. From the sideband_subtraction folder, type: cd main ls classes compare_efficiency.cpp config macro.cpp There is it. Now lets open it. gedit compare_efficiency.cpp Its easy to prepare it for the sideband subtraction comparison. Our main editing point can be found in this part: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = true ; bool doGlobal = true ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; Note In the scope above we see: int useScheme represents which comparison you are doing. bool doTracker is a variable that allow plots for tracker muons. bool doStandalone is a variable that allow plots for standalone muons. bool doGlobal is a variable that allow plots for global muons. bool doPt is a variable that allow plots for muon pT. bool doEta is a variable that allow plots for muon \u03b7. bool doPhi is a variable that allow plots for muon \u03c6. Everything is up to date to compare sideband subtraction's results between real data and simulations, except it is comparing standalone and global muons. As we are looking for tracker muons efficiencies only, you should switch to false variables for Standalone and Global . Also, you will need to change the useScheme variable to plot what you want to plot. As we want to plot efficiency of real data and simulated data , the value has to be 0. See result scructure If you deleted the right lines, your code now should be like this: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; Let your variables like this. Now you need to run the code. To do this, save the file and type on your terminal: root -l compare_efficiency.cpp If everything went well, the message you'll see in terminal at end of the process is: Use Scheme: 0 Done. All result files can be found at \"../results/Comparison_Upsilon_Sideband_Run_vs_MC/\" Note The command above to run the code will display three new windows on your screen with comparison plots. You can avoid them by running straight the command below . root - l - b - q compare_efficiency . cpp In this case, to check it results you are going to need go for result folder (printed on code run) and check images there by yourself. You can try to run new TBrowser again: cd [ FOLDER_PATH ] root - l new TBrowser And as output plots comparsion, you get: Now you can type the command below to quit root and close all created windows: .q How fitting method code stores its files \u00b6 To do the next part, first you need to understand how the fitting method code saves. It is not so different than sideband subtraction method. Lets look at how they are saved. If you look inside fitting\\results' folder, where is stored fitting method results, you will see another folder named efficiencies . lets go there by terminal from fitting folder: cd results/efficiencies ls efficiency Inside of it there is a unique folder named efficiency. It is necessary because later on, we will work in efficiencies in 2 dimentions. The efficiency folder means it is one-dimensional, which we worked here so on. cd efficiency ls Comparison_Run2011_vs_MC Jpsi_MC_2020 Jpsi_Run_2011 Let's go inside one of them: cd Jpsi_Run_2011 ls Pt_trackerMuon.root For every configuration for a specific dataset, they will create .root files inside its respectively folder. For example, this one folder that we choose will have all calculations for the J/\u03c8 real data dataset. If you go with your terminal to this folder and run this command, you'll see that the result files only have one plot on main folder. root -l Pt_trackerMuon.root root [0] Attaching file Pt_trackerMuon.root as _file0... (TFile *) 0x55efb5f44930 root [1] Now lets look at its content. Type on terminal: new TBrowser It has only one plot, because the others are in different files. Inside the folder histograms , you can find the histograms that created this efficiency result. Key Point There is a .root file for each efficiency plot created with the fitting method. Comparison results between real data and simulations for fitting method \u00b6 Go back to the main folder on sideband_subtraction folder. cd main ls classes compare_efficiency.cpp config macro.cpp Open compare_efficiency.cpp again gedit compare_efficiency.cpp This is how your code should look like now: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; You have to do just two things: edit int useScheme value to current analysis. Put others quantities expect pT to false, we did not obtained \u03b7 nor \u03c6 results on previous page. After making those edits Your code should look like this: //CONFIGS int useScheme = 1 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = false ; bool doPhi = false ; Doing this and running the program with: root -l compare_efficiency.cpp Should get you these results: Challenge As you notice here we presented comparison for \u03b7 and \u03c6. Try to go back to fitting method tutorial and redo commands to get efficiencies for these quantities in order to compare with sideband subctraction method. Do not forget to turn bool doEta and bool doPhi true. Now you can type the command below to quit root and close all created windows: .q Comparison results between data from the sideband and data from the fitting method \u00b6 Challenge Using what you did before, try to mix them and plot a comparison between real data for sideband method and real data for sthe fitting method and get an analysis. Notice that: Real data = Run 2011 Simulations = Monte Carlo = MC Tip: you just need to change what you saw in this page to do this comparison. Extra challenge As you did with the last 2 extras challenges, try to redo this exercise comparing results between \u03a5 datas.","title":"Comparison"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#how-to-make-a-comparison-between-two-previous-methods-of-signal-extraction","text":"","title":"How to make a comparison between two previous methods of signal extraction"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#how-sideband-subtraction-method-code-stores-its-files","text":"the Sideband subtraction code saves every efficiency plot in efficiency/plots/ folder inside a single generated_hist.root file. Lets check it! You're probably on the main directory. Lets go back one directory. cd .. ls JPsiToMuMu_mergeMCNtuple.root README.md Run2011AMuOnia_mergeNtuple.root main results A folder named results showed up on this folder. Lets go check its content. cd results ls Jpsi_MC_2020 Jpsi_Run_2011 If you did every step of the sideband subtraction on this page lesson, these results should match with the results on your pc. Access one of those folders (except comparison). cd Jpsi_Run_2011 ls Efficiency_Tracker_Probe_Eta.png Tracker_Probe_Eta_All.png Efficiency_Tracker_Probe_Phi.png Tracker_Probe_Eta_Passing.png Efficiency_Tracker_Probe_Pt.png Tracker_Probe_Phi_All.png generated_hist.root Tracker_Probe_Phi_Passing.png InvariantMass_Tracker.png Tracker_Probe_Pt_All.png InvariantMass_Tracker_region.png Tracker_Probe_Pt_Passing.png Here, all the output plots you saw when running the sideband subtraction method are stored as a .png . Aside from them, there's a generated_hist.root that stores the efficiency in a way that we can manipulate it after. This file is needed to run the comparison between efficiencies for the sideband subtraction method. Lets look inside of this file. Run this command to open generated_hist.root with ROOT: root -l generated_hist.root root [0] Attaching file generated_hist.root as _file0... (TFile *) 0x55dca0f04c50 root [1] Lets check its content. Type on terminal: new TBrowser You should see something like this: This is a visual navigator of a .root file. Here you can see the struture of generated_hist.root . Double click the folders to open them and see their content. The Efficiency plots we see are stored in efficiency/plots/ folder: You can double click each plot to see its content: Tip To close this window, click on terminal and press Ctrl + C . This command stops any processes happening in the terminal. Key Point As you see, the .root file has paths inside of it and the efficiencies plots have a self path inside them as well!","title":"How sideband subtraction method code stores its files"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-real-data-and-simulations-for-sideband-method","text":"After runinng the sideband subtraction code, we get a .root with all the efficiencies plots inside it in two different folders: ../results/Jpsi_Run_2011/generated_hist.root ../results/Jpsi_MC_2020/generated_hist.root We'll get back to this on the discussion below. Head back to the main folder. Inside of it there is a code for the efficiency plot comparison. Lets check it out. From the sideband_subtraction folder, type: cd main ls classes compare_efficiency.cpp config macro.cpp There is it. Now lets open it. gedit compare_efficiency.cpp Its easy to prepare it for the sideband subtraction comparison. Our main editing point can be found in this part: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = true ; bool doGlobal = true ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; Note In the scope above we see: int useScheme represents which comparison you are doing. bool doTracker is a variable that allow plots for tracker muons. bool doStandalone is a variable that allow plots for standalone muons. bool doGlobal is a variable that allow plots for global muons. bool doPt is a variable that allow plots for muon pT. bool doEta is a variable that allow plots for muon \u03b7. bool doPhi is a variable that allow plots for muon \u03c6. Everything is up to date to compare sideband subtraction's results between real data and simulations, except it is comparing standalone and global muons. As we are looking for tracker muons efficiencies only, you should switch to false variables for Standalone and Global . Also, you will need to change the useScheme variable to plot what you want to plot. As we want to plot efficiency of real data and simulated data , the value has to be 0. See result scructure If you deleted the right lines, your code now should be like this: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; Let your variables like this. Now you need to run the code. To do this, save the file and type on your terminal: root -l compare_efficiency.cpp If everything went well, the message you'll see in terminal at end of the process is: Use Scheme: 0 Done. All result files can be found at \"../results/Comparison_Upsilon_Sideband_Run_vs_MC/\" Note The command above to run the code will display three new windows on your screen with comparison plots. You can avoid them by running straight the command below . root - l - b - q compare_efficiency . cpp In this case, to check it results you are going to need go for result folder (printed on code run) and check images there by yourself. You can try to run new TBrowser again: cd [ FOLDER_PATH ] root - l new TBrowser And as output plots comparsion, you get: Now you can type the command below to quit root and close all created windows: .q","title":"Comparison results between real data and simulations for sideband method"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#how-fitting-method-code-stores-its-files","text":"To do the next part, first you need to understand how the fitting method code saves. It is not so different than sideband subtraction method. Lets look at how they are saved. If you look inside fitting\\results' folder, where is stored fitting method results, you will see another folder named efficiencies . lets go there by terminal from fitting folder: cd results/efficiencies ls efficiency Inside of it there is a unique folder named efficiency. It is necessary because later on, we will work in efficiencies in 2 dimentions. The efficiency folder means it is one-dimensional, which we worked here so on. cd efficiency ls Comparison_Run2011_vs_MC Jpsi_MC_2020 Jpsi_Run_2011 Let's go inside one of them: cd Jpsi_Run_2011 ls Pt_trackerMuon.root For every configuration for a specific dataset, they will create .root files inside its respectively folder. For example, this one folder that we choose will have all calculations for the J/\u03c8 real data dataset. If you go with your terminal to this folder and run this command, you'll see that the result files only have one plot on main folder. root -l Pt_trackerMuon.root root [0] Attaching file Pt_trackerMuon.root as _file0... (TFile *) 0x55efb5f44930 root [1] Now lets look at its content. Type on terminal: new TBrowser It has only one plot, because the others are in different files. Inside the folder histograms , you can find the histograms that created this efficiency result. Key Point There is a .root file for each efficiency plot created with the fitting method.","title":"How fitting method code stores its files"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-real-data-and-simulations-for-fitting-method","text":"Go back to the main folder on sideband_subtraction folder. cd main ls classes compare_efficiency.cpp config macro.cpp Open compare_efficiency.cpp again gedit compare_efficiency.cpp This is how your code should look like now: //CONFIGS int useScheme = 0 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = true ; bool doPhi = true ; You have to do just two things: edit int useScheme value to current analysis. Put others quantities expect pT to false, we did not obtained \u03b7 nor \u03c6 results on previous page. After making those edits Your code should look like this: //CONFIGS int useScheme = 1 ; //Jpsi Sideband Run vs Jpsi Sideband MC //Jpsi Fitting Run vs Jpsi Fitting MC //Jpsi Sideband Run vs Jpsi Fitting Run //Upsilon Sideband Run vs Upsilon Sideband MC //Upsilon Fitting Run vs Upsilon Fitting MC //Upsilon Sideband Run vs Upsilon Fitting Run //Muon id analyse bool doTracker = true ; bool doStandalone = false ; bool doGlobal = false ; //quantity analyse bool doPt = true ; bool doEta = false ; bool doPhi = false ; Doing this and running the program with: root -l compare_efficiency.cpp Should get you these results: Challenge As you notice here we presented comparison for \u03b7 and \u03c6. Try to go back to fitting method tutorial and redo commands to get efficiencies for these quantities in order to compare with sideband subctraction method. Do not forget to turn bool doEta and bool doPhi true. Now you can type the command below to quit root and close all created windows: .q","title":"Comparison results between real data and simulations for fitting method"},{"location":"analysis/selection/idefficiencystudy/tutorial/04-comparison/#comparison-results-between-data-from-the-sideband-and-data-from-the-fitting-method","text":"Challenge Using what you did before, try to mix them and plot a comparison between real data for sideband method and real data for sthe fitting method and get an analysis. Notice that: Real data = Run 2011 Simulations = Monte Carlo = MC Tip: you just need to change what you saw in this page to do this comparison. Extra challenge As you did with the last 2 extras challenges, try to redo this exercise comparing results between \u03a5 datas.","title":"Comparison results between data from the sideband and data from the fitting method"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/","text":"Procedures and strategy for estimating the systematics uncertainties \u00b6 Setting it up \u00b6 If you have done the fitting tutorial you already have done this part so you may go directly to \"Generating uncertainties\" Clone this repository and go to the fitting folder. git clone git://github.com/allanjales/TagAndProbe cd TagAndProbe/efficiency_tools/fitting You will also need to download the Run2011AMuOnia_mergeNtuple.root file using this link. https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq Data simplifying \u00b6 In order to run this code, use this command to simplify the data so that it can be read by the RooFit root library. root simplify_data.cpp This will create a \"TagAndProbe_Jpsi_Run2011.root\" file, this process may take a few minutes. move the file to the DATA folder. Estimations of systematics uncertainty sources \u00b6 To estimate the systematic error we will need first to get some uncertainties from the DATA. So, to do that, run the following code. root -l -b -q plot_sys_efficiency.cpp By default, this code will estimate the Muon ID efficiency for the Global Muon ID for |\u03b7| distribution, this can be changed by opening the \"plot_sys_efficiency.cpp\" and commenting and uncommenting the Muon ID and quantity of your desire. This process may take several minutes to complete. The systematics uncertainties will be evaluate by making small changes in the fit on the invariant mass distribution of the resonance. For example, the \u03c8 decaying in dimuons, in this case the changes were: 2x Gaussians (\"2x gaus\" as in the code) which means fitting with two gaussians. The other sources are the upper and under limits of invariant mass distribution and so \"Mass Up\" which means making the mass window bigger, \"Mass Down\" which means making the mass window smaller. Last source you can modify the bin size of the same distribution. \"Bin up\" means making the fit with more bins and \"Bin down\" means making the fit with less bins. In order to do the next step you will have to run the \"plot_sys_efficiency.cpp\" for the Pt of both global and tracker Muon. To get the Pt for the traker Muons the code should look like this. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 3.0 , 3.6 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0.0, 2.0, 3.4, 4.0, 5.0, 6.0, 8.0, 10.0, 40.}; //string quantity = \"Eta\"; double bins[] = {0.0, 0.4, 0.6, 0.95, 1.2, 1.4, 1.6, 1.8, 2.4}; and like this to the global Muons. //Which Muon Id do you want to study? //string MuonId = \"trackerMuon\"; //string MuonId = \"standaloneMuon\"; string MuonId = \"globalMuon\" ; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 3.0 , 3.6 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0.0, 2.0, 3.4, 4.0, 5.0, 6.0, 8.0, 10.0, 40.}; //string quantity = \"Eta\"; double bins[] = {0.0, 0.4, 0.6, 0.95, 1.2, 1.4, 1.6, 1.8, 2.4}; Systematic efficiency overplot \u00b6 To better understand the results of the last part, this code will put all the different plots created previously in an image. root overplot_efficiencies . cpp You should get a result like this: 2D Efficiency Map \u00b6 This code generates a 2D systematic efficiency overplot, it outputs a .root that contains the efficiency histograms that can be visualised by the root TBrowser. root -l -b -q plot_sys_efficiency_2d.cpp This is one of the graphs that will be generated. It is noteworthy that the uncertainties presented above in the 2d map are already the quadrature sum of systematics and statistical uncertainties.","title":"Systematic"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#procedures-and-strategy-for-estimating-the-systematics-uncertainties","text":"","title":"Procedures and strategy for estimating the systematics uncertainties"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#setting-it-up","text":"If you have done the fitting tutorial you already have done this part so you may go directly to \"Generating uncertainties\" Clone this repository and go to the fitting folder. git clone git://github.com/allanjales/TagAndProbe cd TagAndProbe/efficiency_tools/fitting You will also need to download the Run2011AMuOnia_mergeNtuple.root file using this link. https://cernbox.cern.ch/index.php/s/lqHEasYWJpOZsfq","title":"Setting it up"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#data-simplifying","text":"In order to run this code, use this command to simplify the data so that it can be read by the RooFit root library. root simplify_data.cpp This will create a \"TagAndProbe_Jpsi_Run2011.root\" file, this process may take a few minutes. move the file to the DATA folder.","title":"Data simplifying"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#estimations-of-systematics-uncertainty-sources","text":"To estimate the systematic error we will need first to get some uncertainties from the DATA. So, to do that, run the following code. root -l -b -q plot_sys_efficiency.cpp By default, this code will estimate the Muon ID efficiency for the Global Muon ID for |\u03b7| distribution, this can be changed by opening the \"plot_sys_efficiency.cpp\" and commenting and uncommenting the Muon ID and quantity of your desire. This process may take several minutes to complete. The systematics uncertainties will be evaluate by making small changes in the fit on the invariant mass distribution of the resonance. For example, the \u03c8 decaying in dimuons, in this case the changes were: 2x Gaussians (\"2x gaus\" as in the code) which means fitting with two gaussians. The other sources are the upper and under limits of invariant mass distribution and so \"Mass Up\" which means making the mass window bigger, \"Mass Down\" which means making the mass window smaller. Last source you can modify the bin size of the same distribution. \"Bin up\" means making the fit with more bins and \"Bin down\" means making the fit with less bins. In order to do the next step you will have to run the \"plot_sys_efficiency.cpp\" for the Pt of both global and tracker Muon. To get the Pt for the traker Muons the code should look like this. //Which Muon Id do you want to study? string MuonId = \"trackerMuon\" ; //string MuonId = \"standaloneMuon\"; //string MuonId = \"globalMuon\"; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 3.0 , 3.6 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0.0, 2.0, 3.4, 4.0, 5.0, 6.0, 8.0, 10.0, 40.}; //string quantity = \"Eta\"; double bins[] = {0.0, 0.4, 0.6, 0.95, 1.2, 1.4, 1.6, 1.8, 2.4}; and like this to the global Muons. //Which Muon Id do you want to study? //string MuonId = \"trackerMuon\"; //string MuonId = \"standaloneMuon\"; string MuonId = \"globalMuon\" ; //Which quantity do you want to use? string quantity = \"Pt\" ; double bins [] = { 0. , 3.0 , 3.6 , 4.0 , 4.4 , 4.7 , 5.0 , 5.6 , 5.8 , 6.0 , 6.2 , 6.4 , 6.6 , 6.8 , 7.3 , 9.5 , 13.0 , 17.0 , 40. }; //string quantity = \"Eta\"; double bins[] = {-2.4, -1.4, -1.2, -1.0, -0.8, -0.5, -0.2, 0, 0.2, 0.5, 0.8, 1.0, 1.2, 1.4, 2.4}; //string quantity = \"Phi\"; double bins[] = {-3.0, -1.8, -1.6, -1.2, -1.0, -0.7, -0.4, -0.2, 0, 0.2, 0.4, 0.7, 1.0, 1.2, 1.6, 1.8, 3.0}; //string quantity = \"Pt\"; double bins[] = {0.0, 2.0, 3.4, 4.0, 5.0, 6.0, 8.0, 10.0, 40.}; //string quantity = \"Eta\"; double bins[] = {0.0, 0.4, 0.6, 0.95, 1.2, 1.4, 1.6, 1.8, 2.4};","title":"Estimations of systematics uncertainty sources"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#systematic-efficiency-overplot","text":"To better understand the results of the last part, this code will put all the different plots created previously in an image. root overplot_efficiencies . cpp You should get a result like this:","title":"Systematic efficiency overplot"},{"location":"analysis/selection/idefficiencystudy/tutorial/05-systematic/#2d-efficiency-map","text":"This code generates a 2D systematic efficiency overplot, it outputs a .root that contains the efficiency histograms that can be visualised by the root TBrowser. root -l -b -q plot_sys_efficiency_2d.cpp This is one of the graphs that will be generated. It is noteworthy that the uncertainties presented above in the 2d map are already the quadrature sum of systematics and statistical uncertainties.","title":"2D Efficiency Map"},{"location":"analysis/selection/objects/egamma/","text":"Objects \u00b6 Warning This page is under construction This twiki contains information on electron selection intended to be used with 2010 data. The selection is based on cuts on a small number of variables. Different thresholds are used for electrons found in the ECAL barrel and the ECAL endcap. Electron selection variables may be categorized in 3 groups: e-ID variables (shower shape, track cluster matching etc) isolation variables conversion rejection variables The sets of cuts given here are obtained by tuning all cuts together, but the sets of cuts on each of the 3 groups of variables may be used alone quite effectively. The fake rate, and fake sources, vary with ET, and for any set of cuts, rejection power and efficiency vary with ET. However, sets of cuts optimized for ET>25 GeV are near optimal for the interval 100>ET>20 GeV, and can usefully be employed down to 15 GeV. Ultimately the most performant selection should be obtained using multi-variate techniques, likelihood fits etc. Prior to that cut-based selections can provide a useful tool to understand the data and make comparison with MC. The advantages of \"Simple Cuts\" are: Cut inversion (used in many data driven signal extraction and background subtraction methodologies) is simple Smallest statistics are needed for full understanding and efficiency measurement It is simple to cleanly separate the e-ID, isolation and conversion rejection pieces The selection has been tuned in order to get a set of cuts with maximum background rejection for a given efficiency. ELECTRONS Handle < GsfElectronCollection > electrons ; iEvent . getByLabel ( InputTag ( \"gsfElectrons\" ), electrons ); edm :: Handle < reco :: ConversionCollection > hConversions ; iEvent . getByLabel ( \"allConversions\" , hConversions ); edm :: Handle < reco :: BeamSpot > bsHandle ; iEvent . getByLabel ( \"offlineBeamSpot\" , bsHandle ); const reco :: BeamSpot & beamspot = * bsHandle . product (); value_el_n = 0 ; const float el_min_pt = 5 ; std :: vector < GsfElectron > selectedElectrons ; for ( auto it = electrons -> begin (); it != electrons -> end (); it ++ ) { if ( it -> pt () > el_min_pt ) { selectedElectrons . emplace_back ( * it ); value_el_cutbasedid [ value_el_n ] = it -> passingCutBasedPreselection (); value_el_pfid [ value_el_n ] = it -> passingPflowPreselection (); //added for cut based id value_el_sigIetaIeta [ value_el_n ] = it -> sigmaIetaIeta (); value_el_hOverEm [ value_el_n ] = it -> hadronicOverEm (); value_el_fbrem [ value_el_n ] = it -> fbrem (); value_el_eOverP [ value_el_n ] = it -> eSuperClusterOverP (); value_el_dEtaIn [ value_el_n ] = it -> deltaEtaSuperClusterTrackAtVtx (); value_el_dPhiIn [ value_el_n ] = it -> deltaPhiSuperClusterTrackAtVtx (); value_el_ecalE [ value_el_n ] = it -> ecalEnergy (); //value_el_pIn[value_el_n] = it->trackMomentumAtVtx().p(); //Does not compile: 'math::XYZVectorF' has no member named 'p' value_el_pIn [ value_el_n ] = ( it -> ecalEnergy () / it -> eSuperClusterOverP () ); //same as above line according to twiki value_el_dr03TkSumPt [ value_el_n ] = it -> dr03TkSumPt (); value_el_dr03EcalRecHitSumEt [ value_el_n ] = it -> dr03EcalRecHitSumEt (); value_el_dr03HcalTowerSumEt [ value_el_n ] = it -> dr03HcalTowerSumEt (); value_el_expectedHits [ value_el_n ] = it -> gsfTrack () -> trackerExpectedHitsInner (). numberOfHits (); int missing_hits = it -> gsfTrack () -> trackerExpectedHitsInner (). numberOfHits () - it -> gsfTrack () -> hitPattern (). numberOfHits (); bool passelectronveto = ! ConversionTools :: hasMatchedConversion ( * it , hConversions , beamspot . position ()); if ( it -> passingPflowPreselection ()) { auto iso03 = it -> pfIsolationVariables (); value_el_pfreliso03all [ value_el_n ] = ( iso03 . chargedHadronIso + iso03 . neutralHadronIso + iso03 . photonIso ) / it -> pt (); } else { value_el_pfreliso03all [ value_el_n ] = -999 ; } float pfIso = value_el_pfreliso03all [ value_el_n ]; auto trk = it -> gsfTrack (); value_el_jetidx [ value_el_n ] = -1 ; value_el_genpartidx [ value_el_n ] = -1 ; value_el_isLoose [ value_el_n ] = false ; value_el_isMedium [ value_el_n ] = false ; value_el_isTight [ value_el_n ] = false ; if ( abs ( it -> eta ()) <= 1.479 ) { if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .007 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .15 && it -> sigmaIetaIeta () < .01 && it -> hadronicOverEm () < .12 && abs ( trk -> dxy ( pv )) < .02 && abs ( trk -> dz ( pv )) < .2 && missing_hits <= 1 && pfIso < .15 && passelectronveto == true && abs ( 1 / it -> ecalEnergy () -1 / ( it -> ecalEnergy () / it -> eSuperClusterOverP ())) < .05 ){ value_el_isLoose [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .004 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .06 && abs ( trk -> dz ( pv )) < .1 ){ value_el_isMedium [ value_el_n ] = true ; if ( abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .03 && missing_hits <= 0 && pfIso < .10 ){ value_el_isTight [ value_el_n ] = true ; } } } } else if ( abs ( it -> eta ()) > 1.479 && abs ( it -> eta ()) < 2.5 ) { if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .009 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .1 && it -> sigmaIetaIeta () < .03 && it -> hadronicOverEm () < .1 && abs ( trk -> dxy ( pv )) < .02 && abs ( trk -> dz ( pv )) < .2 && missing_hits <= 1 && pfIso < .15 && passelectronveto == true && abs ( 1 / it -> ecalEnergy () -1 / ( it -> ecalEnergy () / it -> eSuperClusterOverP ())) < .05 ) { value_el_isLoose [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .007 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .03 && abs ( trk -> dz ( pv )) < .1 ){ value_el_isMedium [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .005 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .02 && missing_hits <= 0 && pfIso < .10 ){ value_el_isTight [ value_el_n ] = true ; } } } } value_el_n ++ ; } } PHOTONS Handle < PhotonCollection > photons ; iEvent . getByLabel ( InputTag ( \"photons\" ), photons ); Handle < double > rhoHandle ; iEvent . getByLabel ( InputTag ( \"fixedGridRhoAll\" ), rhoHandle ); double rhoIso = std :: max ( * ( rhoHandle . product ()), 0.0 ); value_ph_n = 0 ; const float ph_min_pt = 5 ; std :: vector < Photon > selectedPhotons ; for ( auto it = photons -> begin (); it != photons -> end (); it ++ ) { if ( it -> pt () > ph_min_pt ) { bool passelectronveto = ! ConversionTools :: hasMatchedPromptElectron ( it -> superCluster (), electrons , hConversions , beamspot . position ()); double scEta = ( it ) -> superCluster () -> eta (); double CH_AEff , NH_AEff , Ph_AEff ; if ( fabs ( scEta ) > 2.4 ) { CH_AEff = 0.012 ; NH_AEff = 0.072 ; Ph_AEff = 0.266 ; } else if ( fabs ( scEta ) > 2.3 ) { CH_AEff = 0.020 ; NH_AEff = 0.039 ; Ph_AEff = 0.260 ; } else if ( fabs ( scEta ) > 2.2 ) { CH_AEff = 0.016 ; NH_AEff = 0.024 ; Ph_AEff = 0.262 ; } else if ( fabs ( scEta ) > 2.0 ) { CH_AEff = 0.012 ; NH_AEff = 0.015 ; Ph_AEff = 0.216 ; } else if ( fabs ( scEta ) > 1.479 ) { CH_AEff = 0.014 ; NH_AEff = 0.039 ; Ph_AEff = 0.112 ; } else if ( fabs ( scEta ) > 0.1 ) { CH_AEff = 0.010 ; NH_AEff = 0.057 ; Ph_AEff = 0.130 ; } else { CH_AEff = 0.012 ; NH_AEff = 0.030 ; Ph_AEff = 0.148 ; } selectedPhotons . emplace_back ( * it ); value_ph_pt [ value_ph_n ] = it -> pt (); value_ph_eta [ value_ph_n ] = it -> eta (); value_ph_phi [ value_ph_n ] = it -> phi (); value_ph_charge [ value_ph_n ] = it -> charge (); value_ph_mass [ value_ph_n ] = it -> mass (); value_ph_pfreliso03all [ value_ph_n ] = it -> ecalRecHitSumEtConeDR03 () / it -> pt (); value_ph_jetidx [ value_ph_n ] = -1 ; value_ph_genpartidx [ value_ph_n ] = -1 ; //added for cut based id //value_ph_passelectronveto[max_ph]; value_ph_hOverEm [ value_ph_n ] = it -> hadTowOverEm (); value_ph_sigIetaIeta [ value_ph_n ] = it -> sigmaIetaIeta (); value_ph_chargedHadronIso [ value_ph_n ] = it -> chargedHadronIso (); value_ph_neutralHadronIso [ value_ph_n ] = it -> neutralHadronIso (); value_ph_photonIso [ value_ph_n ] = it -> photonIso (); double corrPFCHIso = max ( it -> chargedHadronIso () - rhoIso * CH_AEff , 0. ); double corrPFNHIso = max ( it -> neutralHadronIso () - rhoIso * NH_AEff , 0. ); double corrPFPhIso = max ( it -> photonIso () - rhoIso * Ph_AEff , 0. ); value_ph_isTight [ value_ph_n ] = false ; value_ph_isMedium [ value_ph_n ] = false ; value_ph_isLoose [ value_ph_n ] = false ; if ( it -> eta () <= 1.479 ){ if ( it -> hadTowOverEm () < .05 && it -> sigmaIetaIeta () < .012 && corrPFCHIso < 2.6 && corrPFNHIso < ( 3.5 + .04 * it -> pt ()) && corrPFPhIso < ( 1.3 + .005 * it -> pt ()) && passelectronveto == true ) { value_ph_isLoose [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < .011 && corrPFCHIso < 1.5 && corrPFNHIso < ( 1.0 + .04 * it -> pt ()) && corrPFPhIso < ( .7 + .005 * it -> pt ())){ value_ph_isMedium [ value_ph_n ] = true ; if ( corrPFCHIso < .7 && corrPFNHIso < ( .4 + .04 * it -> pt ()) && corrPFPhIso < ( .5 + 0.005 * it -> pt ()) ){ value_ph_isTight [ value_ph_n ] = true ; } } } } else if ( it -> eta () > 1.479 && it -> eta () < 2.5 ) { if ( it -> hadTowOverEm () < .05 && it -> sigmaIetaIeta () < .034 && corrPFCHIso < 2.3 && corrPFNHIso < ( 2.9 + .04 * it -> pt ()) && passelectronveto == true ){ value_ph_isLoose [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < .033 && corrPFCHIso < 1.2 && corrPFNHIso < ( 1.5 + .04 * it -> pt ()) && corrPFPhIso < ( 1.0 + .005 * it -> pt ())) { value_ph_isMedium [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < 0.031 && corrPFCHIso < 0.5 ){ value_ph_isTight [ value_ph_n ] = true ; } } } } value_ph_n ++ ; } }","title":"Electrons and Photons"},{"location":"analysis/selection/objects/egamma/#objects","text":"Warning This page is under construction This twiki contains information on electron selection intended to be used with 2010 data. The selection is based on cuts on a small number of variables. Different thresholds are used for electrons found in the ECAL barrel and the ECAL endcap. Electron selection variables may be categorized in 3 groups: e-ID variables (shower shape, track cluster matching etc) isolation variables conversion rejection variables The sets of cuts given here are obtained by tuning all cuts together, but the sets of cuts on each of the 3 groups of variables may be used alone quite effectively. The fake rate, and fake sources, vary with ET, and for any set of cuts, rejection power and efficiency vary with ET. However, sets of cuts optimized for ET>25 GeV are near optimal for the interval 100>ET>20 GeV, and can usefully be employed down to 15 GeV. Ultimately the most performant selection should be obtained using multi-variate techniques, likelihood fits etc. Prior to that cut-based selections can provide a useful tool to understand the data and make comparison with MC. The advantages of \"Simple Cuts\" are: Cut inversion (used in many data driven signal extraction and background subtraction methodologies) is simple Smallest statistics are needed for full understanding and efficiency measurement It is simple to cleanly separate the e-ID, isolation and conversion rejection pieces The selection has been tuned in order to get a set of cuts with maximum background rejection for a given efficiency. ELECTRONS Handle < GsfElectronCollection > electrons ; iEvent . getByLabel ( InputTag ( \"gsfElectrons\" ), electrons ); edm :: Handle < reco :: ConversionCollection > hConversions ; iEvent . getByLabel ( \"allConversions\" , hConversions ); edm :: Handle < reco :: BeamSpot > bsHandle ; iEvent . getByLabel ( \"offlineBeamSpot\" , bsHandle ); const reco :: BeamSpot & beamspot = * bsHandle . product (); value_el_n = 0 ; const float el_min_pt = 5 ; std :: vector < GsfElectron > selectedElectrons ; for ( auto it = electrons -> begin (); it != electrons -> end (); it ++ ) { if ( it -> pt () > el_min_pt ) { selectedElectrons . emplace_back ( * it ); value_el_cutbasedid [ value_el_n ] = it -> passingCutBasedPreselection (); value_el_pfid [ value_el_n ] = it -> passingPflowPreselection (); //added for cut based id value_el_sigIetaIeta [ value_el_n ] = it -> sigmaIetaIeta (); value_el_hOverEm [ value_el_n ] = it -> hadronicOverEm (); value_el_fbrem [ value_el_n ] = it -> fbrem (); value_el_eOverP [ value_el_n ] = it -> eSuperClusterOverP (); value_el_dEtaIn [ value_el_n ] = it -> deltaEtaSuperClusterTrackAtVtx (); value_el_dPhiIn [ value_el_n ] = it -> deltaPhiSuperClusterTrackAtVtx (); value_el_ecalE [ value_el_n ] = it -> ecalEnergy (); //value_el_pIn[value_el_n] = it->trackMomentumAtVtx().p(); //Does not compile: 'math::XYZVectorF' has no member named 'p' value_el_pIn [ value_el_n ] = ( it -> ecalEnergy () / it -> eSuperClusterOverP () ); //same as above line according to twiki value_el_dr03TkSumPt [ value_el_n ] = it -> dr03TkSumPt (); value_el_dr03EcalRecHitSumEt [ value_el_n ] = it -> dr03EcalRecHitSumEt (); value_el_dr03HcalTowerSumEt [ value_el_n ] = it -> dr03HcalTowerSumEt (); value_el_expectedHits [ value_el_n ] = it -> gsfTrack () -> trackerExpectedHitsInner (). numberOfHits (); int missing_hits = it -> gsfTrack () -> trackerExpectedHitsInner (). numberOfHits () - it -> gsfTrack () -> hitPattern (). numberOfHits (); bool passelectronveto = ! ConversionTools :: hasMatchedConversion ( * it , hConversions , beamspot . position ()); if ( it -> passingPflowPreselection ()) { auto iso03 = it -> pfIsolationVariables (); value_el_pfreliso03all [ value_el_n ] = ( iso03 . chargedHadronIso + iso03 . neutralHadronIso + iso03 . photonIso ) / it -> pt (); } else { value_el_pfreliso03all [ value_el_n ] = -999 ; } float pfIso = value_el_pfreliso03all [ value_el_n ]; auto trk = it -> gsfTrack (); value_el_jetidx [ value_el_n ] = -1 ; value_el_genpartidx [ value_el_n ] = -1 ; value_el_isLoose [ value_el_n ] = false ; value_el_isMedium [ value_el_n ] = false ; value_el_isTight [ value_el_n ] = false ; if ( abs ( it -> eta ()) <= 1.479 ) { if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .007 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .15 && it -> sigmaIetaIeta () < .01 && it -> hadronicOverEm () < .12 && abs ( trk -> dxy ( pv )) < .02 && abs ( trk -> dz ( pv )) < .2 && missing_hits <= 1 && pfIso < .15 && passelectronveto == true && abs ( 1 / it -> ecalEnergy () -1 / ( it -> ecalEnergy () / it -> eSuperClusterOverP ())) < .05 ){ value_el_isLoose [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .004 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .06 && abs ( trk -> dz ( pv )) < .1 ){ value_el_isMedium [ value_el_n ] = true ; if ( abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .03 && missing_hits <= 0 && pfIso < .10 ){ value_el_isTight [ value_el_n ] = true ; } } } } else if ( abs ( it -> eta ()) > 1.479 && abs ( it -> eta ()) < 2.5 ) { if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .009 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .1 && it -> sigmaIetaIeta () < .03 && it -> hadronicOverEm () < .1 && abs ( trk -> dxy ( pv )) < .02 && abs ( trk -> dz ( pv )) < .2 && missing_hits <= 1 && pfIso < .15 && passelectronveto == true && abs ( 1 / it -> ecalEnergy () -1 / ( it -> ecalEnergy () / it -> eSuperClusterOverP ())) < .05 ) { value_el_isLoose [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .007 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .03 && abs ( trk -> dz ( pv )) < .1 ){ value_el_isMedium [ value_el_n ] = true ; if ( abs ( it -> deltaEtaSuperClusterTrackAtVtx ()) < .005 && abs ( it -> deltaPhiSuperClusterTrackAtVtx ()) < .02 && missing_hits <= 0 && pfIso < .10 ){ value_el_isTight [ value_el_n ] = true ; } } } } value_el_n ++ ; } } PHOTONS Handle < PhotonCollection > photons ; iEvent . getByLabel ( InputTag ( \"photons\" ), photons ); Handle < double > rhoHandle ; iEvent . getByLabel ( InputTag ( \"fixedGridRhoAll\" ), rhoHandle ); double rhoIso = std :: max ( * ( rhoHandle . product ()), 0.0 ); value_ph_n = 0 ; const float ph_min_pt = 5 ; std :: vector < Photon > selectedPhotons ; for ( auto it = photons -> begin (); it != photons -> end (); it ++ ) { if ( it -> pt () > ph_min_pt ) { bool passelectronveto = ! ConversionTools :: hasMatchedPromptElectron ( it -> superCluster (), electrons , hConversions , beamspot . position ()); double scEta = ( it ) -> superCluster () -> eta (); double CH_AEff , NH_AEff , Ph_AEff ; if ( fabs ( scEta ) > 2.4 ) { CH_AEff = 0.012 ; NH_AEff = 0.072 ; Ph_AEff = 0.266 ; } else if ( fabs ( scEta ) > 2.3 ) { CH_AEff = 0.020 ; NH_AEff = 0.039 ; Ph_AEff = 0.260 ; } else if ( fabs ( scEta ) > 2.2 ) { CH_AEff = 0.016 ; NH_AEff = 0.024 ; Ph_AEff = 0.262 ; } else if ( fabs ( scEta ) > 2.0 ) { CH_AEff = 0.012 ; NH_AEff = 0.015 ; Ph_AEff = 0.216 ; } else if ( fabs ( scEta ) > 1.479 ) { CH_AEff = 0.014 ; NH_AEff = 0.039 ; Ph_AEff = 0.112 ; } else if ( fabs ( scEta ) > 0.1 ) { CH_AEff = 0.010 ; NH_AEff = 0.057 ; Ph_AEff = 0.130 ; } else { CH_AEff = 0.012 ; NH_AEff = 0.030 ; Ph_AEff = 0.148 ; } selectedPhotons . emplace_back ( * it ); value_ph_pt [ value_ph_n ] = it -> pt (); value_ph_eta [ value_ph_n ] = it -> eta (); value_ph_phi [ value_ph_n ] = it -> phi (); value_ph_charge [ value_ph_n ] = it -> charge (); value_ph_mass [ value_ph_n ] = it -> mass (); value_ph_pfreliso03all [ value_ph_n ] = it -> ecalRecHitSumEtConeDR03 () / it -> pt (); value_ph_jetidx [ value_ph_n ] = -1 ; value_ph_genpartidx [ value_ph_n ] = -1 ; //added for cut based id //value_ph_passelectronveto[max_ph]; value_ph_hOverEm [ value_ph_n ] = it -> hadTowOverEm (); value_ph_sigIetaIeta [ value_ph_n ] = it -> sigmaIetaIeta (); value_ph_chargedHadronIso [ value_ph_n ] = it -> chargedHadronIso (); value_ph_neutralHadronIso [ value_ph_n ] = it -> neutralHadronIso (); value_ph_photonIso [ value_ph_n ] = it -> photonIso (); double corrPFCHIso = max ( it -> chargedHadronIso () - rhoIso * CH_AEff , 0. ); double corrPFNHIso = max ( it -> neutralHadronIso () - rhoIso * NH_AEff , 0. ); double corrPFPhIso = max ( it -> photonIso () - rhoIso * Ph_AEff , 0. ); value_ph_isTight [ value_ph_n ] = false ; value_ph_isMedium [ value_ph_n ] = false ; value_ph_isLoose [ value_ph_n ] = false ; if ( it -> eta () <= 1.479 ){ if ( it -> hadTowOverEm () < .05 && it -> sigmaIetaIeta () < .012 && corrPFCHIso < 2.6 && corrPFNHIso < ( 3.5 + .04 * it -> pt ()) && corrPFPhIso < ( 1.3 + .005 * it -> pt ()) && passelectronveto == true ) { value_ph_isLoose [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < .011 && corrPFCHIso < 1.5 && corrPFNHIso < ( 1.0 + .04 * it -> pt ()) && corrPFPhIso < ( .7 + .005 * it -> pt ())){ value_ph_isMedium [ value_ph_n ] = true ; if ( corrPFCHIso < .7 && corrPFNHIso < ( .4 + .04 * it -> pt ()) && corrPFPhIso < ( .5 + 0.005 * it -> pt ()) ){ value_ph_isTight [ value_ph_n ] = true ; } } } } else if ( it -> eta () > 1.479 && it -> eta () < 2.5 ) { if ( it -> hadTowOverEm () < .05 && it -> sigmaIetaIeta () < .034 && corrPFCHIso < 2.3 && corrPFNHIso < ( 2.9 + .04 * it -> pt ()) && passelectronveto == true ){ value_ph_isLoose [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < .033 && corrPFCHIso < 1.2 && corrPFNHIso < ( 1.5 + .04 * it -> pt ()) && corrPFPhIso < ( 1.0 + .005 * it -> pt ())) { value_ph_isMedium [ value_ph_n ] = true ; if ( it -> sigmaIetaIeta () < 0.031 && corrPFCHIso < 0.5 ){ value_ph_isTight [ value_ph_n ] = true ; } } } } value_ph_n ++ ; } }","title":"Objects"},{"location":"analysis/selection/objects/jets/","text":"Jets \u00b6 What are jets? \u00b6 Jets are spatially-grouped collections of long-lived particles that are produced when a quark or gluon hadronizes. The kinetmatic properties of jets resemble that of the initial partons that produced them. In the CMS language, jets are made up of many particles, with the following predictable energy composition: ~65% charged hadrons ~25% photons (from neutral pions) ~10% neutral hadrons Jets are very messy! Hadronization and the subsequent decays of unstable hadrons can produce 100s of particles near each other in the CMS detector. Hence these particles are rarely analyzed individually. How can we determine which particle candidates should be included in each jet? Clustering \u00b6 Jets can be clustered using a variety of different inputs from the CMS detector. \"CaloJets\" use only calorimeter energy deposits. \"GenJets\" use generated particles from a simulation. But by far the most common are \"PFJets\", from particle flow candidates. The result of the CMS Particle Flow algorithm is a list of particle candidates that account for all inner-tracker and muon tracks and all above-threshold energy deposits in the calorimeters. These particles are formed into jets using a \"clustering algorithm\". The most common algorithm used by CMS is the \"anti-kt\" algorithm, which is abbreviated \"AK\". It iterates over particle pairs and finds the two (i and j) that are the closest in some distance measure and determines whether to combine them: The momentum power (-2) used by the anti-kt algorithm means that higher-momentum particles are clustered first. This leads to jets with a round shape that tend to be centered on the hardest particle. In CMS software this clustering is implemented using the fastjet package. Pileup \u00b6 Inevitably, the list of particle flow candidates contains particles that did not originate from the primary interaction point. CMS experiences multiple simultaneous collisions, called \"pileup\", during each \"bunch crossing\" of the LHC, so particles from multiple collisions coexist in the detector. There are various methods to remove their contributions from jets: Charged hadron subtraction CHS : all charged hadron candidates are associated with a track. If the track is not associated with the primary vertex, that charged hadron can be removed from the list. CHS is limited to the region of the detector covered by the inner tracker. The pileup contribution to neutral hadrons has to be removed mathematically which will be discussed later. PileUp Per Particle Identification (PUPPI, available in Run 2): CHS is applied, and then all remaining particles are weighted based on their likelihood of arising from pileup. This method is more stable and performant in high pileup scenarios such as the upcoming HL-LHC era. Accessing Jets in CMS Software \u00b6 Jets software classes have the same basic 4-vector methods as the objects discussed in the previous lesson: Handle < PFJetCollection > myjets ; iEvent . getByLabel ( InputTag ( \"ak5PFJets\" ), myjets ); for ( reco :: PFJetCollection :: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ jet_e . push_back ( itjet -> energy ()); jet_pt . push_back ( itjet -> pt ()); jet_eta . push_back ( itjet -> eta ()); jet_phi . push_back ( itjet -> phi ()); jet_mass . push_back ( itjet -> mass ()); } Jet ID \u00b6 Particle-flow jets are not immune to noise in the detector, and jets used in analyses should be filtered to remove noise jets. CMS has defined a Jet ID with criteria for good jets: The PFlow jets are required to have charged hadron fraction CHF > 0.0 if within tracking fiducial region of |eta| < 2.4, neutral hadron fraction NHF < 1.0, charged electromagnetic (electron) fraction CEF < 1.0, and neutral electromagnetic (photon) fraction NEF < 1.0. These requirements remove fake jets arising from spurious energy depositions in a single sub-detector. These criteria demonstrate how particle-flow jets combine information across subdetectors. Jets will typically have energy from electrons and photons, but those fractions of the total energy should be less than one. Similarly, jets should have some energy from charged hadrons if they overlap the inner tracker, and all the energy should not come from neutral hadrons. A mixture of energy sources is expected for genuine jets. All of these energy fractions (and more) can be accessed from the jet objects. You can use the cms-sw github repository to see what methods are available for PFJets. We can implement a jet ID to reject jets that do not pass so that these jets are not stored in any of the tree branches. This code show an implementation of Jet ID cuts while also applying a minimum momentum threshold. for ( reco :: PFJetCollection :: const_iterator itjet = jets -> begin (); itjet != jets -> end (); ++ itjet ){ if ( itjet -> pt > jet_min_pt && itjet -> chargedHadronEnergyFraction () > 0 && itjet -> neutralHadronEnergyFraction () < 1.0 && itjet -> electronEnergyFraction () < 1.0 && itjet -> photonEnergyFraction () < 1.0 ){ // jet calculations B Tagging Algorithms \u00b6 Jet reconstruction and identification is an important part of the analyses at the LHC. A jet may contain the hadronization products of any quark or gluon, or possibly the decay products of more massive particles such as W or Higgs bosons. Several b tagging\u201d algorithms exist to identify jets from the hadronization of b quarks, which have unique properties that distinguish them from light quark or gluon jets. Tagging algorithms first connect the jets with good quality tracks that are either associated with one of the jet\u2019s particle flow candidates or within a nearby cone. Both tracks and \u201csecondary vertices\u201d (track vertices from the decays of b hadrons) can be used in track-based, vertex-based, or \u201ccombined\u201d tagging algorithms. The specific details depend upon the algorithm use. However, they all exploit properties of b hadrons such as: -long lifetime, -large mass, -high track multiplicity, -large semiloptonic branching fraction, -hard fragmentation function. Tagging algorithms are Algorithms that are used for b-tagging: -Track Counting: identifies a b jet if it contains at least N tracks with significantly non-zero impact parameters. -Jet Probability: combines information from all selected tracks in the jet and uses probability density functions to assign a probability to each track. -Soft Muon and Soft Electron: identifies b jets by searching for a lepton from a semi-leptonic b decay. -Simple Secondary Vertex: reconstructs the b decay vertex and calculates a discriminator using related kinematic variables. - Combined Secondary Vertex: exploits all known kinematic variables of the jets, information about track impact parameter significance and the secondary vertices to distinguish b jets. This tagger became the default CMS algorithm. These algorithms produce a single, real number (often the output of an MVA) called a b tagging \u201cdiscriminator\u201d for each jet. The more positive the discriminator value, the more likely it is that this jet contained b hadrons. Accessing Tagging Information \u00b6 In PatJetAnalyzer.cc we access the information from the Combined Secondary Vertex (CSV) b tagging algorithm and associate discriminator values with the jets. The CSV values are stored in a separate collection in the POET files called a JetTagCollection, which is effectively a vector of associations between jet references and float values (such as a b-tagging discriminator). #include \"DataFormats/PatCandidates/interface/Jet.h\" Handle < PFJetCollection > myjets ; iEvent . getByLabel ( InputTag ( \"ak5PFJets\" ), myjets ); //define b-tag discriminators handle and get the discriminators for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ // from the btag collection get the float (second) from the association to this jet. jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); } You can use the command edmDumpEventContent to investiate other b tagging algorithms available as edm::AssociationVector types. This is an example opening the collections for two alternate taggers--the MVA version of CSV and the high purity track counting tagger, which was the most common tagger in 2011: // inside the jet loop jet_btagheb . push_back ( itjet -> bDiscriminator ( \"simpleSecondaryVertexHighEffBJetTags\" )); jet_btagtc . push_back ( itjet -> bDiscriminator ( \"trackCountingHighEffBJetTags\" )); The distributions in ttbar events (excluding events with values of -9 where the tagger was not evaluated) are shown below. The track counting discriminant is quite different and ranges 0-30 or so. Working Points \u00b6 A jet is considered \"b tagged\" if the discriminator value exceeds some threshold. Different thresholds will have different efficiencies for identifying true b quark jets and for mis-tagging light quark jets. As we saw for muons and other objects, a \"loose\" working point will allow the highest mis-tagging rate, while a \"tight\" working point will sacrifice some correct-tag efficiency to reduce mis-tagging. The CSV algorithm has working points defined based on mis-tagging rate: -Loose = ~10% mis-tagging = discriminator > 0.244 -Medium = ~1% mis-tagging = discriminator > 0.679 -Tight = ~0.1% mis-tagging = discriminator > 0.898 We can count the number of \"Medium CSV\" b-tagged jets by summing up the number of jets with discriminant values greater than 0.679. After adding a variable declaration and branch we can sum up the counter: value_jet_nCSVM = 0 ; for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ // skipping bits jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); if ( jet_btag . at ( value_jet_n ) > 0.679 ) value_jet_nCSVM ++ ; } We show distributions of the number CSV b jets at the medium working point in Drell-Yan events and top pair events. As expected there are significantly more b jets in the top pair sample. Data and Simulation Differences \u00b6 When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources. The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more . Corrections must be applied to make the b-tagging performance match between data and simulation. Read more about these corrections and their uncertainties on this page . When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources. The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more Warning This page is under construction","title":"Jets"},{"location":"analysis/selection/objects/jets/#jets","text":"","title":"Jets"},{"location":"analysis/selection/objects/jets/#what-are-jets","text":"Jets are spatially-grouped collections of long-lived particles that are produced when a quark or gluon hadronizes. The kinetmatic properties of jets resemble that of the initial partons that produced them. In the CMS language, jets are made up of many particles, with the following predictable energy composition: ~65% charged hadrons ~25% photons (from neutral pions) ~10% neutral hadrons Jets are very messy! Hadronization and the subsequent decays of unstable hadrons can produce 100s of particles near each other in the CMS detector. Hence these particles are rarely analyzed individually. How can we determine which particle candidates should be included in each jet?","title":"What are jets?"},{"location":"analysis/selection/objects/jets/#clustering","text":"Jets can be clustered using a variety of different inputs from the CMS detector. \"CaloJets\" use only calorimeter energy deposits. \"GenJets\" use generated particles from a simulation. But by far the most common are \"PFJets\", from particle flow candidates. The result of the CMS Particle Flow algorithm is a list of particle candidates that account for all inner-tracker and muon tracks and all above-threshold energy deposits in the calorimeters. These particles are formed into jets using a \"clustering algorithm\". The most common algorithm used by CMS is the \"anti-kt\" algorithm, which is abbreviated \"AK\". It iterates over particle pairs and finds the two (i and j) that are the closest in some distance measure and determines whether to combine them: The momentum power (-2) used by the anti-kt algorithm means that higher-momentum particles are clustered first. This leads to jets with a round shape that tend to be centered on the hardest particle. In CMS software this clustering is implemented using the fastjet package.","title":"Clustering"},{"location":"analysis/selection/objects/jets/#pileup","text":"Inevitably, the list of particle flow candidates contains particles that did not originate from the primary interaction point. CMS experiences multiple simultaneous collisions, called \"pileup\", during each \"bunch crossing\" of the LHC, so particles from multiple collisions coexist in the detector. There are various methods to remove their contributions from jets: Charged hadron subtraction CHS : all charged hadron candidates are associated with a track. If the track is not associated with the primary vertex, that charged hadron can be removed from the list. CHS is limited to the region of the detector covered by the inner tracker. The pileup contribution to neutral hadrons has to be removed mathematically which will be discussed later. PileUp Per Particle Identification (PUPPI, available in Run 2): CHS is applied, and then all remaining particles are weighted based on their likelihood of arising from pileup. This method is more stable and performant in high pileup scenarios such as the upcoming HL-LHC era.","title":"Pileup"},{"location":"analysis/selection/objects/jets/#accessing-jets-in-cms-software","text":"Jets software classes have the same basic 4-vector methods as the objects discussed in the previous lesson: Handle < PFJetCollection > myjets ; iEvent . getByLabel ( InputTag ( \"ak5PFJets\" ), myjets ); for ( reco :: PFJetCollection :: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ jet_e . push_back ( itjet -> energy ()); jet_pt . push_back ( itjet -> pt ()); jet_eta . push_back ( itjet -> eta ()); jet_phi . push_back ( itjet -> phi ()); jet_mass . push_back ( itjet -> mass ()); }","title":"Accessing Jets in CMS Software"},{"location":"analysis/selection/objects/jets/#jet-id","text":"Particle-flow jets are not immune to noise in the detector, and jets used in analyses should be filtered to remove noise jets. CMS has defined a Jet ID with criteria for good jets: The PFlow jets are required to have charged hadron fraction CHF > 0.0 if within tracking fiducial region of |eta| < 2.4, neutral hadron fraction NHF < 1.0, charged electromagnetic (electron) fraction CEF < 1.0, and neutral electromagnetic (photon) fraction NEF < 1.0. These requirements remove fake jets arising from spurious energy depositions in a single sub-detector. These criteria demonstrate how particle-flow jets combine information across subdetectors. Jets will typically have energy from electrons and photons, but those fractions of the total energy should be less than one. Similarly, jets should have some energy from charged hadrons if they overlap the inner tracker, and all the energy should not come from neutral hadrons. A mixture of energy sources is expected for genuine jets. All of these energy fractions (and more) can be accessed from the jet objects. You can use the cms-sw github repository to see what methods are available for PFJets. We can implement a jet ID to reject jets that do not pass so that these jets are not stored in any of the tree branches. This code show an implementation of Jet ID cuts while also applying a minimum momentum threshold. for ( reco :: PFJetCollection :: const_iterator itjet = jets -> begin (); itjet != jets -> end (); ++ itjet ){ if ( itjet -> pt > jet_min_pt && itjet -> chargedHadronEnergyFraction () > 0 && itjet -> neutralHadronEnergyFraction () < 1.0 && itjet -> electronEnergyFraction () < 1.0 && itjet -> photonEnergyFraction () < 1.0 ){ // jet calculations","title":"Jet ID"},{"location":"analysis/selection/objects/jets/#b-tagging-algorithms","text":"Jet reconstruction and identification is an important part of the analyses at the LHC. A jet may contain the hadronization products of any quark or gluon, or possibly the decay products of more massive particles such as W or Higgs bosons. Several b tagging\u201d algorithms exist to identify jets from the hadronization of b quarks, which have unique properties that distinguish them from light quark or gluon jets. Tagging algorithms first connect the jets with good quality tracks that are either associated with one of the jet\u2019s particle flow candidates or within a nearby cone. Both tracks and \u201csecondary vertices\u201d (track vertices from the decays of b hadrons) can be used in track-based, vertex-based, or \u201ccombined\u201d tagging algorithms. The specific details depend upon the algorithm use. However, they all exploit properties of b hadrons such as: -long lifetime, -large mass, -high track multiplicity, -large semiloptonic branching fraction, -hard fragmentation function. Tagging algorithms are Algorithms that are used for b-tagging: -Track Counting: identifies a b jet if it contains at least N tracks with significantly non-zero impact parameters. -Jet Probability: combines information from all selected tracks in the jet and uses probability density functions to assign a probability to each track. -Soft Muon and Soft Electron: identifies b jets by searching for a lepton from a semi-leptonic b decay. -Simple Secondary Vertex: reconstructs the b decay vertex and calculates a discriminator using related kinematic variables. - Combined Secondary Vertex: exploits all known kinematic variables of the jets, information about track impact parameter significance and the secondary vertices to distinguish b jets. This tagger became the default CMS algorithm. These algorithms produce a single, real number (often the output of an MVA) called a b tagging \u201cdiscriminator\u201d for each jet. The more positive the discriminator value, the more likely it is that this jet contained b hadrons.","title":"B Tagging Algorithms"},{"location":"analysis/selection/objects/jets/#accessing-tagging-information","text":"In PatJetAnalyzer.cc we access the information from the Combined Secondary Vertex (CSV) b tagging algorithm and associate discriminator values with the jets. The CSV values are stored in a separate collection in the POET files called a JetTagCollection, which is effectively a vector of associations between jet references and float values (such as a b-tagging discriminator). #include \"DataFormats/PatCandidates/interface/Jet.h\" Handle < PFJetCollection > myjets ; iEvent . getByLabel ( InputTag ( \"ak5PFJets\" ), myjets ); //define b-tag discriminators handle and get the discriminators for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ // from the btag collection get the float (second) from the association to this jet. jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); } You can use the command edmDumpEventContent to investiate other b tagging algorithms available as edm::AssociationVector types. This is an example opening the collections for two alternate taggers--the MVA version of CSV and the high purity track counting tagger, which was the most common tagger in 2011: // inside the jet loop jet_btagheb . push_back ( itjet -> bDiscriminator ( \"simpleSecondaryVertexHighEffBJetTags\" )); jet_btagtc . push_back ( itjet -> bDiscriminator ( \"trackCountingHighEffBJetTags\" )); The distributions in ttbar events (excluding events with values of -9 where the tagger was not evaluated) are shown below. The track counting discriminant is quite different and ranges 0-30 or so.","title":"Accessing Tagging Information"},{"location":"analysis/selection/objects/jets/#working-points","text":"A jet is considered \"b tagged\" if the discriminator value exceeds some threshold. Different thresholds will have different efficiencies for identifying true b quark jets and for mis-tagging light quark jets. As we saw for muons and other objects, a \"loose\" working point will allow the highest mis-tagging rate, while a \"tight\" working point will sacrifice some correct-tag efficiency to reduce mis-tagging. The CSV algorithm has working points defined based on mis-tagging rate: -Loose = ~10% mis-tagging = discriminator > 0.244 -Medium = ~1% mis-tagging = discriminator > 0.679 -Tight = ~0.1% mis-tagging = discriminator > 0.898 We can count the number of \"Medium CSV\" b-tagged jets by summing up the number of jets with discriminant values greater than 0.679. After adding a variable declaration and branch we can sum up the counter: value_jet_nCSVM = 0 ; for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ // skipping bits jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); if ( jet_btag . at ( value_jet_n ) > 0.679 ) value_jet_nCSVM ++ ; } We show distributions of the number CSV b jets at the medium working point in Drell-Yan events and top pair events. As expected there are significantly more b jets in the top pair sample.","title":"Working Points"},{"location":"analysis/selection/objects/jets/#data-and-simulation-differences","text":"When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources. The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more . Corrections must be applied to make the b-tagging performance match between data and simulation. Read more about these corrections and their uncertainties on this page . When training a tagging algorithm, it is highly probable that the efficiencies for tagging different quark flavors as b jets will vary between simulation and data. These differences must be measured and corrected for using \"scale factors\" constructed from ratios of the efficiencies from different sources. The figures below show examples of the b and light quark efficiencies and scale factors as a function of jet momentum read more Warning This page is under construction","title":"Data and Simulation Differences"},{"location":"analysis/selection/objects/met/","text":"MET \u00b6 What is MET? \u00b6 Missing transverse momentum is the negative vector sum of the transverse momenta of all particle flow candidates in an event. The magnitude of the missing transverse momentum vector is called missing transverse energy and referred to with the acronym \u201cMET\u201d. Since energy corrections are made to the particle flow jets, those corrections are propagated to MET by adding back the momentum vectors of the original jets and then subtracting the momentum vectors of the corrected jets. This correction is called \u201cType 1\u201d and is standard for all CMS analyses. The jet energy corrections will be discussed more deeply at the end of this lesson. In MetAnalyzer.cc we open the particle flow MET module and extract the magnitude and angle of the MET, the sum of all energy in the detector, and variables related to the \u201csignificance\u201d of the MET. Note that MET quantities have a single value for the entire event, unlike the objects studied previously. Handle < PFMETCollection > met ; iEvent . getByLabel ( InputTag ( \"pfMet\" ), met ); value_met_pt = met -> begin () -> pt (); value_met_phi = met -> begin () -> phi (); value_met_sumet = met -> begin () -> sumEt (); value_met_significance = met -> begin () -> significance (); auto cov = met -> begin () -> getSignificanceMatrix (); value_met_covxx = cov [ 0 ][ 0 ]; value_met_covxy = cov [ 0 ][ 1 ]; value_met_covyy = cov [ 1 ][ 1 ]; MET significance can be a useful tool: it describes the likelihood that the MET arose from noise or mismeasurement in the detector as opposed to a neutrino or similar non-interacting particle. The four-vectors of the other physics objects along with their uncertainties are required to compute the significance of the MET signature. MET that is directed nearly (anti)colinnear with a physics object is likely to arise from mismeasurement and should not have a large significance. The difference between the Drell-Yan events with primarily fake MET and the top pair events with primarily genuine MET can be seen by drawing MET_pt or by drawing MET_significance. In both distributions the Drell-Yan events have smaller values than the top pair events. Warning This page is under construction","title":"Missing ET"},{"location":"analysis/selection/objects/met/#met","text":"","title":"MET"},{"location":"analysis/selection/objects/met/#what-is-met","text":"Missing transverse momentum is the negative vector sum of the transverse momenta of all particle flow candidates in an event. The magnitude of the missing transverse momentum vector is called missing transverse energy and referred to with the acronym \u201cMET\u201d. Since energy corrections are made to the particle flow jets, those corrections are propagated to MET by adding back the momentum vectors of the original jets and then subtracting the momentum vectors of the corrected jets. This correction is called \u201cType 1\u201d and is standard for all CMS analyses. The jet energy corrections will be discussed more deeply at the end of this lesson. In MetAnalyzer.cc we open the particle flow MET module and extract the magnitude and angle of the MET, the sum of all energy in the detector, and variables related to the \u201csignificance\u201d of the MET. Note that MET quantities have a single value for the entire event, unlike the objects studied previously. Handle < PFMETCollection > met ; iEvent . getByLabel ( InputTag ( \"pfMet\" ), met ); value_met_pt = met -> begin () -> pt (); value_met_phi = met -> begin () -> phi (); value_met_sumet = met -> begin () -> sumEt (); value_met_significance = met -> begin () -> significance (); auto cov = met -> begin () -> getSignificanceMatrix (); value_met_covxx = cov [ 0 ][ 0 ]; value_met_covxy = cov [ 0 ][ 1 ]; value_met_covyy = cov [ 1 ][ 1 ]; MET significance can be a useful tool: it describes the likelihood that the MET arose from noise or mismeasurement in the detector as opposed to a neutrino or similar non-interacting particle. The four-vectors of the other physics objects along with their uncertainties are required to compute the significance of the MET signature. MET that is directed nearly (anti)colinnear with a physics object is likely to arise from mismeasurement and should not have a large significance. The difference between the Drell-Yan events with primarily fake MET and the top pair events with primarily genuine MET can be seen by drawing MET_pt or by drawing MET_significance. In both distributions the Drell-Yan events have smaller values than the top pair events. Warning This page is under construction","title":"What is MET?"},{"location":"analysis/selection/objects/muons/","text":"Muons \u00b6 The Physics Objects page shows you how to access muon collections in CMS, and which header files should be included in your C++ code in order to access all of their class information. On the Common Tools page you can find instructions to access all the basic kinematic information about any physics object. Muon detector information \u00b6 Muon corrections \u00b6 Warning This page is under construction Installation \u00b6 Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated.","title":"Muons"},{"location":"analysis/selection/objects/muons/#muons","text":"The Physics Objects page shows you how to access muon collections in CMS, and which header files should be included in your C++ code in order to access all of their class information. On the Common Tools page you can find instructions to access all the basic kinematic information about any physics object.","title":"Muons"},{"location":"analysis/selection/objects/muons/#muon-detector-information","text":"","title":"Muon detector information"},{"location":"analysis/selection/objects/muons/#muon-corrections","text":"Warning This page is under construction","title":"Muon corrections"},{"location":"analysis/selection/objects/muons/#installation","text":"Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated.","title":"Installation"},{"location":"analysis/selection/objects/tools/","text":"Common tools for physics objects \u00b6 All CMS physics objects allow you to access important kinematic quantities in a common way. All objects have associated energy-momentum vectors, typically constructed using transverse momentum, pseudorapdity, azimuthal angle, and mass or energy . 4-vector access functions \u00b6 The Physics Objects page shows how to access a collection of muons in an EDAnalyzer. The following member functions are available for muons, electrons, photons, tau leptons, and jets. We will use the example of a loop over the muon collection shown previously: for ( auto mu = mymuons -> begin (); mu != mymuons -> end (); mu ++ ) { // minimal set to build a ROOT TLorentzVector double tranvserve_momentum = mu -> pt (); double pseudorapidity = mu -> eta (); double azimuthal_angle = mu -> phi (); double mass = mu -> mass (); // electric charge double charge = mu -> charge (); // direct 4-vector access math :: XYZLorentzVector four_momentum = mu -> p4 (); // some additional methods double energy = mu -> energy (); double transverse_mass = mu -> mt (); double transverse_energy = mu -> et (); double polar_angle = mu -> theta (); double rapidity = mu -> y (); // or mu->rapidity() double x_momentum = mu -> px (); // similar for y, z. } These and other basic kinematic methods are define here in CMSSW . Track access functions \u00b6 Many objects are also connected to tracks from the CMS tracking detectors. Information from tracks provides other kinematic quantities that are common to multiple types of objects. From a muon object, we can access the associated track while looping over muons via the globalTrack method: auto trk = mu -> globalTrack (); // muon track Often, the most pertinent information about an object (such as a muon) to access from its associated track is its impact parameter with respect to the primary interaction vertex. Since muons can also be tracked through the muon detectors, we first check if the track is well-defined, and then access impact parameters in the xy-plane ( dxy or d0 ) and along the beam axis ( dz ), as well as their respective uncertainties. if ( trk . isNonnull ()) { value_mu_dxy [ value_mu_n ] = trk -> dxy ( pv ); value_mu_dz [ value_mu_n ] = trk -> dz ( pv ); value_mu_dxyErr [ value_mu_n ] = trk -> d0Error (); value_mu_dzErr [ value_mu_n ] = trk -> dzError (); } Challenge: electron track properties \u00b6 Access and store the electron's charge and track impact parameter values, following the examples set for muons. Electron tracks are found using the Gaussian-sum filter method, which influences the member function name to access the track: auto trk = it -> gsfTrack (); // electron track Solution: \u00b6 Again, add information in three places: Declarations: int value_el_charge [ max_el ]; float value_el_dxy [ max_el ]; float value_el_dxyErr [ max_el ]; float value_el_dz [ max_el ]; float value_el_dzErr [ max_el ]; tree -> Branch ( \"Electron_charge\" , value_el_charge , \"Electron_charge[nElectron]/I\" ); tree -> Branch ( \"Electron_dxy\" , value_el_dxy , \"Electron_dxy[nElectron]/F\" ); tree -> Branch ( \"Electron_dxyErr\" , value_el_dxyErr , \"Electron_dxyErr[nElectron]/F\" ); tree -> Branch ( \"Electron_dz\" , value_el_dz , \"Electron_dz[nElectron]/F\" ); tree -> Branch ( \"Electron_dzErr\" , value_el_dzErr , \"Electron_dzErr[nElectron]/F\" ); And access values in the electron loop . The format is identical to the muon loop ! value_el_charge [ value_el_n ] = it -> charge (); auto trk = it -> gsfTrack (); value_el_dxy [ value_el_n ] = trk -> dxy ( pv ); value_el_dz [ value_el_n ] = trk -> dz ( pv ); value_el_dxyErr [ value_el_n ] = trk -> d0Error (); value_el_dzErr [ value_el_n ] = trk -> dzError (); Matching to generated particles \u00b6 Simulated files also contain information about the generator-level particles that were propagated into the showering and detector simulations. Physics objects can be matched to these generated particles spatially. The AOD2NanoAOD tool sets up several utility functions for matching: findBestMatch , findBestVisibleMatch , and subtractInvisible . The findBestMatch function takes generated particles (with an automated type T ) and the 4-vector of a physics object. It uses angular separation to find the closest generated particle to the reconstructed particle: template < typename T > int findBestMatch ( T & gens , reco :: Candidate :: LorentzVector & p4 ) { # initial definition of \"closest\" is really bad float minDeltaR = 999.0 ; int idx = -1 ; # loop over the generated particles for ( auto g = gens . begin (); g != gens . end (); g ++ ) { const auto tmp = deltaR ( g -> p4 (), p4 ); # if it's closer, overwrite the definition of \"closest\" if ( tmp < minDeltaR ) { minDeltaR = tmp ; idx = g - gens . begin (); } } return idx ; # return the index of the match } The other utility functions are similar, but correct for generated particles that decay to neutrinos, which would affect the \"visible\" 4-vector. In the AOD2NanoAOD tool, muons are matched only to \"interesting\" generated particles, which are all the leptons and photons (PDG ID 11, 13, 15, 22). Their generator status must be 1, indicating a final-state particle after any radiation chain. if ( ! isData ){ value_gen_n = 0 ; for ( auto p = selectedMuons . begin (); p != selectedMuons . end (); p ++ ) { // get the muon's 4-vector auto p4 = p -> p4 (); // perform the matching with a utility function auto idx = findBestVisibleMatch ( interestingGenParticles , p4 ); // if a match was found, save the generated particle's information if ( idx != -1 ) { auto g = interestingGenParticles . begin () + idx ; // another example of common 4-vector access functions! value_gen_pt [ value_gen_n ] = g -> pt (); value_gen_eta [ value_gen_n ] = g -> eta (); value_gen_phi [ value_gen_n ] = g -> phi (); value_gen_mass [ value_gen_n ] = g -> mass (); // gen particles also have ID and status from the generator value_gen_pdgid [ value_gen_n ] = g -> pdgId (); value_gen_status [ value_gen_n ] = g -> status (); // save the index of the matched gen particle value_mu_genpartidx [ p - selectedMuons . begin ()] = value_gen_n ; value_gen_n ++ ; } } } Challenge: electron matching \u00b6 Match selected electrons to the interesting generated particles. Compile your code and run over the simulation test file. Using the ROOT TBrowser, look at some histograms of the branches you've added to the tree throughout this episode. $ scram b $ cmsRun configs/simulation_cfg.py $ root -l output.root [0] TBrowser b Solution \u00b6 The structure for this matching exercise is identical to the muon matching segment. Loop over selected electrons, use the findBestVisibleMatch function to match it to an \"interesting\" particle and then to a jet. >> // Match electrons with gen particles and jets >> for ( auto p = selectedElectrons . begin (); p != selectedElectrons . end (); p ++ ) { >> // Gen particle matching >> auto p4 = p -> p4 (); >> auto idx = findBestVisibleMatch ( interestingGenParticles , p4 ); >> if ( idx != -1 ) { >> auto g = interestingGenParticles . begin () + idx ; >> value_gen_pt [ value_gen_n ] = g -> pt (); >> value_gen_eta [ value_gen_n ] = g -> eta (); >> value_gen_phi [ value_gen_n ] = g -> phi (); >> value_gen_mass [ value_gen_n ] = g -> mass (); >> value_gen_pdgid [ value_gen_n ] = g -> pdgId (); >> value_gen_status [ value_gen_n ] = g -> status (); >> value_el_genpartidx [ p - selectedElectrons . begin ()] = value_gen_n ; >> value_gen_n ++ ; >> } >> >> // Jet matching >> value_el_jetidx [ p - selectedElectrons . begin ()] = findBestMatch ( selectedJets , p4 ); >> } Warning This page is under construction","title":"Common Tools"},{"location":"analysis/selection/objects/tools/#common-tools-for-physics-objects","text":"All CMS physics objects allow you to access important kinematic quantities in a common way. All objects have associated energy-momentum vectors, typically constructed using transverse momentum, pseudorapdity, azimuthal angle, and mass or energy .","title":"Common tools for physics objects"},{"location":"analysis/selection/objects/tools/#4-vector-access-functions","text":"The Physics Objects page shows how to access a collection of muons in an EDAnalyzer. The following member functions are available for muons, electrons, photons, tau leptons, and jets. We will use the example of a loop over the muon collection shown previously: for ( auto mu = mymuons -> begin (); mu != mymuons -> end (); mu ++ ) { // minimal set to build a ROOT TLorentzVector double tranvserve_momentum = mu -> pt (); double pseudorapidity = mu -> eta (); double azimuthal_angle = mu -> phi (); double mass = mu -> mass (); // electric charge double charge = mu -> charge (); // direct 4-vector access math :: XYZLorentzVector four_momentum = mu -> p4 (); // some additional methods double energy = mu -> energy (); double transverse_mass = mu -> mt (); double transverse_energy = mu -> et (); double polar_angle = mu -> theta (); double rapidity = mu -> y (); // or mu->rapidity() double x_momentum = mu -> px (); // similar for y, z. } These and other basic kinematic methods are define here in CMSSW .","title":"4-vector access functions"},{"location":"analysis/selection/objects/tools/#track-access-functions","text":"Many objects are also connected to tracks from the CMS tracking detectors. Information from tracks provides other kinematic quantities that are common to multiple types of objects. From a muon object, we can access the associated track while looping over muons via the globalTrack method: auto trk = mu -> globalTrack (); // muon track Often, the most pertinent information about an object (such as a muon) to access from its associated track is its impact parameter with respect to the primary interaction vertex. Since muons can also be tracked through the muon detectors, we first check if the track is well-defined, and then access impact parameters in the xy-plane ( dxy or d0 ) and along the beam axis ( dz ), as well as their respective uncertainties. if ( trk . isNonnull ()) { value_mu_dxy [ value_mu_n ] = trk -> dxy ( pv ); value_mu_dz [ value_mu_n ] = trk -> dz ( pv ); value_mu_dxyErr [ value_mu_n ] = trk -> d0Error (); value_mu_dzErr [ value_mu_n ] = trk -> dzError (); }","title":"Track access functions"},{"location":"analysis/selection/objects/tools/#challenge-electron-track-properties","text":"Access and store the electron's charge and track impact parameter values, following the examples set for muons. Electron tracks are found using the Gaussian-sum filter method, which influences the member function name to access the track: auto trk = it -> gsfTrack (); // electron track","title":"Challenge: electron track properties"},{"location":"analysis/selection/objects/tools/#solution","text":"Again, add information in three places: Declarations: int value_el_charge [ max_el ]; float value_el_dxy [ max_el ]; float value_el_dxyErr [ max_el ]; float value_el_dz [ max_el ]; float value_el_dzErr [ max_el ]; tree -> Branch ( \"Electron_charge\" , value_el_charge , \"Electron_charge[nElectron]/I\" ); tree -> Branch ( \"Electron_dxy\" , value_el_dxy , \"Electron_dxy[nElectron]/F\" ); tree -> Branch ( \"Electron_dxyErr\" , value_el_dxyErr , \"Electron_dxyErr[nElectron]/F\" ); tree -> Branch ( \"Electron_dz\" , value_el_dz , \"Electron_dz[nElectron]/F\" ); tree -> Branch ( \"Electron_dzErr\" , value_el_dzErr , \"Electron_dzErr[nElectron]/F\" ); And access values in the electron loop . The format is identical to the muon loop ! value_el_charge [ value_el_n ] = it -> charge (); auto trk = it -> gsfTrack (); value_el_dxy [ value_el_n ] = trk -> dxy ( pv ); value_el_dz [ value_el_n ] = trk -> dz ( pv ); value_el_dxyErr [ value_el_n ] = trk -> d0Error (); value_el_dzErr [ value_el_n ] = trk -> dzError ();","title":"Solution:"},{"location":"analysis/selection/objects/tools/#matching-to-generated-particles","text":"Simulated files also contain information about the generator-level particles that were propagated into the showering and detector simulations. Physics objects can be matched to these generated particles spatially. The AOD2NanoAOD tool sets up several utility functions for matching: findBestMatch , findBestVisibleMatch , and subtractInvisible . The findBestMatch function takes generated particles (with an automated type T ) and the 4-vector of a physics object. It uses angular separation to find the closest generated particle to the reconstructed particle: template < typename T > int findBestMatch ( T & gens , reco :: Candidate :: LorentzVector & p4 ) { # initial definition of \"closest\" is really bad float minDeltaR = 999.0 ; int idx = -1 ; # loop over the generated particles for ( auto g = gens . begin (); g != gens . end (); g ++ ) { const auto tmp = deltaR ( g -> p4 (), p4 ); # if it's closer, overwrite the definition of \"closest\" if ( tmp < minDeltaR ) { minDeltaR = tmp ; idx = g - gens . begin (); } } return idx ; # return the index of the match } The other utility functions are similar, but correct for generated particles that decay to neutrinos, which would affect the \"visible\" 4-vector. In the AOD2NanoAOD tool, muons are matched only to \"interesting\" generated particles, which are all the leptons and photons (PDG ID 11, 13, 15, 22). Their generator status must be 1, indicating a final-state particle after any radiation chain. if ( ! isData ){ value_gen_n = 0 ; for ( auto p = selectedMuons . begin (); p != selectedMuons . end (); p ++ ) { // get the muon's 4-vector auto p4 = p -> p4 (); // perform the matching with a utility function auto idx = findBestVisibleMatch ( interestingGenParticles , p4 ); // if a match was found, save the generated particle's information if ( idx != -1 ) { auto g = interestingGenParticles . begin () + idx ; // another example of common 4-vector access functions! value_gen_pt [ value_gen_n ] = g -> pt (); value_gen_eta [ value_gen_n ] = g -> eta (); value_gen_phi [ value_gen_n ] = g -> phi (); value_gen_mass [ value_gen_n ] = g -> mass (); // gen particles also have ID and status from the generator value_gen_pdgid [ value_gen_n ] = g -> pdgId (); value_gen_status [ value_gen_n ] = g -> status (); // save the index of the matched gen particle value_mu_genpartidx [ p - selectedMuons . begin ()] = value_gen_n ; value_gen_n ++ ; } } }","title":"Matching to generated particles"},{"location":"analysis/selection/objects/tools/#challenge-electron-matching","text":"Match selected electrons to the interesting generated particles. Compile your code and run over the simulation test file. Using the ROOT TBrowser, look at some histograms of the branches you've added to the tree throughout this episode. $ scram b $ cmsRun configs/simulation_cfg.py $ root -l output.root [0] TBrowser b","title":"Challenge: electron matching"},{"location":"analysis/selection/objects/tools/#solution_1","text":"The structure for this matching exercise is identical to the muon matching segment. Loop over selected electrons, use the findBestVisibleMatch function to match it to an \"interesting\" particle and then to a jet. >> // Match electrons with gen particles and jets >> for ( auto p = selectedElectrons . begin (); p != selectedElectrons . end (); p ++ ) { >> // Gen particle matching >> auto p4 = p -> p4 (); >> auto idx = findBestVisibleMatch ( interestingGenParticles , p4 ); >> if ( idx != -1 ) { >> auto g = interestingGenParticles . begin () + idx ; >> value_gen_pt [ value_gen_n ] = g -> pt (); >> value_gen_eta [ value_gen_n ] = g -> eta (); >> value_gen_phi [ value_gen_n ] = g -> phi (); >> value_gen_mass [ value_gen_n ] = g -> mass (); >> value_gen_pdgid [ value_gen_n ] = g -> pdgId (); >> value_gen_status [ value_gen_n ] = g -> status (); >> value_el_genpartidx [ p - selectedElectrons . begin ()] = value_gen_n ; >> value_gen_n ++ ; >> } >> >> // Jet matching >> value_el_jetidx [ p - selectedElectrons . begin ()] = findBestMatch ( selectedJets , p4 ); >> } Warning This page is under construction","title":"Solution"},{"location":"analysis/systematics/lumiuncertain/","text":"Luminosity Uncertainty \u00b6 Warning This page is under construction","title":"Luminosity Uncertainties"},{"location":"analysis/systematics/lumiuncertain/#luminosity-uncertainty","text":"Warning This page is under construction","title":"Luminosity Uncertainty"},{"location":"analysis/systematics/mcuncertain/","text":"MC Uncertainty \u00b6 Warning This page is under construction","title":"MC Uncertainty"},{"location":"analysis/systematics/mcuncertain/#mc-uncertainty","text":"Warning This page is under construction","title":"MC Uncertainty"},{"location":"analysis/systematics/objectsuncertain/","text":"Object Uncertainty \u00b6 Warning This page is under construction","title":"Object Uncertainty"},{"location":"analysis/systematics/objectsuncertain/#object-uncertainty","text":"Warning This page is under construction","title":"Object Uncertainty"},{"location":"analysis/systematics/pileupuncertain/","text":"Pileup Uncertainty \u00b6 Warning This page is under construction","title":"Pileup Uncertainty"},{"location":"analysis/systematics/pileupuncertain/#pileup-uncertainty","text":"Warning This page is under construction","title":"Pileup Uncertainty"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/","text":"B Tag Uncertainty \u00b6 Scale Factors \u00b6 In simulation, Efficiency for tagging b quarks as b jets: the number of \"real b jets\" (jets spatially matched to generator-level b hadrons) tagged as b jets divided by the number of real b jets. Efficiency for mis-tagging c or light quarks as b jets: real c/light jets tagged as b jets divided by real c/light jets. These values are typically computed as functions of the momentum or pseudorapidity of the jet. The \"real\" flavor of the jet is accessed most simply by creating pat::Jet objects instead of reco::Jet objects. Scale factors to increase or decrease the number of b-tagged jets in simulation can be applied in a number of ways, but typically involve weighting simulation events based on the efficiencies and scale factors relevant to each jet in the event. Scale factors for the CSV algorithm are available for Open Data and involve extracting functions from a comma-separated-values file. The main documentation for b tagging and scale factors can be found in the b tagging recommendation twiki . Applying Scale Factors \u00b6 Calculating Efficiencies \u00b6 The BTagging folder of PhysObjectExtractorTool ( POET ) is used for calculating the efficiency for tagging each flavor of jet as a b quark, as a function of the jet momentum with the file WeightAnalyzer.cc . The purpose of this file is to set up jet momentum histograms for numerators and denominators of efficiency histograms as defined above. The code loops through the jets, checks their flavor, checks their btagging discriminator to see if it passes tight, medium and or loose cut, and then fills the histograms according to that information. double disc = it -> bDiscriminator ( discriminatorStr ); int hadronFlavor = it -> partonFlavour (); if ( abs ( hadronFlavor ) == 5 ){ BEff_Dptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueT ) BEffTight_Nptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueM ) BEffMed_Nptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueL ) BEffLoose_Nptbins_b -> Fill ( pt , weight ); } else if ( abs ( hadronFlavor ) == 4 ){ ... These historgrams are then stored in an output file. Input, output, and other parameters can be changed in the config file . After this, you can save, exit, and compile, and then move onto the config file . You will put the file(s) which you wish to run efficiencies on here: ##### ------- This is a test file process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( ' root : //eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12_DR53X/TTbar_8TeV-Madspin_aMCatNLO-herwig/AODSIM/PU_S10_START53_V19-v2/00000/04FCA1D5-E74C-E311-92CE-002590A887F0.root')) Once this is complete, you can run the config file for your efficiencies. Run Complete \u00b6 Once your run is complete, in the 'BTagging' folder there should be a file called plotBeff.C . This file is set up to do the numerator and denomenator divisions (as defined earlier), show you a histogram of your efficiencies from those calculations, and write the same efficiencies that you calculated in a numerical form. To run this code open this file in root like such: root plotBeff . c The histogram and output should appear through root. An example of what the histogram should look like is this: If Needed: Updating Momentum Bin Code \u00b6 In WeightAnalyzer.cc , there is a spot to input custom jet momentum bins that looks like this: double ptbinsB [ 10 ] = { 0 , 15 , 30 , 50 , 70 , 100 , 150 , 200 , 500 , 1000 }; where a bin's momentums span from 0 to 15, 15 to 30, etc. After your jet momentum bin update, you need to update the actual code that produces the histogram. Continuing this example, there are a total of 9 momentum bins from the numbers given in, ptbinsB. In the histogram producing code, there is a 9 indicating the number of bins: BEff_Dptbins_b = fs -> make < TH1D > ( \"BEff_Dptbins_b \" , \"\" , 9 , ptbinsB ); BEff_Dptbins_b -> Sumw2 (); Where the number 9 is now, this number will need to be updated to your number of bins. Access Efficiencies \u00b6 Once you have your efficiencies, you can then put them in to the 3 look up functions that have been implemented in PatJetAnalyzer for storing efficiencies. Here, for example, is the b tag efficiencies function which returns efficiency given a jet momentum: double PatJetAnalyzer::getBtagEfficiency ( double pt ){ if ( pt < 25 ) return 0.263407 ; else if ( pt < 50 ) return 0.548796 ; else if ( pt < 75 ) return 0.656801 ; else if ( pt < 100 ) return 0.689167 ; else if ( pt < 125 ) return 0.697911 ; else if ( pt < 150 ) return 0.700187 ; else if ( pt < 200 ) return 0.679236 ; else if ( pt < 400 ) return 0.625296 ; else return 0.394916 ; } Access Scale Factors \u00b6 The data file provided by the CMS b tagging group contains the scale factor functions for all types of jets. Some important titles to give more context to are as follows: OperatingPoint - This is the light (0), medium (1), or tight (2) cut of the flavored jet. formula - This is the equation for calculating the scale factor, where x is the momentum of the jet. jetFlavor - b = 0, c = 1, udsg = 2. Sorting Columns and creating filters with the .csv file can make accessing and finding sepcific scale factor equations easier. For example, filtering the OperatingPoint column to only show the number 1 will give you only medium cut jet information. Other useful information about the .csv file can be found here . The scale factor equations from the folumla column have been implemented in POET! In PatJetAnalyzer there are 2 functions, one for b and c flavored jets and one for light flavored jets, that return the scale factor of the jet depending on the momentum of the jet. Below is the b and c tag function. double PatJetAnalyzer::getBorCtagSF ( double pt , double eta ){ if ( pt > 670. ) pt = 670 ; if ( fabs ( eta ) > 2.4 or pt < 20. ) return 1.0 ; return 0.92955 * (( 1. + ( 0.0589629 * pt )) / ( 1. + ( 0.0568063 * pt ))); } Look at this twiki for additional information about scale factors . Calculating Weights \u00b6 Once these functions are updated to their desired states, weight calculating can happen! The first thing to check for when event weight calculating is this: if (jet_btag.at(value_jet_n) > 0.679) . This check is to see whether or not the jet distminator makes the cut we want our jets to make. In this case, we want our jets to make the medium cut (.679). If a jet makes the cut, there are then a couple more checks to be made: if ( abs ( hadronFlavour ) == 5 ){ eff = getBtagEfficiency ( corrpt ); SF = getBorCtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n )); SFd = SF - uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n )); } else if ( abs ( hadronFlavour ) == 4 ){ eff = getCtagEfficiency ( corrpt ); SF = getBorCtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + ( 2 * uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n ))); SFd = SF - ( 2 * uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n ))); } else { eff = getLFtagEfficiency ( corrpt ); SF = getLFtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + ( uncertaintyForLFTagSF ( corrpt , jet_eta . at ( value_jet_n ))); SFd = SF - ( uncertaintyForLFTagSF ( corrpt , jet_eta . at ( value_jet_n ))); } This section first finds which flavor of jet it is (b = 5, c = 4, and light = anything else) and then gets the efficiency for the respected jet, as well as calculates its scale factor. It also calculates its up and down quarked scale factors of the jet. Once these checks and calculations are complete, the following calulations can occur: MC *= eff ; btagWeight *= SF * eff ; btagWeightUp *= SFu * eff ; btagWeightDn *= SFd * eff ; These calculations are the probability of a given configuration of jets in MC simulation ( MC ) and data ( btagWeight , btagWeightUp , and btagWeightDn ). The same process with a little bit different probability calculating is done if the jet did not meet the desired cut. Once these checks have finished for every jet you are looking at, a final calculation for the event weights is done. btagWeight = ( btagWeight / MC ); btagWeightUp = ( btagWeightUp / MC ); btagWeightDn = ( btagWeightDn / MC ); NOTE: There are many ways to go about calculating event weights. This link shows a couple of the different ways. In POET, method 1a is the method used. Uncertainties \u00b6 As we just saw in the \"Calculating Weights\" section above, there are uncertainties that need to be considered. These uncertainties are actually already taken into account in the .csv file. When looking at the scale factor equation, there should be a main equation followed by either an addition or subtraction of a number, which is the uncertainty. Uncertainties for Each Flavor \u00b6 In POET, there are 2 functions for the uncertainty, one for the b tag uncertainty and one for the light flavor tag uncertainty. The reason that there is not one specifically for c tagged jets is because c tagged jet's uncertainty is two times that of the b tagged jet's uncertainty, so you can simply multiply the b tag uncertainty call by two, as seen here: SFu = SF + (2 * uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n))); Here is what the b tag uncertainty function looks like, which returns the uncertainty given a jet momentum: double PatJetAnalyzer::uncertaintyForBTagSF ( double pt , double eta ){ if ( fabs ( eta ) > 2.4 or pt < 20. ) return 0 ; if ( pt < 30 ) return 0.0466655 ; else if ( pt < 40 ) return 0.0203547 ; else if ( pt < 50 ) return 0.0187707 ; else if ( pt < 60 ) return 0.0250719 ; else if ( pt < 70 ) return 0.023081 ; else if ( pt < 80 ) return 0.0183273 ; else if ( pt < 100 ) return 0.0256502 ; else if ( pt < 120 ) return 0.0189555 ; else if ( pt < 160 ) return 0.0236561 ; else if ( pt < 210 ) return 0.0307624 ; else if ( pt < 260 ) return 0.0387889 ; else if ( pt < 320 ) return 0.0443912 ; else if ( pt < 400 ) return 0.0693573 ; else if ( pt < 500 ) return 0.0650147 ; else return 0.066886 ; } Storing Final Weights \u00b6 Also from the \"Calculating Weights\" section, there are 3 final variables that are used to store the final event weights that were calculated: btagWeight , btagWeightUp , and btagWeightDn . When the file has completed running, you can run root with your output file and look up these 3 names to access the data calculated from your run. Here is an example of these variables accessed in root (Normal - Black, Up - Red, Down - Blue): Warning This page is under construction","title":"Tagging uncertainties"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#b-tag-uncertainty","text":"","title":"B Tag Uncertainty"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#scale-factors","text":"In simulation, Efficiency for tagging b quarks as b jets: the number of \"real b jets\" (jets spatially matched to generator-level b hadrons) tagged as b jets divided by the number of real b jets. Efficiency for mis-tagging c or light quarks as b jets: real c/light jets tagged as b jets divided by real c/light jets. These values are typically computed as functions of the momentum or pseudorapidity of the jet. The \"real\" flavor of the jet is accessed most simply by creating pat::Jet objects instead of reco::Jet objects. Scale factors to increase or decrease the number of b-tagged jets in simulation can be applied in a number of ways, but typically involve weighting simulation events based on the efficiencies and scale factors relevant to each jet in the event. Scale factors for the CSV algorithm are available for Open Data and involve extracting functions from a comma-separated-values file. The main documentation for b tagging and scale factors can be found in the b tagging recommendation twiki .","title":"Scale Factors"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#applying-scale-factors","text":"","title":"Applying Scale Factors"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#calculating-efficiencies","text":"The BTagging folder of PhysObjectExtractorTool ( POET ) is used for calculating the efficiency for tagging each flavor of jet as a b quark, as a function of the jet momentum with the file WeightAnalyzer.cc . The purpose of this file is to set up jet momentum histograms for numerators and denominators of efficiency histograms as defined above. The code loops through the jets, checks their flavor, checks their btagging discriminator to see if it passes tight, medium and or loose cut, and then fills the histograms according to that information. double disc = it -> bDiscriminator ( discriminatorStr ); int hadronFlavor = it -> partonFlavour (); if ( abs ( hadronFlavor ) == 5 ){ BEff_Dptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueT ) BEffTight_Nptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueM ) BEffMed_Nptbins_b -> Fill ( pt , weight ); if ( disc >= discriminatorValueL ) BEffLoose_Nptbins_b -> Fill ( pt , weight ); } else if ( abs ( hadronFlavor ) == 4 ){ ... These historgrams are then stored in an output file. Input, output, and other parameters can be changed in the config file . After this, you can save, exit, and compile, and then move onto the config file . You will put the file(s) which you wish to run efficiencies on here: ##### ------- This is a test file process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( ' root : //eospublic.cern.ch//eos/opendata/cms/MonteCarlo2012/Summer12_DR53X/TTbar_8TeV-Madspin_aMCatNLO-herwig/AODSIM/PU_S10_START53_V19-v2/00000/04FCA1D5-E74C-E311-92CE-002590A887F0.root')) Once this is complete, you can run the config file for your efficiencies.","title":"Calculating Efficiencies"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#run-complete","text":"Once your run is complete, in the 'BTagging' folder there should be a file called plotBeff.C . This file is set up to do the numerator and denomenator divisions (as defined earlier), show you a histogram of your efficiencies from those calculations, and write the same efficiencies that you calculated in a numerical form. To run this code open this file in root like such: root plotBeff . c The histogram and output should appear through root. An example of what the histogram should look like is this:","title":"Run Complete"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#if-needed-updating-momentum-bin-code","text":"In WeightAnalyzer.cc , there is a spot to input custom jet momentum bins that looks like this: double ptbinsB [ 10 ] = { 0 , 15 , 30 , 50 , 70 , 100 , 150 , 200 , 500 , 1000 }; where a bin's momentums span from 0 to 15, 15 to 30, etc. After your jet momentum bin update, you need to update the actual code that produces the histogram. Continuing this example, there are a total of 9 momentum bins from the numbers given in, ptbinsB. In the histogram producing code, there is a 9 indicating the number of bins: BEff_Dptbins_b = fs -> make < TH1D > ( \"BEff_Dptbins_b \" , \"\" , 9 , ptbinsB ); BEff_Dptbins_b -> Sumw2 (); Where the number 9 is now, this number will need to be updated to your number of bins.","title":"If Needed: Updating Momentum Bin Code"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#access-efficiencies","text":"Once you have your efficiencies, you can then put them in to the 3 look up functions that have been implemented in PatJetAnalyzer for storing efficiencies. Here, for example, is the b tag efficiencies function which returns efficiency given a jet momentum: double PatJetAnalyzer::getBtagEfficiency ( double pt ){ if ( pt < 25 ) return 0.263407 ; else if ( pt < 50 ) return 0.548796 ; else if ( pt < 75 ) return 0.656801 ; else if ( pt < 100 ) return 0.689167 ; else if ( pt < 125 ) return 0.697911 ; else if ( pt < 150 ) return 0.700187 ; else if ( pt < 200 ) return 0.679236 ; else if ( pt < 400 ) return 0.625296 ; else return 0.394916 ; }","title":"Access Efficiencies"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#access-scale-factors","text":"The data file provided by the CMS b tagging group contains the scale factor functions for all types of jets. Some important titles to give more context to are as follows: OperatingPoint - This is the light (0), medium (1), or tight (2) cut of the flavored jet. formula - This is the equation for calculating the scale factor, where x is the momentum of the jet. jetFlavor - b = 0, c = 1, udsg = 2. Sorting Columns and creating filters with the .csv file can make accessing and finding sepcific scale factor equations easier. For example, filtering the OperatingPoint column to only show the number 1 will give you only medium cut jet information. Other useful information about the .csv file can be found here . The scale factor equations from the folumla column have been implemented in POET! In PatJetAnalyzer there are 2 functions, one for b and c flavored jets and one for light flavored jets, that return the scale factor of the jet depending on the momentum of the jet. Below is the b and c tag function. double PatJetAnalyzer::getBorCtagSF ( double pt , double eta ){ if ( pt > 670. ) pt = 670 ; if ( fabs ( eta ) > 2.4 or pt < 20. ) return 1.0 ; return 0.92955 * (( 1. + ( 0.0589629 * pt )) / ( 1. + ( 0.0568063 * pt ))); } Look at this twiki for additional information about scale factors .","title":"Access Scale Factors"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#calculating-weights","text":"Once these functions are updated to their desired states, weight calculating can happen! The first thing to check for when event weight calculating is this: if (jet_btag.at(value_jet_n) > 0.679) . This check is to see whether or not the jet distminator makes the cut we want our jets to make. In this case, we want our jets to make the medium cut (.679). If a jet makes the cut, there are then a couple more checks to be made: if ( abs ( hadronFlavour ) == 5 ){ eff = getBtagEfficiency ( corrpt ); SF = getBorCtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n )); SFd = SF - uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n )); } else if ( abs ( hadronFlavour ) == 4 ){ eff = getCtagEfficiency ( corrpt ); SF = getBorCtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + ( 2 * uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n ))); SFd = SF - ( 2 * uncertaintyForBTagSF ( corrpt , jet_eta . at ( value_jet_n ))); } else { eff = getLFtagEfficiency ( corrpt ); SF = getLFtagSF ( corrpt , jet_eta . at ( value_jet_n )); SFu = SF + ( uncertaintyForLFTagSF ( corrpt , jet_eta . at ( value_jet_n ))); SFd = SF - ( uncertaintyForLFTagSF ( corrpt , jet_eta . at ( value_jet_n ))); } This section first finds which flavor of jet it is (b = 5, c = 4, and light = anything else) and then gets the efficiency for the respected jet, as well as calculates its scale factor. It also calculates its up and down quarked scale factors of the jet. Once these checks and calculations are complete, the following calulations can occur: MC *= eff ; btagWeight *= SF * eff ; btagWeightUp *= SFu * eff ; btagWeightDn *= SFd * eff ; These calculations are the probability of a given configuration of jets in MC simulation ( MC ) and data ( btagWeight , btagWeightUp , and btagWeightDn ). The same process with a little bit different probability calculating is done if the jet did not meet the desired cut. Once these checks have finished for every jet you are looking at, a final calculation for the event weights is done. btagWeight = ( btagWeight / MC ); btagWeightUp = ( btagWeightUp / MC ); btagWeightDn = ( btagWeightDn / MC ); NOTE: There are many ways to go about calculating event weights. This link shows a couple of the different ways. In POET, method 1a is the method used.","title":"Calculating Weights"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#uncertainties","text":"As we just saw in the \"Calculating Weights\" section above, there are uncertainties that need to be considered. These uncertainties are actually already taken into account in the .csv file. When looking at the scale factor equation, there should be a main equation followed by either an addition or subtraction of a number, which is the uncertainty.","title":"Uncertainties"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#uncertainties-for-each-flavor","text":"In POET, there are 2 functions for the uncertainty, one for the b tag uncertainty and one for the light flavor tag uncertainty. The reason that there is not one specifically for c tagged jets is because c tagged jet's uncertainty is two times that of the b tagged jet's uncertainty, so you can simply multiply the b tag uncertainty call by two, as seen here: SFu = SF + (2 * uncertaintyForBTagSF(corrpt, jet_eta.at(value_jet_n))); Here is what the b tag uncertainty function looks like, which returns the uncertainty given a jet momentum: double PatJetAnalyzer::uncertaintyForBTagSF ( double pt , double eta ){ if ( fabs ( eta ) > 2.4 or pt < 20. ) return 0 ; if ( pt < 30 ) return 0.0466655 ; else if ( pt < 40 ) return 0.0203547 ; else if ( pt < 50 ) return 0.0187707 ; else if ( pt < 60 ) return 0.0250719 ; else if ( pt < 70 ) return 0.023081 ; else if ( pt < 80 ) return 0.0183273 ; else if ( pt < 100 ) return 0.0256502 ; else if ( pt < 120 ) return 0.0189555 ; else if ( pt < 160 ) return 0.0236561 ; else if ( pt < 210 ) return 0.0307624 ; else if ( pt < 260 ) return 0.0387889 ; else if ( pt < 320 ) return 0.0443912 ; else if ( pt < 400 ) return 0.0693573 ; else if ( pt < 500 ) return 0.0650147 ; else return 0.066886 ; }","title":"Uncertainties for Each Flavor"},{"location":"analysis/systematics/objectsuncertain/btaguncertain/#storing-final-weights","text":"Also from the \"Calculating Weights\" section, there are 3 final variables that are used to store the final event weights that were calculated: btagWeight , btagWeightUp , and btagWeightDn . When the file has completed running, you can run root with your output file and look up these 3 names to access the data calculated from your run. Here is an example of these variables accessed in root (Normal - Black, Up - Red, Down - Blue): Warning This page is under construction","title":"Storing Final Weights"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/","text":"Jet Uncertainty \u00b6 Unsurprisingly, the CMS detector does not measure jet energies perfectly, nor do simulation and data agree perfectly! The measured energy of jet must be corrected so that it can be related to the true energy of its parent particle. These corrections account for several effects and are factorized so that each effect can be studied independently. Jet Energy Corrections (JEC) \u00b6 What is JEC? JEC is the first set of corrections applied on jets that adjust the mean of the response distribution in a series of correction levels. Correction Levels Particles from additional interactions in nearby bunch crossings of the LHC contribute energy in the calorimeters that must somehow be distinguished from the energy deposits of the main interaction. Extra energy in a jet's cone can make its measured momentum larger than the momentum of the parent particle. The first layer (\"L1\") of jet energy corrections accounts for pileup by subtracting the average transverse momentum contribution of the pileup interactions to the jet's cone area. This average pileup contribution varies by pseudorapidity and, of course, by the number of interactions in the event. The second and third layers of corrections (\"L2L3\") correct the measured momentum to the true momentum as functions of momentum and pseudorapidity, bringing the reconstructed jet in line with the generated jet. These corrections are derived using momentum balancing and missing energy techniques in dijet and Z boson events. One well-measured object (ex: a jet near the center of the detector, a Z boson reconstructed from leptons) is balanced against a jet for which corrections are derived. All of these corrections are applied to both data and simulation. Data events are then given \"residual\" corrections to bring data into line with the corrected simulation. A final set of flavor-based corrections are used in certain analyses that are especially sensitive to flavor effects. All of the corrections are described in this paper . The figure below shows the result of the L1+L2+L3 corrections on the jet response. Implementing JEC in CMS Software \u00b6 JEC From Text Files There are several methods available for applying jet energy corrections to reconstructed jets. We have demonstrated a method to read in the corrections from text files and extract the corrections manually for each jet. In order to produce these text files, we have to run jec_cfg.py . isData = False #if len(sys.argv) > 1: isData = bool(eval(sys.argv[1])) #print 'Writing JEC text files. isData = ',isData # CMS process initialization process = cms . Process ( 'jecprocess' ) process . load ( 'Configuration.StandardSequences.Services_cff' ) process . load ( 'Configuration.StandardSequences.FrontierConditions_GlobalTag_cff' ) # connect to global tag if isData : # process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/FT53_V21A_AN6_FULL.db') process . GlobalTag . globaltag = 'FT53_V21A_AN6::All' else : # process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/START53_V27.db') process . GlobalTag . globaltag = 'START53_V27::All' # setup JetCorrectorDBReader process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 1 )) process . source = cms . Source ( 'EmptySource' ) process . ak5 = cms . EDAnalyzer ( 'JetCorrectorDBReader' , payloadName = cms . untracked . string ( 'AK5PF' ), printScreen = cms . untracked . bool ( False ), createTextFile = cms . untracked . bool ( True )) if isData : process . ak5 . globalTag = cms . untracked . string ( 'FT53_V21A_AN6' ) else : process . ak5 . globalTag = cms . untracked . string ( 'START53_V27' ) process . p = cms . Path ( process . ak5 ) Note that this analyzer will need to be run with both isData = True and isData = False to produce text files for both. $ cd JEC $ cmsRun jec_cfg.py $ #edit the file and flip isData $ cmsRun jec_cfg.py Applying JEC Correction \u00b6 JEC begins in poet_cfg.py , where we apply jet energy corrections and Type-1 MET corrections on PAT jets, which are a popular object format in CMS that stands for \"Physics Analysis Toolkit\". To do this we will load the global tag and databases directly in the configuration file and use the \u2018addJetCollection\u2019 process to create a collection of pat::jets. Note: The JEC Uncertainty text file is needed for the manually created correction uncertainties created inside of the analyzer. Uncertainty will be covered later. if doPat : ... # Choose which jet correction levels to apply jetcorrlabels = [ 'L1FastJet' , 'L2Relative' , 'L3Absolute' ] if isData : # For data we need to remove generator-level matching processes runOnData ( process , [ 'Jets' , 'METs' ], \"\" , None , []) jetcorrlabels . append ( 'L2L3Residual' ) # Set up the new jet collection process . ak5PFJets . doAreaFastjet = True addPfMET ( process , 'PF' ) addJetCollection ( process , cms . InputTag ( 'ak5PFJets' ), 'AK5' , 'PFCorr' , doJTA = True , doBTagging = True , jetCorrLabel = ( 'AK5PF' , cms . vstring ( jetcorrlabels )), doType1MET = True , doL1Cleaning = True , doL1Counters = False , doJetID = True , jetIdLabel = \"ak5\" , ) process . myjets = cms . EDAnalyzer ( 'PatJetAnalyzer' , InputCollection = cms . InputTag ( \"selectedPatJetsAK5PFCorr\" ), isData = cms . bool ( isData ), jecUncName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/' + JecString + 'Uncertainty_AK5PF.txt' ), jerResName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt' ) ) ... Now we can go into PatJetAnalyzer.cc , where in the jet loop of analyzeJets , the correction has already automatically been corrected for each jet. We then save a uncorrected version of the jet as uncorrJet . for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ pat :: Jet uncorrJet = itjet -> correctedJet ( 0 ); ... Jet Energy Resolution (JER) \u00b6 What is JER? Jet Energy Resolution (JER) corrections are applied after JEC on strictly MC simulations. Unlike JEC, which adjusts for the mean of the response distribution, JER adjusts the width of the distribution. This is because MC simulations tend to be more sharply peaked and less broad than the same distribution in data, therefore we have to increase the resolution based on the effects of pileup, jet size and jet flavor. Accesing JER in CMS Software Unlike JEC, the majority of JER is done inside of PatJetAnalyzer.cc , but we do have to import the file path to the text file containing a jet resolution factor table from the JEC directory in poet_cfg.py . process . myjets = cms . EDAnalyzer ( 'PatJetAnalyzer' , ... jerResName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt' ) ) Back inside the jet loop, we define ptscale , the eventual scale factor multiplied onto the jet momentum. Note: As mentioned previously, if we are running PatJetAnalyzer.cc on data, we do not want to affect to the resolution, so we initialize it as 1 . Next we calculate ptscale using one of two methods: A stochastic smearing method, which is used on generator-level jets ( genJet ), described by equation 4.11 in this dissertation . A hybrid smearing method, which is used otherwise, described in section 8 of the 2017 CMS jet algorithm paper , which also includes more information about JEC in general. Note: Also mentioned previously was the fact that JER is applied after JEC, meaning the pT that is used various times in the evaluations (e.g PTNPU.push_back( itjet->pt() ); ) is the JEC corrected momentum, rather than the uncorrected one. void JetAnalyzer::analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { ... for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ ... ptscale = 1 ; res = 1 ; if ( ! isData ) { std :: vector < float > factors = factorLookup ( fabs ( itjet -> eta ())); // returns in order {factor, factor_down, factor_up} std :: vector < float > feta ; std :: vector < float > PTNPU ; feta . push_back ( fabs ( itjet -> eta ()) ); PTNPU . push_back ( itjet -> pt () ); PTNPU . push_back ( vertices -> size () ); res = jer_ -> correction ( feta , PTNPU ); float pt = itjet -> pt (); const reco :: GenJet * genJet = itjet -> genJet (); bool smeared = false ; if ( genJet ){ double deltaPt = fabs ( genJet -> pt () - pt ); double deltaR = reco :: deltaR ( genJet -> p4 (), itjet -> p4 ()); if (( deltaR < 0.2 ) && deltaPt <= 3 * pt * res ){ double gen_pt = genJet -> pt (); double reco_pt = pt ; double deltapt = ( reco_pt - gen_pt ) * factors [ 0 ]; double deltapt_down = ( reco_pt - gen_pt ) * factors [ 1 ]; double deltapt_up = ( reco_pt - gen_pt ) * factors [ 2 ]; ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); ... smeared = true ; } } if ( ! smeared && factors [ 0 ] > 0 ) { TRandom3 JERrand ; JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 0 ] * ( factors [ 0 ] + 2 )) * res * pt ) / pt ); ... } Jet Correction Uncertainty \u00b6 An important factor we have to keep in mind when applying both JEC and JER are the statistical uncertainities. These uncertainties have several sources, shown in the figure below. The L1 (pileup) uncertainty dominates at low momentum, while the L3 (absolute scale) uncertainty takes over for higher momentum jets. All corrections are quite precise for jets located near the center of the CMS barrel region, and the precision drops as pseudorapidity increases and different subdetectors lose coverage. These uncertainties are accounted for by including an \"up\" and \"down\" version of our correction factor. JEC Uncertainty While the JEC corrected momentum can be accessed automatically through the jet object (e.g. itjet->pt() ), the \"up\" and \"down\" versions must be calculated manually. Here in the jet loop, the corrUp and corrDown variables are created in part using jetUnc_->getUncertainty() (This object is created from the a text file which was briefly mentioned during the JEC initialization in poet_cfg.py of the Implementing JEC in CMS Software section). In order to access the getUncertainty function, we use a JEC uncertainty object, in this case called jecUnc_ , where we input information about the jet, like its psuedorapidity and momentum. for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ ... double corrUp = 1.0 ; double corrDown = 1.0 ; jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrUp = ( 1 + fabs ( jecUnc_ -> getUncertainty ( 1 ))); jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrDown = ( 1 - fabs ( jecUnc_ -> getUncertainty ( -1 ))); ... JER Uncertainty Just how ptscale was manually calculated on genJets using this line: ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); We calculate the JER uncertainty like so: ptscale_up = max ( 0.0 , ( reco_pt + deltapt_up ) / reco_pt ); ptscale_down = max ( 0.0 , ( reco_pt + deltapt_down ) / reco_pt ); Otherwise for non-genJets, JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_down = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 1 ] * ( factors [ 1 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_up = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 2 ] * ( factors [ 2 ] + 2 )) * res * pt ) / pt ); Storing the corrections \u00b6 The final step in actualizing the jet corrections occurs after the JEC/JER calculations, where we fill the five momentum vectors for each jet. corr_jet_pt is the JEC + JER corrected pT corr_jet_ptUp and corr_jet_ptDown are the (\"up\" and \"down\" versions of the JEC) + JER corrected pT corr_jet_ptSmearUp and corr_jet_ptSmearDown are the JEC + (smeared \"up\" and \"down\" versions of the JER) corrected pT corr_jet_pt . push_back ( ptscale * itjet -> pt ()); corr_jet_ptUp . push_back ( ptscale * corrUp * itjet -> pt ()); corr_jet_ptDown . push_back ( ptscale * corrDown * itjet -> pt ()); corr_jet_ptSmearUp . push_back ( ptscale_up * itjet -> pt ()); corr_jet_ptSmearDown . push_back ( ptscale_down * itjet -> pt ()); Putting it all together \u00b6 Inside of the dropdown is the full jet loop, comprised of the storing of the uncorrected jet object, creation of JEC uncertainty, JER corrections + uncertainty, and storing of the corrected momentum. Full Jet Loop for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ pat :: Jet uncorrJet = itjet -> correctedJet ( 0 ); double corrUp = 1.0 ; double corrDown = 1.0 ; jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrUp = ( 1 + fabs ( jecUnc_ -> getUncertainty ( 1 ))); jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrDown = ( 1 - fabs ( jecUnc_ -> getUncertainty ( -1 ))); ptscale = 1 ; ptscale_down = 1 ; ptscale_up = 1 ; res = 1 ; if ( ! isData ) { std :: vector < float > factors = factorLookup ( fabs ( itjet -> eta ())); // returns in order {factor, factor_down, factor_up} std :: vector < float > feta ; std :: vector < float > PTNPU ; feta . push_back ( fabs ( itjet -> eta ()) ); PTNPU . push_back ( itjet -> pt () ); PTNPU . push_back ( vertices -> size () ); res = jer_ -> correction ( feta , PTNPU ); float pt = itjet -> pt (); const reco :: GenJet * genJet = itjet -> genJet (); bool smeared = false ; if ( genJet ){ double deltaPt = fabs ( genJet -> pt () - pt ); double deltaR = reco :: deltaR ( genJet -> p4 (), itjet -> p4 ()); if (( deltaR < 0.2 ) && deltaPt <= 3 * pt * res ){ double gen_pt = genJet -> pt (); double reco_pt = pt ; double deltapt = ( reco_pt - gen_pt ) * factors [ 0 ]; double deltapt_down = ( reco_pt - gen_pt ) * factors [ 1 ]; double deltapt_up = ( reco_pt - gen_pt ) * factors [ 2 ]; ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); ptscale_up = max ( 0.0 , ( reco_pt + deltapt_up ) / reco_pt ); ptscale_down = max ( 0.0 , ( reco_pt + deltapt_down ) / reco_pt ); smeared = true ; } } if ( ! smeared && factors [ 0 ] > 0 ) { TRandom3 JERrand ; JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 0 ] * ( factors [ 0 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_down = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 1 ] * ( factors [ 1 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_up = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 2 ] * ( factors [ 2 ] + 2 )) * res * pt ) / pt ); } } if ( ptscale * itjet -> pt () <= min_pt ) continue ; jet_pt . push_back ( uncorrJet . pt ()); jet_eta . push_back ( itjet -> eta ()); jet_phi . push_back ( itjet -> phi ()); jet_ch . push_back ( itjet -> charge ()); jet_mass . push_back ( uncorrJet . mass ()); jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); corr_jet_pt . push_back ( ptscale * itjet -> pt ()); corr_jet_ptUp . push_back ( ptscale * corrUp * itjet -> pt ()); corr_jet_ptDown . push_back ( ptscale * corrDown * itjet -> pt ()); corr_jet_ptSmearUp . push_back ( ptscale_up * itjet -> pt ()); corr_jet_ptSmearDown . push_back ( ptscale_down * itjet -> pt ()); corr_jet_mass . push_back ( itjet -> mass ()); corr_jet_e . push_back ( itjet -> energy ()); corr_jet_px . push_back ( itjet -> px ()); corr_jet_py . push_back ( itjet -> py ()); corr_jet_pz . push_back ( itjet -> pz ()); ... }","title":"Jet/MET uncertainties"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-uncertainty","text":"Unsurprisingly, the CMS detector does not measure jet energies perfectly, nor do simulation and data agree perfectly! The measured energy of jet must be corrected so that it can be related to the true energy of its parent particle. These corrections account for several effects and are factorized so that each effect can be studied independently.","title":"Jet Uncertainty"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-energy-corrections-jec","text":"What is JEC? JEC is the first set of corrections applied on jets that adjust the mean of the response distribution in a series of correction levels. Correction Levels Particles from additional interactions in nearby bunch crossings of the LHC contribute energy in the calorimeters that must somehow be distinguished from the energy deposits of the main interaction. Extra energy in a jet's cone can make its measured momentum larger than the momentum of the parent particle. The first layer (\"L1\") of jet energy corrections accounts for pileup by subtracting the average transverse momentum contribution of the pileup interactions to the jet's cone area. This average pileup contribution varies by pseudorapidity and, of course, by the number of interactions in the event. The second and third layers of corrections (\"L2L3\") correct the measured momentum to the true momentum as functions of momentum and pseudorapidity, bringing the reconstructed jet in line with the generated jet. These corrections are derived using momentum balancing and missing energy techniques in dijet and Z boson events. One well-measured object (ex: a jet near the center of the detector, a Z boson reconstructed from leptons) is balanced against a jet for which corrections are derived. All of these corrections are applied to both data and simulation. Data events are then given \"residual\" corrections to bring data into line with the corrected simulation. A final set of flavor-based corrections are used in certain analyses that are especially sensitive to flavor effects. All of the corrections are described in this paper . The figure below shows the result of the L1+L2+L3 corrections on the jet response.","title":"Jet Energy Corrections (JEC)"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#implementing-jec-in-cms-software","text":"JEC From Text Files There are several methods available for applying jet energy corrections to reconstructed jets. We have demonstrated a method to read in the corrections from text files and extract the corrections manually for each jet. In order to produce these text files, we have to run jec_cfg.py . isData = False #if len(sys.argv) > 1: isData = bool(eval(sys.argv[1])) #print 'Writing JEC text files. isData = ',isData # CMS process initialization process = cms . Process ( 'jecprocess' ) process . load ( 'Configuration.StandardSequences.Services_cff' ) process . load ( 'Configuration.StandardSequences.FrontierConditions_GlobalTag_cff' ) # connect to global tag if isData : # process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/FT53_V21A_AN6_FULL.db') process . GlobalTag . globaltag = 'FT53_V21A_AN6::All' else : # process.GlobalTag.connect = cms.string('sqlite_file:/cvmfs/cms-opendata-conddb.cern.ch/START53_V27.db') process . GlobalTag . globaltag = 'START53_V27::All' # setup JetCorrectorDBReader process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 1 )) process . source = cms . Source ( 'EmptySource' ) process . ak5 = cms . EDAnalyzer ( 'JetCorrectorDBReader' , payloadName = cms . untracked . string ( 'AK5PF' ), printScreen = cms . untracked . bool ( False ), createTextFile = cms . untracked . bool ( True )) if isData : process . ak5 . globalTag = cms . untracked . string ( 'FT53_V21A_AN6' ) else : process . ak5 . globalTag = cms . untracked . string ( 'START53_V27' ) process . p = cms . Path ( process . ak5 ) Note that this analyzer will need to be run with both isData = True and isData = False to produce text files for both. $ cd JEC $ cmsRun jec_cfg.py $ #edit the file and flip isData $ cmsRun jec_cfg.py","title":"Implementing JEC in CMS Software"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#applying-jec-correction","text":"JEC begins in poet_cfg.py , where we apply jet energy corrections and Type-1 MET corrections on PAT jets, which are a popular object format in CMS that stands for \"Physics Analysis Toolkit\". To do this we will load the global tag and databases directly in the configuration file and use the \u2018addJetCollection\u2019 process to create a collection of pat::jets. Note: The JEC Uncertainty text file is needed for the manually created correction uncertainties created inside of the analyzer. Uncertainty will be covered later. if doPat : ... # Choose which jet correction levels to apply jetcorrlabels = [ 'L1FastJet' , 'L2Relative' , 'L3Absolute' ] if isData : # For data we need to remove generator-level matching processes runOnData ( process , [ 'Jets' , 'METs' ], \"\" , None , []) jetcorrlabels . append ( 'L2L3Residual' ) # Set up the new jet collection process . ak5PFJets . doAreaFastjet = True addPfMET ( process , 'PF' ) addJetCollection ( process , cms . InputTag ( 'ak5PFJets' ), 'AK5' , 'PFCorr' , doJTA = True , doBTagging = True , jetCorrLabel = ( 'AK5PF' , cms . vstring ( jetcorrlabels )), doType1MET = True , doL1Cleaning = True , doL1Counters = False , doJetID = True , jetIdLabel = \"ak5\" , ) process . myjets = cms . EDAnalyzer ( 'PatJetAnalyzer' , InputCollection = cms . InputTag ( \"selectedPatJetsAK5PFCorr\" ), isData = cms . bool ( isData ), jecUncName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/' + JecString + 'Uncertainty_AK5PF.txt' ), jerResName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt' ) ) ... Now we can go into PatJetAnalyzer.cc , where in the jet loop of analyzeJets , the correction has already automatically been corrected for each jet. We then save a uncorrected version of the jet as uncorrJet . for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ pat :: Jet uncorrJet = itjet -> correctedJet ( 0 ); ...","title":"Applying JEC Correction"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-energy-resolution-jer","text":"What is JER? Jet Energy Resolution (JER) corrections are applied after JEC on strictly MC simulations. Unlike JEC, which adjusts for the mean of the response distribution, JER adjusts the width of the distribution. This is because MC simulations tend to be more sharply peaked and less broad than the same distribution in data, therefore we have to increase the resolution based on the effects of pileup, jet size and jet flavor. Accesing JER in CMS Software Unlike JEC, the majority of JER is done inside of PatJetAnalyzer.cc , but we do have to import the file path to the text file containing a jet resolution factor table from the JEC directory in poet_cfg.py . process . myjets = cms . EDAnalyzer ( 'PatJetAnalyzer' , ... jerResName = cms . FileInPath ( 'PhysObjectExtractorTool/PhysObjectExtractor/JEC/JetResolutionInputAK5PF.txt' ) ) Back inside the jet loop, we define ptscale , the eventual scale factor multiplied onto the jet momentum. Note: As mentioned previously, if we are running PatJetAnalyzer.cc on data, we do not want to affect to the resolution, so we initialize it as 1 . Next we calculate ptscale using one of two methods: A stochastic smearing method, which is used on generator-level jets ( genJet ), described by equation 4.11 in this dissertation . A hybrid smearing method, which is used otherwise, described in section 8 of the 2017 CMS jet algorithm paper , which also includes more information about JEC in general. Note: Also mentioned previously was the fact that JER is applied after JEC, meaning the pT that is used various times in the evaluations (e.g PTNPU.push_back( itjet->pt() ); ) is the JEC corrected momentum, rather than the uncorrected one. void JetAnalyzer::analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { ... for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ ... ptscale = 1 ; res = 1 ; if ( ! isData ) { std :: vector < float > factors = factorLookup ( fabs ( itjet -> eta ())); // returns in order {factor, factor_down, factor_up} std :: vector < float > feta ; std :: vector < float > PTNPU ; feta . push_back ( fabs ( itjet -> eta ()) ); PTNPU . push_back ( itjet -> pt () ); PTNPU . push_back ( vertices -> size () ); res = jer_ -> correction ( feta , PTNPU ); float pt = itjet -> pt (); const reco :: GenJet * genJet = itjet -> genJet (); bool smeared = false ; if ( genJet ){ double deltaPt = fabs ( genJet -> pt () - pt ); double deltaR = reco :: deltaR ( genJet -> p4 (), itjet -> p4 ()); if (( deltaR < 0.2 ) && deltaPt <= 3 * pt * res ){ double gen_pt = genJet -> pt (); double reco_pt = pt ; double deltapt = ( reco_pt - gen_pt ) * factors [ 0 ]; double deltapt_down = ( reco_pt - gen_pt ) * factors [ 1 ]; double deltapt_up = ( reco_pt - gen_pt ) * factors [ 2 ]; ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); ... smeared = true ; } } if ( ! smeared && factors [ 0 ] > 0 ) { TRandom3 JERrand ; JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 0 ] * ( factors [ 0 ] + 2 )) * res * pt ) / pt ); ... }","title":"Jet Energy Resolution (JER)"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#jet-correction-uncertainty","text":"An important factor we have to keep in mind when applying both JEC and JER are the statistical uncertainities. These uncertainties have several sources, shown in the figure below. The L1 (pileup) uncertainty dominates at low momentum, while the L3 (absolute scale) uncertainty takes over for higher momentum jets. All corrections are quite precise for jets located near the center of the CMS barrel region, and the precision drops as pseudorapidity increases and different subdetectors lose coverage. These uncertainties are accounted for by including an \"up\" and \"down\" version of our correction factor. JEC Uncertainty While the JEC corrected momentum can be accessed automatically through the jet object (e.g. itjet->pt() ), the \"up\" and \"down\" versions must be calculated manually. Here in the jet loop, the corrUp and corrDown variables are created in part using jetUnc_->getUncertainty() (This object is created from the a text file which was briefly mentioned during the JEC initialization in poet_cfg.py of the Implementing JEC in CMS Software section). In order to access the getUncertainty function, we use a JEC uncertainty object, in this case called jecUnc_ , where we input information about the jet, like its psuedorapidity and momentum. for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ ... double corrUp = 1.0 ; double corrDown = 1.0 ; jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrUp = ( 1 + fabs ( jecUnc_ -> getUncertainty ( 1 ))); jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrDown = ( 1 - fabs ( jecUnc_ -> getUncertainty ( -1 ))); ... JER Uncertainty Just how ptscale was manually calculated on genJets using this line: ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); We calculate the JER uncertainty like so: ptscale_up = max ( 0.0 , ( reco_pt + deltapt_up ) / reco_pt ); ptscale_down = max ( 0.0 , ( reco_pt + deltapt_down ) / reco_pt ); Otherwise for non-genJets, JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_down = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 1 ] * ( factors [ 1 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_up = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 2 ] * ( factors [ 2 ] + 2 )) * res * pt ) / pt );","title":"Jet Correction Uncertainty"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#storing-the-corrections","text":"The final step in actualizing the jet corrections occurs after the JEC/JER calculations, where we fill the five momentum vectors for each jet. corr_jet_pt is the JEC + JER corrected pT corr_jet_ptUp and corr_jet_ptDown are the (\"up\" and \"down\" versions of the JEC) + JER corrected pT corr_jet_ptSmearUp and corr_jet_ptSmearDown are the JEC + (smeared \"up\" and \"down\" versions of the JER) corrected pT corr_jet_pt . push_back ( ptscale * itjet -> pt ()); corr_jet_ptUp . push_back ( ptscale * corrUp * itjet -> pt ()); corr_jet_ptDown . push_back ( ptscale * corrDown * itjet -> pt ()); corr_jet_ptSmearUp . push_back ( ptscale_up * itjet -> pt ()); corr_jet_ptSmearDown . push_back ( ptscale_down * itjet -> pt ());","title":"Storing the corrections"},{"location":"analysis/systematics/objectsuncertain/jetmetuncertain/#putting-it-all-together","text":"Inside of the dropdown is the full jet loop, comprised of the storing of the uncorrected jet object, creation of JEC uncertainty, JER corrections + uncertainty, and storing of the corrected momentum. Full Jet Loop for ( std :: vector < pat :: Jet >:: const_iterator itjet = myjets -> begin (); itjet != myjets -> end (); ++ itjet ){ pat :: Jet uncorrJet = itjet -> correctedJet ( 0 ); double corrUp = 1.0 ; double corrDown = 1.0 ; jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrUp = ( 1 + fabs ( jecUnc_ -> getUncertainty ( 1 ))); jecUnc_ -> setJetEta ( itjet -> eta () ); jecUnc_ -> setJetPt ( itjet -> pt () ); corrDown = ( 1 - fabs ( jecUnc_ -> getUncertainty ( -1 ))); ptscale = 1 ; ptscale_down = 1 ; ptscale_up = 1 ; res = 1 ; if ( ! isData ) { std :: vector < float > factors = factorLookup ( fabs ( itjet -> eta ())); // returns in order {factor, factor_down, factor_up} std :: vector < float > feta ; std :: vector < float > PTNPU ; feta . push_back ( fabs ( itjet -> eta ()) ); PTNPU . push_back ( itjet -> pt () ); PTNPU . push_back ( vertices -> size () ); res = jer_ -> correction ( feta , PTNPU ); float pt = itjet -> pt (); const reco :: GenJet * genJet = itjet -> genJet (); bool smeared = false ; if ( genJet ){ double deltaPt = fabs ( genJet -> pt () - pt ); double deltaR = reco :: deltaR ( genJet -> p4 (), itjet -> p4 ()); if (( deltaR < 0.2 ) && deltaPt <= 3 * pt * res ){ double gen_pt = genJet -> pt (); double reco_pt = pt ; double deltapt = ( reco_pt - gen_pt ) * factors [ 0 ]; double deltapt_down = ( reco_pt - gen_pt ) * factors [ 1 ]; double deltapt_up = ( reco_pt - gen_pt ) * factors [ 2 ]; ptscale = max ( 0.0 , ( reco_pt + deltapt ) / reco_pt ); ptscale_up = max ( 0.0 , ( reco_pt + deltapt_up ) / reco_pt ); ptscale_down = max ( 0.0 , ( reco_pt + deltapt_down ) / reco_pt ); smeared = true ; } } if ( ! smeared && factors [ 0 ] > 0 ) { TRandom3 JERrand ; JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 0 ] * ( factors [ 0 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_down = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 1 ] * ( factors [ 1 ] + 2 )) * res * pt ) / pt ); JERrand . SetSeed ( abs ( static_cast < int > ( itjet -> phi () * 1e4 ))); ptscale_up = max ( 0.0 , JERrand . Gaus ( pt , sqrt ( factors [ 2 ] * ( factors [ 2 ] + 2 )) * res * pt ) / pt ); } } if ( ptscale * itjet -> pt () <= min_pt ) continue ; jet_pt . push_back ( uncorrJet . pt ()); jet_eta . push_back ( itjet -> eta ()); jet_phi . push_back ( itjet -> phi ()); jet_ch . push_back ( itjet -> charge ()); jet_mass . push_back ( uncorrJet . mass ()); jet_btag . push_back ( itjet -> bDiscriminator ( \"combinedSecondaryVertexBJetTags\" )); corr_jet_pt . push_back ( ptscale * itjet -> pt ()); corr_jet_ptUp . push_back ( ptscale * corrUp * itjet -> pt ()); corr_jet_ptDown . push_back ( ptscale * corrDown * itjet -> pt ()); corr_jet_ptSmearUp . push_back ( ptscale_up * itjet -> pt ()); corr_jet_ptSmearDown . push_back ( ptscale_down * itjet -> pt ()); corr_jet_mass . push_back ( itjet -> mass ()); corr_jet_e . push_back ( itjet -> energy ()); corr_jet_px . push_back ( itjet -> px ()); corr_jet_py . push_back ( itjet -> py ()); corr_jet_pz . push_back ( itjet -> pz ()); ... }","title":"Putting it all together "},{"location":"analysis/systematics/objectsuncertain/leptonuncertain/","text":"Object Uncertainty \u00b6 Warning This page is under construction","title":"Lepton/Photon uncertainties"},{"location":"analysis/systematics/objectsuncertain/leptonuncertain/#object-uncertainty","text":"Warning This page is under construction","title":"Object Uncertainty"},{"location":"cmssw/cmsswanalyzers/","text":"Analyzers \u00b6 First, a few general words about analysis in the CMSSW framework. Physics analysis proceeds via a series of subsequent steps. Building blocks are identified and more complex objects are built on top of them. How to write a Framework Module and run the job with the cmsRun can be found here . When setting up code for the new EDM (such as creating a new EDProducer) there is a fair amount of 'boiler plate' code that you must write. To make writing such code easier CMS provides a series of scripts that will generate the necessary directory structure and files needed so that all you need to do is write your actual algorithms. CMSSW distiguishes the following module types : EDAnalyzer: takes input from the event and processes the input without writing information back to the event EDProducer: takes input from the event and produces new output which is saved in the event EDFilter: decides if processing the event can be stopped and continued EventSetup: external service not bound to the event structure which provides information useable by all modules (e.g. Geometry, Magnetic Field, etc.) In order to generate above modules: mkedanlzr : makes a skeleton of a package containing an EDAnalyzer mkedprod : makes a skeleton of a package containing an EDProducer mkedfltr : makes a skeleton of a package containing an EDFilter mkrecord : makes a complete implementation of a Record used by the EventSetup More generators are available and you can find them here Warning This page is under construction","title":"Analyzers"},{"location":"cmssw/cmsswanalyzers/#analyzers","text":"First, a few general words about analysis in the CMSSW framework. Physics analysis proceeds via a series of subsequent steps. Building blocks are identified and more complex objects are built on top of them. How to write a Framework Module and run the job with the cmsRun can be found here . When setting up code for the new EDM (such as creating a new EDProducer) there is a fair amount of 'boiler plate' code that you must write. To make writing such code easier CMS provides a series of scripts that will generate the necessary directory structure and files needed so that all you need to do is write your actual algorithms. CMSSW distiguishes the following module types : EDAnalyzer: takes input from the event and processes the input without writing information back to the event EDProducer: takes input from the event and produces new output which is saved in the event EDFilter: decides if processing the event can be stopped and continued EventSetup: external service not bound to the event structure which provides information useable by all modules (e.g. Geometry, Magnetic Field, etc.) In order to generate above modules: mkedanlzr : makes a skeleton of a package containing an EDAnalyzer mkedprod : makes a skeleton of a package containing an EDProducer mkedfltr : makes a skeleton of a package containing an EDFilter mkrecord : makes a complete implementation of a Record used by the EventSetup More generators are available and you can find them here Warning This page is under construction","title":"Analyzers"},{"location":"cmssw/cmsswconditions/","text":"Conditions \u00b6 This page explains the use of global tags and the condition database with the CMS Open Data. All information was taken from here . A Global Tag is a coherent collection of records of additional data needed by the reconstruction and analysis software. The Global Tag is defined for each data-taking period, separately for collision and simulated data. These records are stored in the condition database. Condition data include non-event-related information (Alignment, Calibration, Temperature, etc.) and parameters for the simulation/reconstruction/analysis software. For CMS Open Data, the condition data are provided as sqlite files in the /cvmfs/cms-opendata-conddb.cern.ch/ directory, which is accessible through the CMS Open Data VM. Most physics objects such as electrons , muons , photons in the CMS Open Data are already calibrated and ready-to-use, and no additional corrections are needed other than selection and identification criteria, which will be applied in the analysis code. Therefore, simple analyses do not need to access the condition database. For example you can check the Higgs analysis example . However, access to the condition database is necessary, for example, for jet energy corrections and trigger configuration information. Examples of such analyses are for the PAT object production or the top quark pair production . Note that when you need to access the condition database, the first time you run the job on the CMS Open Data VM, it will download the condition data from the /cvmfs area. It will take time (an example run of a 10 Mbps line took 45 mins), but it will only happen once as the files will be cached on your VM. The job will not produce any output during this time, but you can check the ongoing processes with the command 'top' and you can monitor the progress of reading the condition data to the local cache with the command 'df'. Collision data and Monte Carlo data sets can be found at http://opendata.cern.ch/docs/cms-guide-for-condition-database for years 2010, 2011 and 2012. Warning This page is under construction","title":"Conditions Data"},{"location":"cmssw/cmsswconditions/#conditions","text":"This page explains the use of global tags and the condition database with the CMS Open Data. All information was taken from here . A Global Tag is a coherent collection of records of additional data needed by the reconstruction and analysis software. The Global Tag is defined for each data-taking period, separately for collision and simulated data. These records are stored in the condition database. Condition data include non-event-related information (Alignment, Calibration, Temperature, etc.) and parameters for the simulation/reconstruction/analysis software. For CMS Open Data, the condition data are provided as sqlite files in the /cvmfs/cms-opendata-conddb.cern.ch/ directory, which is accessible through the CMS Open Data VM. Most physics objects such as electrons , muons , photons in the CMS Open Data are already calibrated and ready-to-use, and no additional corrections are needed other than selection and identification criteria, which will be applied in the analysis code. Therefore, simple analyses do not need to access the condition database. For example you can check the Higgs analysis example . However, access to the condition database is necessary, for example, for jet energy corrections and trigger configuration information. Examples of such analyses are for the PAT object production or the top quark pair production . Note that when you need to access the condition database, the first time you run the job on the CMS Open Data VM, it will download the condition data from the /cvmfs area. It will take time (an example run of a 10 Mbps line took 45 mins), but it will only happen once as the files will be cached on your VM. The job will not produce any output during this time, but you can check the ongoing processes with the command 'top' and you can monitor the progress of reading the condition data to the local cache with the command 'df'. Collision data and Monte Carlo data sets can be found at http://opendata.cern.ch/docs/cms-guide-for-condition-database for years 2010, 2011 and 2012. Warning This page is under construction","title":"Conditions"},{"location":"cmssw/cmsswconfigure/","text":"Configuration \u00b6 A configuration document, written using the Python language, is used to configure the cmsRun executable. A Python configuration program specifies which modules, inputs, outputs and services are to be loaded during execution, how to configure these modules and services, and in what order to execute them. All information can be found at twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideAboutPythonConfigFile .","title":"Configuration"},{"location":"cmssw/cmsswconfigure/#configuration","text":"A configuration document, written using the Python language, is used to configure the cmsRun executable. A Python configuration program specifies which modules, inputs, outputs and services are to be loaded during execution, how to configure these modules and services, and in what order to execute them. All information can be found at twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideAboutPythonConfigFile .","title":"Configuration"},{"location":"cmssw/cmsswdatamodel/","text":"Data Model \u00b6 The CMS Event Data Model (EDM) is centered around the concept of an Event . Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. In software terms, an Event starts as a collection of the RAW data from a detector or MC event, stored as a single entity in memory, a C++ type-safe container called edm::Event . An Event is a C++ object container for all RAW and reconstructed data related to a particular collision. During processing, data are passed from one module to the next via the Event, and are accessed only through the Event. All objects in the Event may be individually or collectively stored in ROOT files, and are thus directly browsable in ROOT. More and detailed information can be found here . The CMS Data Hierarchy \u00b6 CMS Data is arranged into a hierarchy of data tiers. Each physics event is written into each data tier, where the tiers each contain different levels of information about the event. The different tiers each have different uses. The three main data tiers written in CMS are: RAW: full event information from the Tier-0 (i.e. from CERN), containing 'raw' detector information (detector element hits, etc) RAW is not used directly for analysis RECO (\"RECOnstructed data\"): the output from first-pass processing by the Tier-0. This layer contains reconstructed physics objects, but it's still very detailed. RECO can be used for analysis, but is too big for frequent or heavy use when CMS has collected a substantial data sample. RECO Data Format Table AOD (\"Analysis Object Data\"): this is a \"distilled\" version of the RECO event information, and was used for most analyses on Run 1 data. AOD provides a trade-off between event size and complexity of the available information to optimize flexibility and speed for analyses. AOD Data Format Table MINIAOD: slimmer version of AOD, used for analyses on Run 2 data. MINIAOD is approximately one tenth of the size of AOD. The reduction is obtained defining light-weight physics-object candidate representations, increasing transverse momentum thresholds for storing physics-object candidates, and reduced numerical precision when it is not required at the analysis level. MINIAOD physics objects table The data tiers are described in more detail in a dedicated WorkBook chapter on Data Formats and Tiers .","title":"Data Model"},{"location":"cmssw/cmsswdatamodel/#data-model","text":"The CMS Event Data Model (EDM) is centered around the concept of an Event . Physically, an event is the result of a single readout of the detector electronics and the signals that will (in general) have been generated by particles, tracks, energy deposits, present in a number of bunch crossings. In software terms, an Event starts as a collection of the RAW data from a detector or MC event, stored as a single entity in memory, a C++ type-safe container called edm::Event . An Event is a C++ object container for all RAW and reconstructed data related to a particular collision. During processing, data are passed from one module to the next via the Event, and are accessed only through the Event. All objects in the Event may be individually or collectively stored in ROOT files, and are thus directly browsable in ROOT. More and detailed information can be found here .","title":"Data Model"},{"location":"cmssw/cmsswdatamodel/#the-cms-data-hierarchy","text":"CMS Data is arranged into a hierarchy of data tiers. Each physics event is written into each data tier, where the tiers each contain different levels of information about the event. The different tiers each have different uses. The three main data tiers written in CMS are: RAW: full event information from the Tier-0 (i.e. from CERN), containing 'raw' detector information (detector element hits, etc) RAW is not used directly for analysis RECO (\"RECOnstructed data\"): the output from first-pass processing by the Tier-0. This layer contains reconstructed physics objects, but it's still very detailed. RECO can be used for analysis, but is too big for frequent or heavy use when CMS has collected a substantial data sample. RECO Data Format Table AOD (\"Analysis Object Data\"): this is a \"distilled\" version of the RECO event information, and was used for most analyses on Run 1 data. AOD provides a trade-off between event size and complexity of the available information to optimize flexibility and speed for analyses. AOD Data Format Table MINIAOD: slimmer version of AOD, used for analyses on Run 2 data. MINIAOD is approximately one tenth of the size of AOD. The reduction is obtained defining light-weight physics-object candidate representations, increasing transverse momentum thresholds for storing physics-object candidates, and reduced numerical precision when it is not required at the analysis level. MINIAOD physics objects table The data tiers are described in more detail in a dedicated WorkBook chapter on Data Formats and Tiers .","title":"The CMS Data Hierarchy"},{"location":"cmssw/cmsswoverview/","text":"Overview \u00b6 The overall collection of software, referred to as CMS Software (CMSSW), is built around a Framework, an Event Data Model (EDM), and Services needed by the simulation, calibration and alignment, and reconstruction modules that process event data so that physicists can perform analysis. The primary goal of the Framework and EDM is to facilitate the development and deployment of reconstruction and analysis software. The CMSSW event processing model consists of one executable, called cmsRun , and many plug-in modules which are managed by the Framework. All the code needed in the event processing (calibration, reconstruction algorithms, etc.) is contained in the modules. The same executable is used for both detector and Monte Carlo data. More and detailed information can be found here .","title":"Overview"},{"location":"cmssw/cmsswoverview/#overview","text":"The overall collection of software, referred to as CMS Software (CMSSW), is built around a Framework, an Event Data Model (EDM), and Services needed by the simulation, calibration and alignment, and reconstruction modules that process event data so that physicists can perform analysis. The primary goal of the Framework and EDM is to facilitate the development and deployment of reconstruction and analysis software. The CMSSW event processing model consists of one executable, called cmsRun , and many plug-in modules which are managed by the Framework. All the code needed in the event processing (calibration, reconstruction algorithms, etc.) is contained in the modules. The same executable is used for both detector and Monte Carlo data. More and detailed information can be found here .","title":"Overview"},{"location":"tools/cernportal/","text":"The CERN Open Data Portal \u00b6 All CMS open data is available through the CERN Open Data portal . The portal hosts data from many experiments and offers search options, such as experiment, type or energy of collisions, type of data (from collisions or simulated), and many more. A brief description of the portal and those of each experiment are available from the \"About\" dropdown menu top right. The CERN Open Data portal contains the data records, environment, software and supplementary material to enable research-level use of open data. It also includes some basic documentation and topical guides. For CMS, this Open data guide complements the information available on the portal. The data records are accessed either using XRootD, which allows the data to be streamed, or through direct http download. A command-line tool cernopendata-client is also available for data download and inspection.","title":"CERN Open Data Portal"},{"location":"tools/cernportal/#the-cern-open-data-portal","text":"All CMS open data is available through the CERN Open Data portal . The portal hosts data from many experiments and offers search options, such as experiment, type or energy of collisions, type of data (from collisions or simulated), and many more. A brief description of the portal and those of each experiment are available from the \"About\" dropdown menu top right. The CERN Open Data portal contains the data records, environment, software and supplementary material to enable research-level use of open data. It also includes some basic documentation and topical guides. For CMS, this Open data guide complements the information available on the portal. The data records are accessed either using XRootD, which allows the data to be streamed, or through direct http download. A command-line tool cernopendata-client is also available for data download and inspection.","title":"The CERN Open Data Portal"},{"location":"tools/cmsopendata/","text":"CMS Open Data \u00b6 The CMS experiment at CERN has released research-quality data from particle collisions at the LHC since 2014. Almost all data from the first LHC run in 2010\u20132012 (\"Run1\") with the corresponding simulated samples are in the public domain, and several scientific studies have been performed using these data. First data from the second LHC run in 2015-2018 (\"Run2\") have been released in 2021. Open data are released after an embargo period of six years, which allows the collaboration to understand the detector performance and to exploit the scientific potential of these data. This is also necessary for the time needed to reprocess the data with the best available knowledge before the release. The first release of each year\u2019s data consists of 50% of the integrated luminosity recorded by the experiment, and the remaining data will be released within ten years, unless active analysis is still ongoing. However, the amount of open data will be limited to 20% of data with the similar centre-of-mass energy and collision type while such data are still planned to be taken. This approach allows for a fairly prompt release of the data after a major reprocessing once the reconstruction has been optimised, but still guarantees that the collaboration will have the opportunity to complete the planned studies with the complete dataset first. The open data releases are regulated in the CMS data preservation, re-use and open access policy . CERN open data portal includes a brief description about CMS open data and different tools available to analyze them. The main points are: the released data are as those used by the CMS collaboration, with all their complexicity some CMS-specific software is needed and available to get started with these data a computing environment compatible with the data and software needed for their analysis is provided. The experimental particle physics data are complex and studying them requires a solid understanding of the underlying physics, knowledge of different detector systems involved in data taking, and some mastering of the data handling. Some of these challenges have been addressed in this note , and this guide is part of the measures taken to improve the usability of CMS open data.","title":"CMS Open Data"},{"location":"tools/cmsopendata/#cms-open-data","text":"The CMS experiment at CERN has released research-quality data from particle collisions at the LHC since 2014. Almost all data from the first LHC run in 2010\u20132012 (\"Run1\") with the corresponding simulated samples are in the public domain, and several scientific studies have been performed using these data. First data from the second LHC run in 2015-2018 (\"Run2\") have been released in 2021. Open data are released after an embargo period of six years, which allows the collaboration to understand the detector performance and to exploit the scientific potential of these data. This is also necessary for the time needed to reprocess the data with the best available knowledge before the release. The first release of each year\u2019s data consists of 50% of the integrated luminosity recorded by the experiment, and the remaining data will be released within ten years, unless active analysis is still ongoing. However, the amount of open data will be limited to 20% of data with the similar centre-of-mass energy and collision type while such data are still planned to be taken. This approach allows for a fairly prompt release of the data after a major reprocessing once the reconstruction has been optimised, but still guarantees that the collaboration will have the opportunity to complete the planned studies with the complete dataset first. The open data releases are regulated in the CMS data preservation, re-use and open access policy . CERN open data portal includes a brief description about CMS open data and different tools available to analyze them. The main points are: the released data are as those used by the CMS collaboration, with all their complexicity some CMS-specific software is needed and available to get started with these data a computing environment compatible with the data and software needed for their analysis is provided. The experimental particle physics data are complex and studying them requires a solid understanding of the underlying physics, knowledge of different detector systems involved in data taking, and some mastering of the data handling. Some of these challenges have been addressed in this note , and this guide is part of the measures taken to improve the usability of CMS open data.","title":"CMS Open Data"},{"location":"tools/cmstwiki/","text":"The CMS Twiki \u00b6 Warning This page is under construction","title":"CMS Twiki"},{"location":"tools/cmstwiki/#the-cms-twiki","text":"Warning This page is under construction","title":"The CMS Twiki"},{"location":"tools/cppandpython/","text":"C++ and python \u00b6 As you saw in the ROOT section, CMS primarily uses C++ and python to analyze data. Here are some computing tutorials. C++ \u00b6 Basic Modern C++ cplusplus.com Python \u00b6 Programming with Python Plotting and Programming in Python","title":"C++ and Python"},{"location":"tools/cppandpython/#c-and-python","text":"As you saw in the ROOT section, CMS primarily uses C++ and python to analyze data. Here are some computing tutorials.","title":"C++ and python"},{"location":"tools/cppandpython/#c","text":"Basic Modern C++ cplusplus.com","title":"C++"},{"location":"tools/cppandpython/#python","text":"Programming with Python Plotting and Programming in Python","title":"Python"},{"location":"tools/docker/","text":"Docker \u00b6 Warning This page is under construction Docker is a commercial implementation of a container , a way to package up a snapshot of everything needed to run some particular version of software (OS, libraries, compilers, etc.). It is a very effective way of interfacing with the CMS open data as it gives you the proper environment you need to analyze these data. To learn more about Docker in general, from a HEP perspective, you may want to check out this Introduction to Docker , from Matthew Feickert. To account for the different running conditions in Run 1 vs Run 2, click the appropriate tab below for Run 1 vs Run 2 data. Run 1 Data Run 2 Data You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers You can also jump right in with the most recent tutorial on the CMS open data containers . You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers You can also jump right in with the most recent tutorial on the CMS open data containers .","title":"Docker"},{"location":"tools/docker/#docker","text":"Warning This page is under construction Docker is a commercial implementation of a container , a way to package up a snapshot of everything needed to run some particular version of software (OS, libraries, compilers, etc.). It is a very effective way of interfacing with the CMS open data as it gives you the proper environment you need to analyze these data. To learn more about Docker in general, from a HEP perspective, you may want to check out this Introduction to Docker , from Matthew Feickert. To account for the different running conditions in Run 1 vs Run 2, click the appropriate tab below for Run 1 vs Run 2 data. Run 1 Data Run 2 Data You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers You can also jump right in with the most recent tutorial on the CMS open data containers . You can find the list of Docker container images available for CMS open data in the guide page for CMS open data containers You can also jump right in with the most recent tutorial on the CMS open data containers .","title":"Docker"},{"location":"tools/git/","text":"Git \u00b6 Git is an open-source distributed version control system. Here are some helpful links to learn how to use git. Version Control with Git Pro Git Book git reference","title":"Git"},{"location":"tools/git/#git","text":"Git is an open-source distributed version control system. Here are some helpful links to learn how to use git. Version Control with Git Pro Git Book git reference","title":"Git"},{"location":"tools/root/","text":"ROOT \u00b6 Warning This page is under construction From ROOT's webpage A modular scientific software toolkit. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R. It is the primary toolkit for many experimental analysis and while you are free to analyze these datasets however you like, some familiarity with ROOT will serve you well when accessing the data. To get started analyzing data with ROOT and C++, start with C++ and ROOT . A ROOT installation comes with the virtual machine and/or docker installation pointed to in this guide. To learn more about ROOT, see the ROOT Manual . Many ROOT examples can be found here . If you don't know where to start, we would recommend fillrandom.C - fill in a 1D histogram from a parametric function basic.C - read in data and create a root file h1ReadAndDraw.c - read in a 1D histogram from a ROOT file, and then draw the histogram draw2dopt.C - explore 2D drawing options Python has become the language of choice for many analysts and most of the examples you'll see make use of the PyROOT module, callable from python. For more on pyROOT, see Python interface: PyROOT . You can go through a number of examples here . If you don't know where to start, we would recommend hsimple.py - create and draw histograms fillrandom.py - fill in a 1D histogram from a parametric function, and save your output as a root file fit1.py - open the root file created from fillrandom.py, and do a fit","title":"ROOT"},{"location":"tools/root/#root","text":"Warning This page is under construction From ROOT's webpage A modular scientific software toolkit. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualisation and storage. It is mainly written in C++ but integrated with other languages such as Python and R. It is the primary toolkit for many experimental analysis and while you are free to analyze these datasets however you like, some familiarity with ROOT will serve you well when accessing the data. To get started analyzing data with ROOT and C++, start with C++ and ROOT . A ROOT installation comes with the virtual machine and/or docker installation pointed to in this guide. To learn more about ROOT, see the ROOT Manual . Many ROOT examples can be found here . If you don't know where to start, we would recommend fillrandom.C - fill in a 1D histogram from a parametric function basic.C - read in data and create a root file h1ReadAndDraw.c - read in a 1D histogram from a ROOT file, and then draw the histogram draw2dopt.C - explore 2D drawing options Python has become the language of choice for many analysts and most of the examples you'll see make use of the PyROOT module, callable from python. For more on pyROOT, see Python interface: PyROOT . You can go through a number of examples here . If you don't know where to start, we would recommend hsimple.py - create and draw histograms fillrandom.py - fill in a 1D histogram from a parametric function, and save your output as a root file fit1.py - open the root file created from fillrandom.py, and do a fit","title":"ROOT"},{"location":"tools/unix/","text":"Unix \u00b6 The unix shell provides a command-line interpreter. The shell provides an interface between the user and the kernel. The shell will pass your commands to the operating system. Mastering basic shell commands will help you speed up and automate a variety of tasks. You can get started with unix by working through the exercises in The Unix Shell . More advanced material is available in Extra Unix Shell Material . A linux commmand line tutorial focused on ubuntu is available at The Linux command line for beginners .","title":"UNIX"},{"location":"tools/unix/#unix","text":"The unix shell provides a command-line interpreter. The shell provides an interface between the user and the kernel. The shell will pass your commands to the operating system. Mastering basic shell commands will help you speed up and automate a variety of tasks. You can get started with unix by working through the exercises in The Unix Shell . More advanced material is available in Extra Unix Shell Material . A linux commmand line tutorial focused on ubuntu is available at The Linux command line for beginners .","title":"Unix"},{"location":"tools/virtualmachines/","text":"Virtual machines \u00b6 CMS open data and legacy data, even though still exciting and full of potential, are already a few years old. Because of the rapidly evolving technolgies, the computing environments that were used to analyze these data are already ancient compared to the current, bleeding edge ones. Therefore, in order to mantain our ability to study these data, we have to rely on technologies that help us preserve adequate computer environments. One way of doing this is by using virtual machines. In simple words, a virtual machine is an emulation of a computer system that can run within another system. The latter is usually known as the host . Open data releases, CMSSW versions and operating systems \u00b6 CMS open data from our 2010 release can be studied using CMSSW_4_2_8, a version of the CMSSW software that used to run under Scientific Linux CERN 5 (slc5) operating system. Likewise, open data from our 2011/2012 release used CMSSW_5_3_32 under Scientific Linux CERN 6 (slc6). The virtual machines that are used to analyze these data, therefore, need to consider all these compatibility subtleties. Virtual machine images \u00b6 In practical terms, a virtual machine image is a computer file that has all the right ingredients to create a virtual computer inside a given host. This file, however, needs to be decoded by a virtual machine interpreter, usually known as hypervisor , which runs on the host machine. One of the most famous hypervisors is Oracle's VirtualBox . CMS virtual images \u00b6 The most current images for CMS open data usage are described separately in the CERN Open Portal site for 2010 and 2011/2012 . They come equiped with the ROOT framework, CMSSW and CVMFS access. Remember When installing a CMS virtual machine (following the instructions below), always use the latest image file available for 2010 or 2011/2012 data. Installation \u00b6 Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated. Test the environment; again, 2010 or 2011/2012 , depending on the release. Finally, check for any known issues or limitations ( 2010 , 2011/2012 .)","title":"Virtual Machines"},{"location":"tools/virtualmachines/#virtual-machines","text":"CMS open data and legacy data, even though still exciting and full of potential, are already a few years old. Because of the rapidly evolving technolgies, the computing environments that were used to analyze these data are already ancient compared to the current, bleeding edge ones. Therefore, in order to mantain our ability to study these data, we have to rely on technologies that help us preserve adequate computer environments. One way of doing this is by using virtual machines. In simple words, a virtual machine is an emulation of a computer system that can run within another system. The latter is usually known as the host .","title":"Virtual machines"},{"location":"tools/virtualmachines/#open-data-releases-cmssw-versions-and-operating-systems","text":"CMS open data from our 2010 release can be studied using CMSSW_4_2_8, a version of the CMSSW software that used to run under Scientific Linux CERN 5 (slc5) operating system. Likewise, open data from our 2011/2012 release used CMSSW_5_3_32 under Scientific Linux CERN 6 (slc6). The virtual machines that are used to analyze these data, therefore, need to consider all these compatibility subtleties.","title":"Open data releases, CMSSW versions and operating systems"},{"location":"tools/virtualmachines/#virtual-machine-images","text":"In practical terms, a virtual machine image is a computer file that has all the right ingredients to create a virtual computer inside a given host. This file, however, needs to be decoded by a virtual machine interpreter, usually known as hypervisor , which runs on the host machine. One of the most famous hypervisors is Oracle's VirtualBox .","title":"Virtual machine images"},{"location":"tools/virtualmachines/#cms-virtual-images","text":"The most current images for CMS open data usage are described separately in the CERN Open Portal site for 2010 and 2011/2012 . They come equiped with the ROOT framework, CMSSW and CVMFS access. Remember When installing a CMS virtual machine (following the instructions below), always use the latest image file available for 2010 or 2011/2012 data.","title":"CMS virtual images"},{"location":"tools/virtualmachines/#installation","text":"Detailed instructions on how to install the CERN virtual machines can be found in the 2010 and 2011/2012 virtual machine installation guides from the CERN Open Portal. Choose the one to follow depending on the data release you will be working on. In summary, the basic steps are as follows: Download and install the latest (or even better, the latest tested) version of VirtualBox . Note that it is available for an ample range of platforms. Download the latest CMS virtual image file. Choose between 2010 or 2011/2012 , depending on the data release of interest. Once downloaded, import the image file into VirtualBox. Remember Always use the latest image file available for 2010 or 2011/2012 . Older ones are usually deprecated. Test the environment; again, 2010 or 2011/2012 , depending on the release. Finally, check for any known issues or limitations ( 2010 , 2011/2012 .)","title":"Installation"}]}